{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roberta-base with mixout and CLS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0811 23:51:42.003524 4689735104 file_utils.py:41] PyTorch version 1.5.0 available.\n",
      "I0811 23:51:55.148578 4689735104 file_utils.py:57] TensorFlow version 2.2.0-rc3 available.\n",
      "I0811 23:51:58.876697 4689735104 modeling.py:230] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: WARNING Calling wandb.login() without arguments from jupyter should prompt you for an api key.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: /Users/victor/.netrc\n",
      "/Users/victor/anaconda3/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n",
      "I0811 23:52:02.329663 4689735104 textcleaner.py:37] 'pattern' package not found; tag filters are not available for English\n",
      "W0811 23:52:02.783984 4689735104 deprecation.py:323] From /Users/victor/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: WARNING Calling wandb.login() without arguments from jupyter should prompt you for an api key.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: /Users/victor/.netrc\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from dotenv import find_dotenv, load_dotenv\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import argparse\n",
    "\n",
    "try:\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '../src'))\n",
    "except:\n",
    "    sys.path.append(os.path.join(os.getcwd(), '../src'))\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcontrib.optim import SWA\n",
    "from torch.optim import Adam, SGD \n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, CyclicLR, \\\n",
    "                                     CosineAnnealingWarmRestarts\n",
    "\n",
    "from consNLP.data import load_data, data_utils, fetch_dataset\n",
    "from consNLP.models import transformer_models, activations, layers, losses, scorers\n",
    "from consNLP.visualization import visualize\n",
    "from consNLP.trainer.trainer import BasicTrainer, PLTrainer, test_pl_trainer\n",
    "from consNLP.trainer.trainer_utils import set_seed, _has_apex, _torch_lightning_available, _has_wandb, _torch_gpu_available, _num_gpus, _torch_tpu_available\n",
    "from consNLP.preprocessing.custom_tokenizer import BERTweetTokenizer\n",
    "\n",
    "if _has_apex:\n",
    "    #from torch.cuda import amp\n",
    "    from apex import amp\n",
    "\n",
    "if _torch_tpu_available:\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "\n",
    "if _has_wandb:\n",
    "    import wandb\n",
    "    try:\n",
    "        load_dotenv(find_dotenv())\n",
    "        wandb.login(key=os.environ['WANDB_API_KEY'])\n",
    "    except:\n",
    "        _has_wandb = False\n",
    "\n",
    "if _torch_lightning_available:\n",
    "    import pytorch_lightning as pl\n",
    "    from pytorch_lightning import Trainer, seed_everything\n",
    "    from pytorch_lightning.loggers import WandbLogger\n",
    "    from pytorch_lightning.metrics.metric import NumpyMetric\n",
    "    from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "\n",
    "import tokenizers\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb Logging: False, GPU: False, Pytorch Lightning: True, TPU: False, Apex: False\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(prog='Torch trainer function',conflict_handler='resolve')\n",
    "\n",
    "parser.add_argument('--train_data', type=str, default='../data/raw/COVID19Tweet-master/train.tsv', required=False,\n",
    "                    help='train data')\n",
    "parser.add_argument('--val_data', type=str, default='../data/raw/COVID19Tweet-master/valid.tsv', required=False,\n",
    "                    help='validation data')\n",
    "parser.add_argument('--test_data', type=str, default=None, required=False,\n",
    "                    help='test data')\n",
    "\n",
    "parser.add_argument('--task_type', type=str, default='binary_sequence_classification', required=False,\n",
    "                    help='type of task')\n",
    "\n",
    "parser.add_argument('--transformer_model_pretrained_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer model pretrained path or huggingface model name')\n",
    "parser.add_argument('--transformer_config_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer config file path or huggingface model name')\n",
    "parser.add_argument('--transformer_tokenizer_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer tokenizer file path or huggingface model name')\n",
    "parser.add_argument('--bpe_vocab_path', type=str, default='', required=False,\n",
    "                    help='bytepairencoding vocab file path')\n",
    "parser.add_argument('--bpe_merges_path', type=str, default='', required=False,\n",
    "                    help='bytepairencoding merges file path')\n",
    "parser.add_argument('--berttweettokenizer_path', type=str, default='', required=False,\n",
    "                    help='BERTweet tokenizer path')\n",
    "\n",
    "parser.add_argument('--max_text_len', type=int, default=80, required=False,\n",
    "                    help='maximum length of text')\n",
    "parser.add_argument('--epochs', type=int, default=5, required=False,\n",
    "                    help='number of epochs')\n",
    "parser.add_argument('--lr', type=float, default=.00003, required=False,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--loss_function', type=str, default='bcelogit', required=False,\n",
    "                    help='loss function')\n",
    "parser.add_argument('--metric', type=str, default='f1', required=False,\n",
    "                    help='scorer metric')\n",
    "\n",
    "parser.add_argument('--use_lightning_trainer', type=bool, default=True, required=False,\n",
    "                    help='if lightning trainer needs to be used')\n",
    "parser.add_argument('--use_torch_trainer', type=bool, default=False, required=False,\n",
    "                    help='if custom torch trainer needs to be used')\n",
    "parser.add_argument('--use_apex', type=bool, default=False, required=False,\n",
    "                    help='if apex needs to be used')\n",
    "parser.add_argument('--use_gpu', type=bool, default=False, required=False,\n",
    "                    help='GPU mode')\n",
    "parser.add_argument('--use_TPU', type=bool, default=False, required=False,\n",
    "                    help='TPU mode')\n",
    "parser.add_argument('--num_gpus', type=int, default=0, required=False,\n",
    "                    help='Number of GPUs')\n",
    "parser.add_argument('--num_tpus', type=int, default=0, required=False,\n",
    "                    help='Number of TPUs')\n",
    "\n",
    "parser.add_argument('--train_batch_size', type=int, default=32, required=False,\n",
    "                    help='train batch size')\n",
    "parser.add_argument('--eval_batch_size', type=int, default=16, required=False,\n",
    "                    help='eval batch size')\n",
    "\n",
    "parser.add_argument('--model_save_path', type=str, default='../models/model1_mixout/', required=False,\n",
    "                    help='seed')\n",
    "\n",
    "parser.add_argument('--wandb_logging', type=bool, default=False, required=False,\n",
    "                    help='wandb logging needed')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=42, required=False,\n",
    "                    help='seed')\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "print (\"Wandb Logging: {}, GPU: {}, Pytorch Lightning: {}, TPU: {}, Apex: {}\".format(\\\n",
    "            _has_wandb and args.wandb_logging, _torch_gpu_available,\\\n",
    "            _torch_lightning_available and args.use_lightning_trainer, _torch_tpu_available, _has_apex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape = False\n",
    "final_activation = None\n",
    "convert_output = None\n",
    "\n",
    "if args.task_type == 'binary_sequence_classification':\n",
    "    if args.metric != 'roc_auc_score': \n",
    "        convert_output = 'round'\n",
    "    if args.loss_function == 'bcelogit':\n",
    "        final_activation = 'sigmoid'\n",
    "        \n",
    "elif args.task_type == 'multiclass_sequence_classification':\n",
    "    convert_output = 'max'\n",
    "    \n",
    "elif args.task_type == 'binary_token_classification':\n",
    "    reshape = True\n",
    "    if args.metric != 'roc_auc_score': \n",
    "        convert_output = 'round'\n",
    "    if args.loss_function == 'bcelogit':\n",
    "        final_activation = 'sigmoid'\n",
    "        \n",
    "elif args.task_type == 'multiclass_token_classification':\n",
    "    reshape = True\n",
    "    convert_output = 'max'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data.load_custom_text_as_pd(args.train_data,sep='\\t',header=True, \\\n",
    "                              text_column=['Text'],target_column=['Label'])\n",
    "val_df = load_data.load_custom_text_as_pd(args.val_data,sep='\\t', header=True, \\\n",
    "                          text_column=['Text'],target_column=['Label'])\n",
    "\n",
    "train_df = pd.DataFrame(train_df,copy=False)\n",
    "val_df = pd.DataFrame(val_df,copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1241490299215634434</td>\n",
       "      <td>Official death toll from #covid19 in the Unite...</td>\n",
       "      <td>INFORMATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1245916400981381130</td>\n",
       "      <td>Dearest Mr. President @USER 1,169 coronavirus ...</td>\n",
       "      <td>INFORMATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1241132432402849793</td>\n",
       "      <td>Latest Updates March 20 ⚠️5274 new cases and 3...</td>\n",
       "      <td>INFORMATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1236107253666607104</td>\n",
       "      <td>真把公主不当干部 BREAKING: 21 people on Grand Princess...</td>\n",
       "      <td>INFORMATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1239673817552879619</td>\n",
       "      <td>OKLAHOMA CITY — The State Department of Educat...</td>\n",
       "      <td>UNINFORMATIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id                                              words  \\\n",
       "0  1241490299215634434  Official death toll from #covid19 in the Unite...   \n",
       "1  1245916400981381130  Dearest Mr. President @USER 1,169 coronavirus ...   \n",
       "2  1241132432402849793  Latest Updates March 20 ⚠️5274 new cases and 3...   \n",
       "3  1236107253666607104  真把公主不当干部 BREAKING: 21 people on Grand Princess...   \n",
       "4  1239673817552879619  OKLAHOMA CITY — The State Department of Educat...   \n",
       "\n",
       "          labels  \n",
       "0    INFORMATIVE  \n",
       "1    INFORMATIVE  \n",
       "2    INFORMATIVE  \n",
       "3    INFORMATIVE  \n",
       "4  UNINFORMATIVE  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = args.model_save_path\n",
    "try:\n",
    "    os.makedirs(model_save_dir)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.labels, label2idx = data_utils.convert_categorical_label_to_int(train_df.labels, \\\n",
    "                                                         save_path=os.path.join(model_save_dir,'label2idx.pkl'))\n",
    "\n",
    "val_df.labels, _ = data_utils.convert_categorical_label_to_int(val_df.labels, \\\n",
    "                                                         save_path=os.path.join(model_save_dir,'label2idx.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1241490299215634434</td>\n",
       "      <td>Official death toll from #covid19 in the Unite...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1245916400981381130</td>\n",
       "      <td>Dearest Mr. President @USER 1,169 coronavirus ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1241132432402849793</td>\n",
       "      <td>Latest Updates March 20 ⚠️5274 new cases and 3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1236107253666607104</td>\n",
       "      <td>真把公主不当干部 BREAKING: 21 people on Grand Princess...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1239673817552879619</td>\n",
       "      <td>OKLAHOMA CITY — The State Department of Educat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id                                              words  \\\n",
       "0  1241490299215634434  Official death toll from #covid19 in the Unite...   \n",
       "1  1245916400981381130  Dearest Mr. President @USER 1,169 coronavirus ...   \n",
       "2  1241132432402849793  Latest Updates March 20 ⚠️5274 new cases and 3...   \n",
       "3  1236107253666607104  真把公主不当干部 BREAKING: 21 people on Grand Princess...   \n",
       "4  1239673817552879619  OKLAHOMA CITY — The State Department of Educat...   \n",
       "\n",
       "   labels  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0811 23:55:14.491278 4689735104 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /Users/victor/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0811 23:55:14.492293 4689735104 configuration_utils.py:319] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0811 23:55:16.571876 4689735104 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /Users/victor/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0811 23:55:16.572763 4689735104 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /Users/victor/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    }
   ],
   "source": [
    "if args.berttweettokenizer_path:\n",
    "    tokenizer = BERTweetTokenizer(args.berttweettokenizer_path)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.transformer_model_pretrained_path)\n",
    "\n",
    "if not args.berttweettokenizer_path:\n",
    "    try:\n",
    "        bpetokenizer = tokenizers.ByteLevelBPETokenizer(args.bpe_vocab_path, \\\n",
    "                                        args.bpe_merges_path)\n",
    "    except:\n",
    "        bpetokenizer = None \n",
    "else:\n",
    "    bpetokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_utils.TransformerDataset(train_df.words, bpetokenizer=bpetokenizer, tokenizer=tokenizer, MAX_LEN=args.max_text_len, \\\n",
    "              target_label=train_df.labels, sequence_target=False, target_text=None, conditional_label=None, conditional_all_labels=None)\n",
    "\n",
    "val_dataset = data_utils.TransformerDataset(val_df.words, bpetokenizer=bpetokenizer, tokenizer=tokenizer, MAX_LEN=args.max_text_len, \\\n",
    "              target_label=val_df.labels, sequence_target=False, target_text=None, conditional_label=None, conditional_all_labels=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWithMixout(nn.Module):\n",
    "    def __init__(self, model, main_dropout_prob=0, mixout_prob=.7, dropout=.3, n_out=1):\n",
    "        super(TransformerWithMixout, self).__init__()\n",
    "        for i in range(model.config.num_hidden_layers):\n",
    "            num = '{}'.format(i)\n",
    "            for name, module in model._modules['encoder']._modules['layer']._modules[num]._modules['output']._modules.items():\n",
    "                if name == 'dropout' and isinstance(module, nn.Dropout):\n",
    "                    model._modules['encoder']._modules['layer']._modules[num]._modules['output']._modules[name] = nn.Dropout(main_dropout_prob)\n",
    "                    #setattr(model, name, nn.Dropout(0))\n",
    "                if name.split('.')[-1] == 'dense' and isinstance(module, nn.Linear):\n",
    "                    target_state_dict = module.state_dict()\n",
    "                    bias = True if module.bias is not None else False\n",
    "                    new_module = layers.MixLinear(module.in_features, module.out_features, \n",
    "                                           bias, target_state_dict['weight'], mixout_prob)\n",
    "                    new_module.load_state_dict(target_state_dict)\n",
    "                    #setattr(model, name, new_module)\n",
    "                    model._modules['encoder']._modules['layer']._modules[num]._modules['output']._modules[name] = new_module\n",
    "            \n",
    "            #model._modules['drop'] = nn.Dropout(main_dropout_prob)\n",
    "            \n",
    "            #module = model._modules['out']\n",
    "            #target_state_dict = module.state_dict()\n",
    "            #bias = True if module.bias is not None else False\n",
    "            #new_module = MixLinear(module.in_features, module.out_features, \n",
    "            #                       bias, target_state_dict['weight'], mixout_prob)\n",
    "            #new_module.load_state_dict(target_state_dict)\n",
    "                    \n",
    "            #model._modules['out'] = new_module\n",
    "\n",
    "        self.base_model = model\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(model.config.hidden_size, n_out)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        o2 = self.base_model(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        o2 = o2[0][:,1:,:]\n",
    "        bo = self.drop(o2)\n",
    "        bo = torch.mean(o2, dim=1)\n",
    "        #bo = self.drop(o2)\n",
    "        output = self.out(bo)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0811 23:56:08.665372 4689735104 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /Users/victor/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0811 23:56:08.666308 4689735104 configuration_utils.py:319] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0811 23:56:09.784987 4689735104 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /Users/victor/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(args.transformer_config_path, output_hidden_states=True, output_attentions=True)\n",
    "basemodel = AutoModel.from_pretrained(args.transformer_model_pretrained_path,config=config)\n",
    "model = TransformerWithMixout(basemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _torch_tpu_available and args.use_TPU:\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "      train_dataset,\n",
    "      num_replicas=xm.xrt_world_size(),\n",
    "      rank=xm.get_ordinal(),\n",
    "      shuffle=True\n",
    "    )\n",
    "\n",
    "    val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "      val_dataset,\n",
    "      num_replicas=xm.xrt_world_size(),\n",
    "      rank=xm.get_ordinal(),\n",
    "      shuffle=False\n",
    "    )\n",
    "\n",
    "if _torch_tpu_available and args.use_TPU:\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.train_batch_size, sampler=train_sampler,\n",
    "        drop_last=True,num_workers=2)\n",
    "\n",
    "    val_data_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=args.eval_batch_size, sampler=val_sampler,\n",
    "        drop_last=False,num_workers=1)\n",
    "else:\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.train_batch_size)\n",
    "\n",
    "    val_data_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=args.eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "I0812 08:03:41.591238 4689735104 distributed.py:29] GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "I0812 08:03:41.595145 4689735104 distributed.py:29] TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name   | Type                  | Params\n",
      "-------------------------------------------------\n",
      "0 | model  | TransformerWithMixout | 124 M \n",
      "1 | metric | PLMetric              | 0     \n",
      "I0812 08:03:41.677448 4689735104 lightning.py:1495] \n",
      "  | Name   | Type                  | Params\n",
      "-------------------------------------------------\n",
      "0 | model  | TransformerWithMixout | 124 M \n",
      "1 | metric | PLMetric              | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CPU\n",
      "[LOG] Total number of parameters to learn 124646401\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss = 0.351 val metric = 0.903 \n",
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db8d8827e844503ab059dd1c40ec233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00000: val_metric reached 0.90166 (best 0.90166), saving model to ../models/model1_mixout/epoch=0_v0.ckpt as top 1\n",
      "I0812 08:32:03.452784 4689735104 model_checkpoint.py:346] \n",
      "Epoch 00000: val_metric reached 0.90166 (best 0.90166), saving model to ../models/model1_mixout/epoch=0_v0.ckpt as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss = 0.501 val metric = 0.902 \n",
      "Train loss = 0.017 Train metric = 0.994\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_metric  was not in top 1\n",
      "I0812 09:00:05.145328 4689735104 model_checkpoint.py:314] \n",
      "Epoch 00001: val_metric  was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss = 0.672 val metric = 0.884 \n",
      "Train loss = 0.019 Train metric = 0.993\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_metric  was not in top 1\n",
      "I0812 09:33:04.674398 4689735104 model_checkpoint.py:314] \n",
      "Epoch 00002: val_metric  was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss = 0.461 val metric = 0.895 \n",
      "Train loss = 0.016 Train metric = 0.995\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_metric  was not in top 1\n",
      "I0812 10:07:17.873924 4689735104 model_checkpoint.py:314] \n",
      "Epoch 00003: val_metric  was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss = 0.533 val metric = 0.883 \n",
      "Train loss = 0.013 Train metric = 0.996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if args.use_torch_trainer:\n",
    "    device = torch.device(\"cuda\" if _torch_gpu_available and args.use_gpu else \"cpu\")\n",
    "\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        device=xm.xla_device()\n",
    "\n",
    "    print (\"Device: {}\".format(device))\n",
    "    \n",
    "    if args.use_TPU and _torch_tpu_available and args.num_tpus > 1:\n",
    "        train_data_loader = torch_xla.distributed.parallel_loader.ParallelLoader(train_data_loader, [device])\n",
    "        train_data_loader = train_data_loader.per_device_loader(device)\n",
    "\n",
    "\n",
    "    trainer = BasicTrainer(model, train_data_loader, val_data_loader, device, args.transformer_model_pretrained_path, \\\n",
    "                               final_activation=final_activation, \\\n",
    "                               test_data_loader=val_data_loader)\n",
    "\n",
    "    param_optimizer = list(trainer.model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.001,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    num_train_steps = int(len(train_data_loader) * args.epochs)\n",
    "\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        optimizer = AdamW(optimizer_parameters, lr=args.lr*xm.xrt_world_size())\n",
    "    else:\n",
    "        optimizer = AdamW(optimizer_parameters, lr=args.lr)\n",
    "\n",
    "    if args.use_apex and _has_apex:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
    "    \n",
    "    loss = losses.get_loss(args.loss_function)\n",
    "    scorer = scorers.SKMetric(args.metric, convert=convert_output, reshape=reshape) \n",
    "    \n",
    "    def _mp_fn(rank, flags, trainer, epochs, lr, metric, loss_function, optimizer, scheduler, model_save_path, num_gpus, num_tpus,  \\\n",
    "                max_grad_norm, early_stopping_rounds, snapshot_ensemble, is_amp, use_wandb, seed):\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "        a = trainer.train(epochs, lr, metric, loss_function, optimizer, scheduler, model_save_path, num_gpus, num_tpus,  \\\n",
    "                max_grad_norm, early_stopping_rounds, snapshot_ensemble, is_amp, use_wandb, seed)\n",
    "\n",
    "    FLAGS = {}\n",
    "    if _torch_tpu_available and args.use_TPU:\n",
    "        xmp.spawn(_mp_fn, args=(FLAGS, trainer, args.epochs, args.lr, scorer, loss, optimizer, scheduler, args.model_save_path, args.num_gpus, args.num_tpus, \\\n",
    "                 1, 3, False, args.use_apex, False, args.seed), nprocs=8, start_method='fork')\n",
    "    else:\n",
    "        use_wandb = _has_wandb and args.wandb_logging\n",
    "        trainer.train(args.epochs, args.lr, scorer, loss, optimizer, scheduler, args.model_save_path, args.num_gpus, args.num_tpus,  \\\n",
    "                max_grad_norm=1, early_stopping_rounds=3, snapshot_ensemble=False, is_amp=args.use_apex, use_wandb=use_wandb, seed=args.seed)\n",
    "\n",
    "elif args.use_lightning_trainer and _torch_lightning_available:\n",
    "    from pytorch_lightning import Trainer, seed_everything\n",
    "    seed_everything(args.seed)\n",
    "    \n",
    "    loss = losses.get_loss(args.loss_function)\n",
    "    scorer = scorers.PLMetric(args.metric, convert=convert_output, reshape=reshape)\n",
    "    \n",
    "    log_args = {'description': args.transformer_model_pretrained_path, 'loss': loss.__class__.__name__, 'epochs': args.epochs, 'learning_rate': args.lr}\n",
    "\n",
    "    if _has_wandb and not _torch_tpu_available and args.wandb_logging:\n",
    "        wandb.init(project=\"WNUT-Task-2\",config=log_args)\n",
    "        wandb_logger = WandbLogger()\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                filepath=args.model_save_path,\n",
    "                save_top_k=1,\n",
    "                verbose=True,\n",
    "                monitor='val_metric',\n",
    "                mode='max'\n",
    "                )\n",
    "    earlystop = EarlyStopping(\n",
    "                monitor='val_metric',\n",
    "                patience=3,\n",
    "               verbose=False,\n",
    "               mode='max'\n",
    "               )\n",
    "\n",
    "    if args.use_gpu and _torch_gpu_available:\n",
    "        print (\"using GPU\")\n",
    "        if args.wandb_logging:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, logger=wandb_logger, precision=16, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, logger=wandb_logger, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, precision=16, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(gpus=args.num_gpus, max_epochs=args.epochs, \\\n",
    "                            checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    elif args.use_TPU and _torch_tpu_available:\n",
    "        print (\"using TPU\")\n",
    "        if _has_apex:\n",
    "            trainer = Trainer(num_tpu_cores=args.num_tpus, max_epochs=args.epochs, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            trainer = Trainer(num_tpu_cores=args.num_tpus, max_epochs=args.epochs, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    else:\n",
    "        print (\"using CPU\")\n",
    "        if args.wandb_logging:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(max_epochs=args.epochs, logger=wandb_logger, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(max_epochs=args.epochs, logger=wandb_logger, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "        else:\n",
    "            if _has_apex:\n",
    "                trainer = Trainer(max_epochs=args.epochs, precision=16, \\\n",
    "                        checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "            else:\n",
    "                trainer = Trainer(max_epochs=args.epochs, checkpoint_callback=checkpoint_callback, callbacks=[earlystop])\n",
    "\n",
    "    num_train_steps = int(len(train_data_loader) * args.epochs)\n",
    "\n",
    "    pltrainer = PLTrainer(num_train_steps, model, scorer, loss, args.lr, \\\n",
    "                          final_activation=final_activation, seed=42)\n",
    "\n",
    "    #try:\n",
    "    #    print (\"Loaded model from previous checkpoint\")\n",
    "    #    pltrainer = PLTrainer.load_from_checkpoint(args.model_save_path)\n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "    trainer.fit(pltrainer, train_data_loader, val_data_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|▊         | 1/13 [00:02<00:27,  2.33s/it]\u001b[A\n",
      " 15%|█▌        | 2/13 [00:04<00:23,  2.17s/it]\u001b[A\n",
      " 23%|██▎       | 3/13 [00:05<00:20,  2.08s/it]\u001b[A\n",
      " 31%|███       | 4/13 [00:08<00:18,  2.07s/it]\u001b[A\n",
      " 38%|███▊      | 5/13 [00:10<00:16,  2.06s/it]\u001b[A\n",
      " 46%|████▌     | 6/13 [00:12<00:14,  2.03s/it]\u001b[A\n",
      " 54%|█████▍    | 7/13 [00:13<00:11,  1.97s/it]\u001b[A\n",
      " 62%|██████▏   | 8/13 [00:15<00:09,  1.92s/it]\u001b[A\n",
      " 69%|██████▉   | 9/13 [00:17<00:07,  1.91s/it]\u001b[A\n",
      " 77%|███████▋  | 10/13 [00:19<00:05,  1.88s/it]\u001b[A\n",
      " 85%|████████▍ | 11/13 [00:21<00:03,  1.86s/it]\u001b[A\n",
      " 92%|█████████▏| 12/13 [00:23<00:01,  1.85s/it]\u001b[A\n",
      "100%|██████████| 13/13 [00:23<00:00,  1.84s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_output2 = []\n",
    "\n",
    "for val_batch in tqdm(val_data_loader):\n",
    "    out = torch.sigmoid(pltrainer(val_batch)).detach().cpu().numpy()\n",
    "    test_output2.extend(out[:,0].tolist())\n",
    "    \n",
    "#test_output2 = np.concatenate(test_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.90560145],\n",
       "       [0.90560145, 1.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output1 = np.array(test_output1)[:,0]\n",
    "test_output2 = np.array(test_output2)\n",
    "np.corrcoef(test_output1,test_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
