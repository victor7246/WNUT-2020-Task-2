{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0929 16:53:47.328201 4382481856 file_utils.py:41] PyTorch version 1.5.0 available.\n",
      "I0929 16:54:03.835326 4382481856 file_utils.py:57] TensorFlow version 2.2.0-rc3 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '../'))\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import cv2\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "from captum import attr\n",
    "\n",
    "#from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization\n",
    "\n",
    "from src import data, models, visualization\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data.load_data.load_custom_text_as_pd('../data/raw/COVID19Tweet/train.tsv',sep='\\t', header=True, \\\n",
    "                          text_column=['Text'],target_column=['Label'])\n",
    "\n",
    "val_df = data.load_data.load_custom_text_as_pd('../data/raw/COVID19Tweet/valid.tsv',sep='\\t', header=False, \\\n",
    "                          text_column=['Text'],target_column=['Label'])\n",
    "val_df = pd.DataFrame(val_df,copy=False)\n",
    "val_df.columns = ['Id','words','labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1241490299215634434</td>\n",
       "      <td>Official death toll from #covid19 in the Unite...</td>\n",
       "      <td>INFORMATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1245916400981381130</td>\n",
       "      <td>Dearest Mr. President @USER 1,169 coronavirus ...</td>\n",
       "      <td>INFORMATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1241132432402849793</td>\n",
       "      <td>Latest Updates March 20 ‚ö†Ô∏è5274 new cases and 3...</td>\n",
       "      <td>INFORMATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1236107253666607104</td>\n",
       "      <td>ÁúüÊääÂÖ¨‰∏ª‰∏çÂΩìÂπ≤ÈÉ® BREAKING: 21 people on Grand Princess...</td>\n",
       "      <td>INFORMATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1239673817552879619</td>\n",
       "      <td>OKLAHOMA CITY ‚Äî The State Department of Educat...</td>\n",
       "      <td>UNINFORMATIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id                                              words  \\\n",
       "0  1241490299215634434  Official death toll from #covid19 in the Unite...   \n",
       "1  1245916400981381130  Dearest Mr. President @USER 1,169 coronavirus ...   \n",
       "2  1241132432402849793  Latest Updates March 20 ‚ö†Ô∏è5274 new cases and 3...   \n",
       "3  1236107253666607104  ÁúüÊääÂÖ¨‰∏ª‰∏çÂΩìÂπ≤ÈÉ® BREAKING: 21 people on Grand Princess...   \n",
       "4  1239673817552879619  OKLAHOMA CITY ‚Äî The State Department of Educat...   \n",
       "\n",
       "          labels  \n",
       "0    INFORMATIVE  \n",
       "1    INFORMATIVE  \n",
       "2    INFORMATIVE  \n",
       "3    INFORMATIVE  \n",
       "4  UNINFORMATIVE  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1241728922192142336</td>\n",
       "      <td>For those saying Pakistan isn‚Äôt Italy; After 3...</td>\n",
       "      <td>UNINFORMATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1235713405992030209</td>\n",
       "      <td>Second case DR üá©üá¥ The Canadian woman has not b...</td>\n",
       "      <td>INFORMATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1245941302367305728</td>\n",
       "      <td>Kill Chain: the cyber war on America's electio...</td>\n",
       "      <td>UNINFORMATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1245913002840391681</td>\n",
       "      <td>Town hosts FIRST #Virtual #TownCouncil meeting...</td>\n",
       "      <td>UNINFORMATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1240543259299987457</td>\n",
       "      <td>Report suggested that the actual number of und...</td>\n",
       "      <td>UNINFORMATIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id                                              words  \\\n",
       "0  1241728922192142336  For those saying Pakistan isn‚Äôt Italy; After 3...   \n",
       "1  1235713405992030209  Second case DR üá©üá¥ The Canadian woman has not b...   \n",
       "2  1245941302367305728  Kill Chain: the cyber war on America's electio...   \n",
       "3  1245913002840391681  Town hosts FIRST #Virtual #TownCouncil meeting...   \n",
       "4  1240543259299987457  Report suggested that the actual number of und...   \n",
       "\n",
       "          labels  \n",
       "0  UNINFORMATIVE  \n",
       "1    INFORMATIVE  \n",
       "2  UNINFORMATIVE  \n",
       "3  UNINFORMATIVE  \n",
       "4  UNINFORMATIVE  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0929 16:54:11.030981 4382481856 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /Users/victor/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0929 16:54:11.032019 4382481856 configuration_utils.py:319] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0929 16:54:13.545865 4382481856 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /Users/victor/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0929 16:54:13.546581 4382481856 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /Users/victor/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot TF-IDF Conditioned on label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "                         lowercase=True, #this will convert all the tokens into lower case\n",
    "                         stop_words='english', #remove english stopwords from vocabulary. if we need the stopwords this value should be None\n",
    "                         analyzer='word', #tokens should be words. we can also use char for character tokens\n",
    "                         max_features=10000, #maximum vocabulary size to restrict too many features\n",
    "                         min_df = 2,\n",
    "                         max_df = .5,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: This dataframe has a column name that matches the 'value_name' column name of the resultiing Dataframe. In the future this will raise an error, please set the 'value_name' parameter of DataFrame.melt to a unique name.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "visualization.visualize.plot_top_words_conditional(train_df.words,train_df.labels,vectorizer=vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze submitted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_max_len': 100,\n",
       " 'epochs': 15,\n",
       " 'learning_rate': 2e-05,\n",
       " 'batch_size': 32,\n",
       " 'dropout': 0.2,\n",
       " 'model_description': 'roberta-base mean of all tokens from last 4 layers'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = '../models/submitted_model/'\n",
    "max_text_len = 100\n",
    "config = pickle.load(open(os.path.join(model_save_dir,'config.pkl'),'rb'))\n",
    "label2idx = pickle.load(open(os.path.join(model_save_dir,'label2idx.pkl'),'rb'))\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNINFORMATIVE': 0, 'INFORMATIVE': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2label = {i:w for (w,i) in label2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 1860.36it/s]\n"
     ]
    }
   ],
   "source": [
    "val_df.labels, _ = data.data_utils.convert_categorical_label_to_int(val_df.labels, \\\n",
    "                                                         save_path=os.path.join(model_save_dir,'label2idx.pkl'))\n",
    "valX = data.data_utils.compute_transformer_input_arrays(val_df, 'words', tokenizer, max_text_len)\n",
    "val_outputs = data.data_utils.compute_output_arrays(val_df, 'labels')\n",
    "val_dataset = data.data_utils.TorchDataLoader(valX[0], valX[1], valX[2], val_outputs)\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0929 16:55:03.924986 4382481856 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /Users/victor/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0929 16:55:03.925929 4382481856 configuration_utils.py:319] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0929 16:55:05.011840 4382481856 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /Users/victor/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n"
     ]
    }
   ],
   "source": [
    "model1 = models.torch_models.TransformerAvgPool('roberta-base', device, dropout=config['dropout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.load_state_dict(torch.load(os.path.join(model_save_dir, 'model.bin'),map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [02:34<00:00,  2.45s/it]\n"
     ]
    }
   ],
   "source": [
    "all_outputs1 = []\n",
    "all_emb = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, d in tqdm(enumerate(val_data_loader), total=len(val_data_loader)):\n",
    "        ids = d['ids'].to(device)\n",
    "        mask = d['mask'].to(device)\n",
    "        token_type_ids = d['token_type_ids'].to(device)\n",
    "        \n",
    "        emb = model1.base_model(ids, mask, token_type_ids)[-1]\n",
    "        emb = torch.cat([emb[i] for i in model1.layers], -1)\n",
    "        emb = torch.mean(emb, 1)\n",
    "        emb = model1.drop(emb)\n",
    "        \n",
    "        all_outputs1.extend(torch.sigmoid(model1(ids, mask, token_type_ids)).cpu().detach().numpy().tolist())\n",
    "        all_emb.extend(emb.cpu().detach().numpy().tolist())\n",
    "    \n",
    "all_outputs1 = np.array(all_outputs1)\n",
    "val_df['submitted_model'] = all_outputs1[:,0]\n",
    "all_emb = np.array(all_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "      <th>submitted_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1241728922192142336</td>\n",
       "      <td>For those saying Pakistan isn‚Äôt Italy; After 3...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.824923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1235713405992030209</td>\n",
       "      <td>Second case DR üá©üá¥ The Canadian woman has not b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1245941302367305728</td>\n",
       "      <td>Kill Chain: the cyber war on America's electio...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1245913002840391681</td>\n",
       "      <td>Town hosts FIRST #Virtual #TownCouncil meeting...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1240543259299987457</td>\n",
       "      <td>Report suggested that the actual number of und...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1244613970142662662</td>\n",
       "      <td>Death happens all the time but is something we...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1241885583322071042</td>\n",
       "      <td>cw | coronavirus i can‚Äôt see my dad for a week...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.992968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1236098806309150720</td>\n",
       "      <td>The issue? The Coronavirus cases in MoCo. The ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1251111178329358337</td>\n",
       "      <td>This is nothing more than BS rhetoric in an at...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1236202072539717632</td>\n",
       "      <td>UPDATE: the two people at our Hail Creek opera...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id                                              words  \\\n",
       "0  1241728922192142336  For those saying Pakistan isn‚Äôt Italy; After 3...   \n",
       "1  1235713405992030209  Second case DR üá©üá¥ The Canadian woman has not b...   \n",
       "2  1245941302367305728  Kill Chain: the cyber war on America's electio...   \n",
       "3  1245913002840391681  Town hosts FIRST #Virtual #TownCouncil meeting...   \n",
       "4  1240543259299987457  Report suggested that the actual number of und...   \n",
       "5  1244613970142662662  Death happens all the time but is something we...   \n",
       "6  1241885583322071042  cw | coronavirus i can‚Äôt see my dad for a week...   \n",
       "7  1236098806309150720  The issue? The Coronavirus cases in MoCo. The ...   \n",
       "8  1251111178329358337  This is nothing more than BS rhetoric in an at...   \n",
       "9  1236202072539717632  UPDATE: the two people at our Hail Creek opera...   \n",
       "\n",
       "   labels  submitted_model  \n",
       "0       0         0.824923  \n",
       "1       1         0.999979  \n",
       "2       0         0.000118  \n",
       "3       0         0.000231  \n",
       "4       0         0.004176  \n",
       "5       1         0.999648  \n",
       "6       1         0.992968  \n",
       "7       0         0.000810  \n",
       "8       0         0.000313  \n",
       "9       1         0.999983  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0929 16:57:52.725656 4382481856 utils.py:141] NumExpr defaulting to 4 threads.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(88, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df[np.round(val_df.submitted_model) != val_df.labels].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2).fit_transform(all_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis_data = pd.DataFrame()\n",
    "vis_data['x'] = X_embedded[:,0]\n",
    "vis_data['y'] = X_embedded[:,1]\n",
    "vis_data['label'] = [idx2label[i] for i in val_df.labels.values]\n",
    "\n",
    "plt.scatter(vis_data[vis_data.label == 'INFORMATIVE'].x, vis_data[vis_data.label == 'INFORMATIVE'].y, label='INFORMATIVE')\n",
    "plt.scatter(vis_data[vis_data.label == 'UNINFORMATIVE'].x, vis_data[vis_data.label == 'UNINFORMATIVE'].y, label='UNINFORMATIVE')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"../visualizations/embeddings.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Multi-Sample Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_max_len': 100,\n",
       " 'epochs': 15,\n",
       " 'learning_rate': 2e-05,\n",
       " 'batch_size': 32,\n",
       " 'dropout': 0.1,\n",
       " 'mixout': 0,\n",
       " 'l2': 0,\n",
       " 'multi_sample_dropout_count': 7,\n",
       " 'model_description': 'roberta-base with dropout 0.1, mixout prob 0, multi_sample_dropout_count 7 and l2 regularization 0'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = '../models/multisample_model/'\n",
    "max_text_len = 100\n",
    "config = pickle.load(open(os.path.join(model_save_dir,'config.pkl'),'rb'))\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0929 16:58:02.819173 4382481856 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /Users/victor/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0929 16:58:02.819995 4382481856 configuration_utils.py:319] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0929 16:58:03.871038 4382481856 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /Users/victor/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n"
     ]
    }
   ],
   "source": [
    "model2 = models.torch_models.TransformerMultiSample('roberta-base', device, dropout=config['dropout'],\\\n",
    "                                                   multi_sample_dropout_count=config['multi_sample_dropout_count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_state_dict(torch.load(os.path.join(model_save_dir, 'model.bin'),map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [02:32<00:00,  2.42s/it]\n"
     ]
    }
   ],
   "source": [
    "all_logits = []\n",
    "all_outputs2 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, d in tqdm(enumerate(val_data_loader), total=len(val_data_loader)):\n",
    "        ids = d['ids'].to(device)\n",
    "        mask = d['mask'].to(device)\n",
    "        token_type_ids = d['token_type_ids'].to(device)\n",
    "\n",
    "        outputs = model2.base_model(ids, mask, token_type_ids)[0][:,0,:]\n",
    "        outputs = torch.cat([model2.outs[i](model2.drops[i](outputs)) for i in range(model2.multi_sample_dropout_count)], -1)\n",
    "\n",
    "        all_logits.extend(outputs.cpu().detach().numpy().tolist())\n",
    "\n",
    "        all_outputs2.extend(torch.sigmoid(model2(ids, mask, token_type_ids)).cpu().detach().numpy().tolist())\n",
    "    \n",
    "all_logits = np.array(all_logits)\n",
    "all_outputs2 = np.array(all_outputs2)\n",
    "val_df['submitted_multidrop'] = all_outputs2[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "      <th>submitted_model</th>\n",
       "      <th>submitted_multidrop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1241728922192142336</td>\n",
       "      <td>For those saying Pakistan isn‚Äôt Italy; After 3...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.824923</td>\n",
       "      <td>0.432517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1235713405992030209</td>\n",
       "      <td>Second case DR üá©üá¥ The Canadian woman has not b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.542584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1245941302367305728</td>\n",
       "      <td>Kill Chain: the cyber war on America's electio...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.449389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1245913002840391681</td>\n",
       "      <td>Town hosts FIRST #Virtual #TownCouncil meeting...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.477913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1240543259299987457</td>\n",
       "      <td>Report suggested that the actual number of und...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004176</td>\n",
       "      <td>0.459396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1244613970142662662</td>\n",
       "      <td>Death happens all the time but is something we...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999648</td>\n",
       "      <td>0.525245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1241885583322071042</td>\n",
       "      <td>cw | coronavirus i can‚Äôt see my dad for a week...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.992968</td>\n",
       "      <td>0.446962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1236098806309150720</td>\n",
       "      <td>The issue? The Coronavirus cases in MoCo. The ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.445049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1251111178329358337</td>\n",
       "      <td>This is nothing more than BS rhetoric in an at...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.463307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1236202072539717632</td>\n",
       "      <td>UPDATE: the two people at our Hail Creek opera...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.532491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id                                              words  \\\n",
       "0  1241728922192142336  For those saying Pakistan isn‚Äôt Italy; After 3...   \n",
       "1  1235713405992030209  Second case DR üá©üá¥ The Canadian woman has not b...   \n",
       "2  1245941302367305728  Kill Chain: the cyber war on America's electio...   \n",
       "3  1245913002840391681  Town hosts FIRST #Virtual #TownCouncil meeting...   \n",
       "4  1240543259299987457  Report suggested that the actual number of und...   \n",
       "5  1244613970142662662  Death happens all the time but is something we...   \n",
       "6  1241885583322071042  cw | coronavirus i can‚Äôt see my dad for a week...   \n",
       "7  1236098806309150720  The issue? The Coronavirus cases in MoCo. The ...   \n",
       "8  1251111178329358337  This is nothing more than BS rhetoric in an at...   \n",
       "9  1236202072539717632  UPDATE: the two people at our Hail Creek opera...   \n",
       "\n",
       "   labels  submitted_model  submitted_multidrop  \n",
       "0       0         0.824923             0.432517  \n",
       "1       1         0.999979             0.542584  \n",
       "2       0         0.000118             0.449389  \n",
       "3       0         0.000231             0.477913  \n",
       "4       0         0.004176             0.459396  \n",
       "5       1         0.999648             0.525245  \n",
       "6       1         0.992968             0.446962  \n",
       "7       0         0.000810             0.445049  \n",
       "8       0         0.000313             0.463307  \n",
       "9       1         0.999983             0.532491  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(all_logits.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.07768844721098164, 0.25344765560883104)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_diag = np.where(~np.eye(corr.shape[0],dtype=bool))\n",
    "mean_corr = np.mean([corr[non_diag[0][i],non_diag[1][i]] for i in range(len(non_diag[0]))])\n",
    "var_corr = np.var([corr[non_diag[0][i],non_diag[1][i]] for i in range(len(non_diag[0]))])\n",
    "\n",
    "mean_corr, var_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_max_len': 100,\n",
       " 'epochs': 15,\n",
       " 'learning_rate': 2e-05,\n",
       " 'batch_size': 32,\n",
       " 'dropout': 0.1,\n",
       " 'mixout': 0,\n",
       " 'l2': 0,\n",
       " 'multi_sample_dropout_count': 0,\n",
       " 'model_description': 'roberta-base with dropout 0.1, mixout prob 0, multi_sample_dropout_count 0, l2 regularization 0 and augmentation rate 0.1'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = '../models/augmented_model/'\n",
    "max_text_len = 100\n",
    "config = pickle.load(open(os.path.join(model_save_dir,'config.pkl'),'rb'))\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0929 17:00:49.962167 4382481856 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /Users/victor/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0929 17:00:49.963119 4382481856 configuration_utils.py:319] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0929 17:00:51.043888 4382481856 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /Users/victor/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n"
     ]
    }
   ],
   "source": [
    "model3 = models.torch_models.Transformer('roberta-base', dropout=config['dropout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.load_state_dict(torch.load(os.path.join(model_save_dir, 'model.bin'),map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [01:15<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "all_outputs3 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, d in tqdm(enumerate(val_data_loader), total=len(val_data_loader)):\n",
    "        ids = d['ids'].to(device)\n",
    "        mask = d['mask'].to(device)\n",
    "        token_type_ids = d['token_type_ids'].to(device)\n",
    "\n",
    "        all_outputs3.extend(torch.sigmoid(model3(ids, mask, token_type_ids)).cpu().detach().numpy().tolist())\n",
    "    \n",
    "all_outputs3 = np.array(all_outputs3)\n",
    "val_df['augmented_model'] = all_outputs3[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixout Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_max_len': 100,\n",
       " 'epochs': 15,\n",
       " 'learning_rate': 2e-05,\n",
       " 'batch_size': 32,\n",
       " 'dropout': 0,\n",
       " 'mixout': 0.6,\n",
       " 'l2': 0,\n",
       " 'multi_sample_dropout_count': 0,\n",
       " 'model_description': 'roberta-base with dropout 0, mixout prob 0.6, multi_sample_dropout_count 0, l2 regularization 0 and augmentation rate 0.3'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = '../models/mixout_model/'\n",
    "max_text_len = 100\n",
    "config = pickle.load(open(os.path.join(model_save_dir,'config.pkl'),'rb'))\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-f294a3ec7d90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'*.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mupdated_checkpoint_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_checkpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "checkpoints = glob(model_save_dir+'*.ckpt')\n",
    "best_checkpoint = torch.load(checkpoints[0],map_location=device)\n",
    "updated_checkpoint_state = OrderedDict([('.'.join(key.split('.')[1:]), v) for key, v in best_checkpoint['state_dict'].items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = models.torch_models.TransformerWithMixout('roberta-base', mixout_prob=config['mixout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.load_state_dict(updated_checkpoint_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs4 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi, d in tqdm(enumerate(val_data_loader), total=len(val_data_loader)):\n",
    "        ids = d['ids'].to(device)\n",
    "        mask = d['mask'].to(device)\n",
    "        token_type_ids = d['token_type_ids'].to(device)\n",
    "\n",
    "        all_outputs4.extend(torch.sigmoid(model4(ids, mask, token_type_ids)).cpu().detach().numpy().tolist())\n",
    "    \n",
    "all_outputs4 = np.array(all_outputs4)\n",
    "val_df['mixout_model'] = all_outputs4[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[np.round(val_df.submitted_model) != val_df.labels][(np.round(val_df.submitted_multidrop) == val_df.labels) | (np.round(val_df.augmented_model) == val_df.labels) | (np.round(val_df.mixout_model) == val_df.labels)].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[np.round(val_df.submitted_model) != val_df.labels][(np.round(val_df.submitted_multidrop) == val_df.labels) | (np.round(val_df.augmented_model) == val_df.labels) | (np.round(val_df.mixout_model) == val_df.labels)].labels.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[np.round(val_df.submitted_model) != val_df.labels][np.round(val_df.augmented_model) == val_df.labels][np.round(val_df.submitted_multidrop) == val_df.labels][np.round(val_df.mixout_model) == val_df.labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.words.iloc[278]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.words.iloc[879]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.words.iloc[165]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.words.iloc[532]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captum Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def captum_text_interpreter(idx, model, tokenizer, idx2label):\n",
    "    global attributions_ig, attributions, new_attributions, new_tokens\n",
    "    \n",
    "    bpe_initial = 'ƒ†'\n",
    "    text = val_df.words.iloc[idx]\n",
    "    true_label = idx2label[val_df.labels.iloc[idx]]\n",
    "    \n",
    "    d = {\n",
    "            \"ids\": torch.tensor([valX[0][idx,:]], dtype=torch.long),\n",
    "            \"mask\": torch.tensor([valX[1][idx,:]], dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor([valX[2][idx,:]], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    orig_tokens = tokenizer.tokenize(text,add_special_tokens=True)\n",
    "    \n",
    "    model.to(device)                              \n",
    "    model.eval()\n",
    "\n",
    "    preds_proba = torch.sigmoid(model(d[\"ids\"].to(device),d[\"mask\"].to(device),d[\"token_type_ids\"].to(device))).detach().cpu().numpy()\n",
    "    preds = np.round(preds_proba)\n",
    "    preds_proba = preds_proba[0][0]\n",
    "    predicted_class = idx2label[preds[0][0]]\n",
    "\n",
    "    lig = attr.LayerIntegratedGradients(model, model.base_model.embeddings)\n",
    "    \n",
    "    reference_indices = [0] + [1]*(d[\"ids\"].shape[1]-2) + [2]\n",
    "    reference_indices = torch.tensor([reference_indices], dtype=torch.long)\n",
    "    \n",
    "    attributions_ig, delta = lig.attribute(inputs=d[\"ids\"],baselines=reference_indices,additional_forward_args=(d[\"mask\"],d[\"token_type_ids\"]), \\\n",
    "                                           return_convergence_delta=True)\n",
    "    \n",
    "    attributions = attributions_ig.sum(dim=2).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.detach().cpu().numpy()\n",
    "    \n",
    "    attributions = attributions[1:]\n",
    "    \n",
    "    new_attributions = []\n",
    "    new_tokens = []\n",
    "    \n",
    "    for i, token in enumerate(orig_tokens):\n",
    "        if token[0] == bpe_initial:\n",
    "            tok_ = token[1:]\n",
    "            val = attributions[i]\n",
    "            \n",
    "            new_attributions.append(val)\n",
    "            new_tokens.append(tok_)\n",
    "            \n",
    "        else:\n",
    "            new_tokens[-1] = new_tokens[-1] + token\n",
    "            new_attributions[-1] = new_attributions[-1] + attributions[i]\n",
    "            \n",
    "\n",
    "    #orig_tokens = [i.replace(bpe_initial,'') for i in orig_tokens]\n",
    "    \n",
    "    attr.visualization.visualize_text([attr.visualization.VisualizationDataRecord(\n",
    "                            word_attributions=np.array(new_attributions), #attributions\n",
    "                            pred_prob=preds_proba,\n",
    "                            pred_class=predicted_class,\n",
    "                            true_class=true_label,\n",
    "                            attr_class=predicted_class,\n",
    "                            attr_score=np.array(new_attributions).sum(), #attributions\n",
    "                            raw_input=new_tokens, #orig_tokens\n",
    "                            convergence_score=delta)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>UNINFORMATIVE</b></text></td><td><text style=\"padding-right:2em\"><b>INFORMATIVE (0.94)</b></text></td><td><text style=\"padding-right:2em\"><b>INFORMATIVE</b></text></td><td><text style=\"padding-right:2em\"><b>0.78</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Interesting.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Could                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> been                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> covid19?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> If                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> China                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> october/november                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> are                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> chances                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> there                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(0, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 0                    </font></mark><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cases                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> here                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> until                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> end                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> january?                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>UNINFORMATIVE</b></text></td><td><text style=\"padding-right:2em\"><b>UNINFORMATIVE (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>UNINFORMATIVE</b></text></td><td><text style=\"padding-right:2em\"><b>-0.38</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Interesting.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Could                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> been                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> covid19?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> If                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> China                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> october/november                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> are                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> chances                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> there                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 0                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cases                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> here                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> until                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> end                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> january?                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "captum_text_interpreter(879, model1, tokenizer, idx2label)\n",
    "captum_text_interpreter(879, model3, tokenizer, idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>UNINFORMATIVE</b></text></td><td><text style=\"padding-right:2em\"><b>UNINFORMATIVE (0.45)</b></text></td><td><text style=\"padding-right:2em\"><b>UNINFORMATIVE</b></text></td><td><text style=\"padding-right:2em\"><b>0.92</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Interesting.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Could                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> been                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> covid19?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> If                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> China                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> october/november                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> are                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> chances                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> there                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 0                    </font></mark><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cases                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> here                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> until                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> end                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> january?                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "captum_text_interpreter(879, model2, tokenizer, idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>INFORMATIVE</b></text></td><td><text style=\"padding-right:2em\"><b>UNINFORMATIVE (0.35)</b></text></td><td><text style=\"padding-right:2em\"><b>UNINFORMATIVE</b></text></td><td><text style=\"padding-right:2em\"><b>0.22</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #kanikakapoor                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #Nagpur                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #IndianRailways                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 3                    </font></mark><mark style=\"background-color: hsl(0, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> above                    </font></mark><mark style=\"background-color: hsl(120, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cases                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> alone                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> enough                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #india                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> govt                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> declare                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stage                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 3                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lockdown                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 15                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> days.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> BJP                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> isn't                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> doing                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> so                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> immediately                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pr                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> exercise                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #JantaCurfew                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> will                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> junked.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Politics                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dire                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> times                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> √∞≈Å¬§¬¢                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #coronavirus                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #coronavirusindia                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>INFORMATIVE</b></text></td><td><text style=\"padding-right:2em\"><b>INFORMATIVE (0.90)</b></text></td><td><text style=\"padding-right:2em\"><b>INFORMATIVE</b></text></td><td><text style=\"padding-right:2em\"><b>1.89</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #kanikakapoor                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #Nagpur                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #IndianRailways                    </font></mark><mark style=\"background-color: hsl(0, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 3                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> above                    </font></mark><mark style=\"background-color: hsl(120, 75%, 74%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cases                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> alone                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> enough                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #india                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> govt                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> declare                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stage                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 3                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lockdown                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 15                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> days.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> BJP                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> isn't                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> doing                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> so                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> immediately                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pr                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> exercise                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #JantaCurfew                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> will                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> junked.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Politics                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dire                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> times                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> √∞≈Å¬§¬¢                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #coronavirus                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #coronavirusindia                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "captum_text_interpreter(278, model1, tokenizer, idx2label)\n",
    "captum_text_interpreter(278, model3, tokenizer, idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>INFORMATIVE</b></text></td><td><text style=\"padding-right:2em\"><b>UNINFORMATIVE (0.46)</b></text></td><td><text style=\"padding-right:2em\"><b>UNINFORMATIVE</b></text></td><td><text style=\"padding-right:2em\"><b>0.70</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #kanikakapoor                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #Nagpur                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #IndianRailways                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 3                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> above                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cases                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> alone                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> enough                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #india                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> govt                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> declare                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stage                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 3                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lockdown                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 15                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> days.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> BJP                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> isn't                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> doing                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> so                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> immediately                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pr                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> exercise                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #JantaCurfew                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> will                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> junked.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Politics                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dire                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> times                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> √∞≈Å¬§¬¢                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #coronavirus                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #coronavirusindia                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "captum_text_interpreter(278, model2, tokenizer, idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04297468564053351,\n",
       " 0.14935168532653287,\n",
       " 0.04450860246880199,\n",
       " -0.41225474401342277,\n",
       " -0.2792259049685253,\n",
       " 0.5331761509297077,\n",
       " -0.13535226610533044,\n",
       " 0.1360909600636808,\n",
       " 0.23910642032995175,\n",
       " 0.1108432946823692,\n",
       " -0.009269189662674086,\n",
       " 0.17625386095654305,\n",
       " 0.22879027736886975,\n",
       " -0.06967323421920713,\n",
       " 0.03951006844751535,\n",
       " 0.003022926719986401,\n",
       " 0.062044524628408294,\n",
       " 0.0052733864714256794,\n",
       " 0.16820273902646235,\n",
       " 0.18231092130237536,\n",
       " -0.009544090475070884,\n",
       " 0.07390177604085468,\n",
       " 0.03730748439971525,\n",
       " 0.0203775191660311,\n",
       " 0.10843857804164592,\n",
       " -0.013348231183350929,\n",
       " -0.0027390088926460736,\n",
       " -0.02539101666251659,\n",
       " 0.07135972513163756,\n",
       " 0.04868760709099575,\n",
       " 0.15014727429553287,\n",
       " -0.07435178163445069,\n",
       " -0.019267790627382892,\n",
       " 0.021252186426403617,\n",
       " 0.026691781837389017,\n",
       " 0.011799319688967231,\n",
       " 0.06929671311000904,\n",
       " 0.040276606065803175,\n",
       " 0.13589422000886542]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#kanikakapoor',\n",
       " '#Nagpur',\n",
       " '#IndianRailways',\n",
       " '3',\n",
       " 'above',\n",
       " 'cases',\n",
       " 'alone',\n",
       " 'enough',\n",
       " 'for',\n",
       " '#india',\n",
       " 'govt',\n",
       " 'to',\n",
       " 'declare',\n",
       " 'stage',\n",
       " '3',\n",
       " 'and',\n",
       " 'lockdown',\n",
       " 'for',\n",
       " '15',\n",
       " 'days.',\n",
       " 'BJP',\n",
       " \"isn't\",\n",
       " 'doing',\n",
       " 'so',\n",
       " 'immediately',\n",
       " 'as',\n",
       " 'pr',\n",
       " 'exercise',\n",
       " '#JantaCurfew',\n",
       " 'will',\n",
       " 'be',\n",
       " 'junked.',\n",
       " 'Politics',\n",
       " 'in',\n",
       " 'dire',\n",
       " 'times',\n",
       " '√∞≈Å¬§¬¢',\n",
       " '#coronavirus',\n",
       " '#coronavirusindia']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
