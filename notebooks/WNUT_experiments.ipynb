{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WNUT_experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPH7obXQkEgf69nhKYgD4n6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victor7246/WNUT-2020-Task-2/blob/master/WNUT_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOcMjrHZfz8A",
        "colab_type": "text"
      },
      "source": [
        "### Clone data and codes from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XETv9xcw4cYf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "478dec1b-f6dd-457c-f4b0-4f3d19d71b44"
      },
      "source": [
        "!git clone https://github.com/victor7246/WNUT-2020-Task-2.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'WNUT-2020-Task-2'...\n",
            "remote: Enumerating objects: 338, done.\u001b[K\n",
            "remote: Counting objects: 100% (338/338), done.\u001b[K\n",
            "remote: Compressing objects: 100% (211/211), done.\u001b[K\n",
            "remote: Total 338 (delta 160), reused 271 (delta 96), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (338/338), 29.64 MiB | 9.35 MiB/s, done.\n",
            "Resolving deltas: 100% (160/160), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZduKwRwt6rke",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9de6e8ac-48e9-4f6b-8fe3-266ef626f544"
      },
      "source": [
        "!git clone https://github.com/VinAIResearch/COVID19Tweet.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'COVID19Tweet'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 30 (delta 9), reused 14 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (30/30), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_ZLlK0F6zlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp COVID19Tweet/* WNUT-2020-Task-2/data/raw/COVID19Tweet/"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcrM9ngC-QyS",
        "colab_type": "text"
      },
      "source": [
        "### Installing Dependencies\n",
        "\n",
        "**Add .env to experiments folder for WANDB**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRYUyutx5Bni",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "977f6460-8dba-4f51-8a1a-9ac91a498393"
      },
      "source": [
        "!pip install -r WNUT-2020-Task-2/requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from -r WNUT-2020-Task-2/requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: Sphinx in /usr/local/lib/python3.6/dist-packages (from -r WNUT-2020-Task-2/requirements.txt (line 3)) (1.8.5)\n",
            "Requirement already satisfied: coverage in /usr/local/lib/python3.6/dist-packages (from -r WNUT-2020-Task-2/requirements.txt (line 4)) (3.7.1)\n",
            "Collecting flake8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/20/6326a9a0c6f0527612bae748c4c03df5cd69cf06dfb2cf59d85c6e165a6a/flake8-3.8.3-py2.py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 2.2MB/s \n",
            "\u001b[?25hCollecting python-dotenv\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/16/28d434b28c5be29a6af8fd0e3a2bda3bd30500ef0cd17bc79f7a6793a8d4/python_dotenv-0.14.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r WNUT-2020-Task-2/requirements.txt (line 7)) (1.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r WNUT-2020-Task-2/requirements.txt (line 8)) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r WNUT-2020-Task-2/requirements.txt (line 9)) (4.41.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from -r WNUT-2020-Task-2/requirements.txt (line 10)) (0.0)\n",
            "Collecting transformers==2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 7.3MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/af/2f10c8ee22d7a05fe8c9be58ad5c55b71ab4dd895b44f0156bfd5535a708/pytorch_lightning-0.9.0-py3-none-any.whl (408kB)\n",
            "\u001b[K     |████████████████████████████████| 409kB 16.6MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/cc/cbc171540f682a7ebcbe0ca8bcd0cb002e9dfaac0d10dba51717141c7c23/wandb-0.9.6-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 16.0MB/s \n",
            "\u001b[?25hCollecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ee/fedc3509145ad60fe5b418783f4a4c1b5462a4f0e8c7bbdbda52bdcda486/tokenizers-0.8.1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 30.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r WNUT-2020-Task-2/requirements.txt (line 15)) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r WNUT-2020-Task-2/requirements.txt (line 16)) (1.4.1)\n",
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/49/bf0ec241496a82c9dd2f0b6ff6f8156b6b2b72b849df8c00a4f2bcf61485/allennlp-1.0.0-py3-none-any.whl (473kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 40.3MB/s \n",
            "\u001b[?25hCollecting fairseq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/bf/de299e082e7af010d35162cb9a185dc6c17db71624590f2f379aeb2519ff/fairseq-0.9.0.tar.gz (306kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 39.6MB/s \n",
            "\u001b[?25hCollecting fastBPE\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/37/f97181428a5d151501b90b2cebedf97c81b034ace753606a3cda5ad4e6e2/fastBPE-0.1.0.tar.gz\n",
            "Collecting nlpaug\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/34/74123dd169accea4e2d79ebeeec55ac6fdc91212fe0bc9490dea0d5d9c3f/nlpaug-0.0.20-py3-none-any.whl (368kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 39.3MB/s \n",
            "\u001b[?25hCollecting textattack\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/84/29a32334c6cffca496511924e3f0ee6ff47282fba557979bf9c321adddc7/textattack-0.2.8-py3-none-any.whl (207kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 44.9MB/s \n",
            "\u001b[?25hCollecting pandas-bokeh\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/35/63225d72335661633de44d7da7a28be327a52f9cb0973d9119119fa6752f/pandas_bokeh-0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from -r WNUT-2020-Task-2/requirements.txt (line 23)) (2.2.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from -r WNUT-2020-Task-2/requirements.txt (line 24)) (3.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r WNUT-2020-Task-2/requirements.txt (line 25)) (3.2.2)\n",
            "Collecting pyLDAvis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 39.2MB/s \n",
            "\u001b[?25hCollecting captum\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/de/c018e206d463d9975444c28b0a4f103c9ca4b2faedf943df727e402a1a1e/captum-0.2.0-py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 41.0MB/s \n",
            "\u001b[?25hCollecting eli5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/2f/c85c7d8f8548e460829971785347e14e45fa5c6617da374711dec8cb38cc/eli5-0.10.1-py2.py3-none-any.whl (105kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 43.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from -r WNUT-2020-Task-2/requirements.txt (line 29)) (1.6.0+cu101)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 44.2MB/s \n",
            "\u001b[?25hCollecting subword_nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.6/dist-packages (from -r WNUT-2020-Task-2/requirements.txt (line 32)) (0.8.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r WNUT-2020-Task-2/requirements.txt (line 33)) (2.3.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r WNUT-2020-Task-2/requirements.txt (line 3)) (1.2.4)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r WNUT-2020-Task-2/requirements.txt (line 3)) (2.8.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r WNUT-2020-Task-2/requirements.txt (line 3)) (2.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r WNUT-2020-Task-2/requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r WNUT-2020-Task-2/requirements.txt (line 3)) (20.4)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r WNUT-2020-Task-2/requirements.txt (line 3)) (2.11.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r WNUT-2020-Task-2/requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r WNUT-2020-Task-2/requirements.txt (line 3)) (49.6.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r WNUT-2020-Task-2/requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r WNUT-2020-Task-2/requirements.txt (line 3)) (2.0.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r WNUT-2020-Task-2/requirements.txt (line 3)) (0.7.12)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r WNUT-2020-Task-2/requirements.txt (line 3)) (0.15.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from flake8->-r WNUT-2020-Task-2/requirements.txt (line 5)) (1.7.0)\n",
            "Collecting pycodestyle<2.7.0,>=2.6.0a1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/5b/88879fb861ab79aef45c7e199cae3ef7af487b5603dcb363517a50602dd7/pycodestyle-2.6.0-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.9MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7.0,>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Collecting pyflakes<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/5b/fd01b0c696f2f9a6d2c839883b642493b431f28fa32b29abc465ef675473/pyflakes-2.2.0-py2.py3-none-any.whl (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->-r WNUT-2020-Task-2/requirements.txt (line 7)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r WNUT-2020-Task-2/requirements.txt (line 7)) (2018.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->-r WNUT-2020-Task-2/requirements.txt (line 11)) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->-r WNUT-2020-Task-2/requirements.txt (line 11)) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->-r WNUT-2020-Task-2/requirements.txt (line 11)) (1.14.48)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->-r WNUT-2020-Task-2/requirements.txt (line 11)) (0.7)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 42.9MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 44.3MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 42.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb->-r WNUT-2020-Task-2/requirements.txt (line 13)) (5.4.8)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/4b/317ff319141f105e43705e5883b7a8c61c8eeedcf01c53deca9182d44ee8/sentry_sdk-0.17.2-py2.py3-none-any.whl (115kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 41.9MB/s \n",
            "\u001b[?25hCollecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/06/121302598a4fc01aca942d937f4a2c33430b7181137b35758913a8db10ad/watchdog-0.10.3.tar.gz (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.6MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/1e/a45320cab182bf1c8656107b3d4c042e659742822fc6bff150d769a984dd/GitPython-3.1.7-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 38.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb->-r WNUT-2020-Task-2/requirements.txt (line 13)) (7.352.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Collecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r WNUT-2020-Task-2/requirements.txt (line 15)) (0.16.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp->-r WNUT-2020-Task-2/requirements.txt (line 17)) (3.2.5)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp->-r WNUT-2020-Task-2/requirements.txt (line 17)) (3.6.4)\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 42.4MB/s \n",
            "\u001b[?25hCollecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/70/ed1ba808a87d896b9f4d25400dda54e089ca7a97e87cee620b3744997c89/jsonnet-0.16.0.tar.gz (256kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp->-r WNUT-2020-Task-2/requirements.txt (line 17)) (2.10.0)\n",
            "Collecting overrides==3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/42/8d/caa729f809ecdf8e76fac3c1ff7d3f0b72c398c9dd8a6919927a30a873b3/overrides-3.0.0.tar.gz\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/af/ca/4fee219cc4113a5635e348ad951cf8a2e47fed2e3342312493f5b73d0007/jsonpickle-1.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq->-r WNUT-2020-Task-2/requirements.txt (line 18)) (1.14.2)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq->-r WNUT-2020-Task-2/requirements.txt (line 18)) (0.29.21)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/d3/be980ad7cda7c4bbfa97ee3de062fb3014fc1a34d6dd5b82d7b92f8d6522/sacrebleu-1.4.13-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n",
            "\u001b[?25hCollecting lemminflect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/c5/62e8dd0b6cbfea212cf55a2338838d85a819dbda9462ba53a415dcf19b86/lemminflect-0.2.1-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 42.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2 in /usr/local/lib/python3.6/dist-packages (from textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (2.3.0)\n",
            "Collecting bert-score\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/8c/70244d0f351176c9984643e2037edc18b9124491b47ad19191039a18d2bd/bert_score-0.3.5-py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.0MB/s \n",
            "\u001b[?25hCollecting nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/e3/bcdc59f3434b224040c1047769c47b82705feca2b89ebbc28311e3764782/nlp-0.4.0-py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 38.2MB/s \n",
            "\u001b[?25hCollecting lru-dict\n",
            "  Downloading https://files.pythonhosted.org/packages/00/a5/32ed6e10246cd341ca8cc205acea5d208e4053f48a4dced2b1b31d45ba3f/lru-dict-1.1.6.tar.gz\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (from textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (0.9.0)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting flair>=0.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/82/898d26ae4c7a8c2cb51dfc776c7b323b049981a08300d811e18ac12825a8/flair-0.6.0.post1-py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 43.0MB/s \n",
            "\u001b[?25hCollecting language-tool-python\n",
            "  Downloading https://files.pythonhosted.org/packages/98/82/04522172b2e0bcb1a65ea13b8bd69e2c03331f8e69885369c551f2964db5/language_tool_python-2.4.0-py3-none-any.whl\n",
            "Collecting visdom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/75/e078f5a2e1df7e0d3044749089fc2823e62d029cc027ed8ae5d71fafcbdc/visdom-0.1.8.9.tar.gz (676kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 45.1MB/s \n",
            "\u001b[?25hCollecting sentence-transformers>0.2.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/77/3d68869a5967c54b36c829f41b8380fc7d0d9611a53ec90136dbc6386402/sentence-transformers-0.3.5.tar.gz (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: bokeh>=2.0 in /usr/local/lib/python3.6/dist-packages (from pandas-bokeh->-r WNUT-2020-Task-2/requirements.txt (line 22)) (2.1.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r WNUT-2020-Task-2/requirements.txt (line 23)) (0.7.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r WNUT-2020-Task-2/requirements.txt (line 23)) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->-r WNUT-2020-Task-2/requirements.txt (line 23)) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->-r WNUT-2020-Task-2/requirements.txt (line 23)) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r WNUT-2020-Task-2/requirements.txt (line 23)) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r WNUT-2020-Task-2/requirements.txt (line 23)) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r WNUT-2020-Task-2/requirements.txt (line 23)) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r WNUT-2020-Task-2/requirements.txt (line 23)) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r WNUT-2020-Task-2/requirements.txt (line 23)) (2.0.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->-r WNUT-2020-Task-2/requirements.txt (line 24)) (2.1.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r WNUT-2020-Task-2/requirements.txt (line 25)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r WNUT-2020-Task-2/requirements.txt (line 25)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r WNUT-2020-Task-2/requirements.txt (line 25)) (2.4.7)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis->-r WNUT-2020-Task-2/requirements.txt (line 26)) (0.35.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis->-r WNUT-2020-Task-2/requirements.txt (line 26)) (2.7.1)\n",
            "Collecting funcy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/4b/6ffa76544e46614123de31574ad95758c421aae391a1764921b8a81e1eae/funcy-1.14.tar.gz (548kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 41.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from eli5->-r WNUT-2020-Task-2/requirements.txt (line 28)) (0.10.1)\n",
            "Requirement already satisfied: attrs>16.0.0 in /usr/local/lib/python3.6/dist-packages (from eli5->-r WNUT-2020-Task-2/requirements.txt (line 28)) (20.1.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from eli5->-r WNUT-2020-Task-2/requirements.txt (line 28)) (0.8.7)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow_addons->-r WNUT-2020-Task-2/requirements.txt (line 32)) (2.7.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r WNUT-2020-Task-2/requirements.txt (line 33)) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r WNUT-2020-Task-2/requirements.txt (line 33)) (3.12.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r WNUT-2020-Task-2/requirements.txt (line 33)) (3.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r WNUT-2020-Task-2/requirements.txt (line 33)) (1.17.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r WNUT-2020-Task-2/requirements.txt (line 33)) (1.31.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r WNUT-2020-Task-2/requirements.txt (line 33)) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r WNUT-2020-Task-2/requirements.txt (line 33)) (0.4.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r WNUT-2020-Task-2/requirements.txt (line 33)) (0.8.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.6/dist-packages (from sphinxcontrib-websupport->Sphinx->-r WNUT-2020-Task-2/requirements.txt (line 3)) (1.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->Sphinx->-r WNUT-2020-Task-2/requirements.txt (line 3)) (1.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->Sphinx->-r WNUT-2020-Task-2/requirements.txt (line 3)) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->Sphinx->-r WNUT-2020-Task-2/requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->Sphinx->-r WNUT-2020-Task-2/requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->Sphinx->-r WNUT-2020-Task-2/requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->flake8->-r WNUT-2020-Task-2/requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.48 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0->-r WNUT-2020-Task-2/requirements.txt (line 11)) (1.17.48)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0->-r WNUT-2020-Task-2/requirements.txt (line 11)) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0->-r WNUT-2020-Task-2/requirements.txt (line 11)) (0.3.3)\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.4MB/s \n",
            "\u001b[?25hCollecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb->-r WNUT-2020-Task-2/requirements.txt (line 13)) (2.3)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp->-r WNUT-2020-Task-2/requirements.txt (line 17)) (1.9.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp->-r WNUT-2020-Task-2/requirements.txt (line 17)) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp->-r WNUT-2020-Task-2/requirements.txt (line 17)) (8.4.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp->-r WNUT-2020-Task-2/requirements.txt (line 17)) (1.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq->-r WNUT-2020-Task-2/requirements.txt (line 18)) (2.20)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2->textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2->textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2->textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (3.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2->textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2->textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2->textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (2.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2->textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2->textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (1.12.1)\n",
            "Collecting pyarrow>=0.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/99/0a605f016121ca314d1469dc9069e4978395bc46fda40f73099d90ad3ba4/pyarrow-1.0.1-cp36-cp36m-manylinux2014_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 198kB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp->textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (0.3.2)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 42.9MB/s \n",
            "\u001b[?25hCollecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 38.6MB/s \n",
            "\u001b[?25hCollecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/4b/f29cb8cf226d49b14f0d18bd175916dd0054dc66e42488f2c808075ef5a8/konoha-4.6.1-py3-none-any.whl\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from flair>=0.5.1->textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (4.2.6)\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/91/77/3f0f53856e86af32b1d3c86652815277f7b5f880002584eb30db115b6df5/bpemb-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair>=0.5.1->textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (0.1.2)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/76/a1/05d7f62f956d77b23a640efc650f80ce24483aa2f85a09c03fb64f49e879/Deprecated-1.2.10-py2.py3-none-any.whl\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.4MB/s \n",
            "\u001b[?25hCollecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 37.2MB/s \n",
            "\u001b[?25hCollecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/ed/b2b072c1d53388931e626ca0816cdb8e03416ad9a440c8d9e0e060859f41/Janome-0.4.0-py2.py3-none-any.whl (19.7MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7MB 160kB/s \n",
            "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from visdom->textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (5.1.1)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from visdom->textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (19.0.2)\n",
            "Collecting jsonpatch\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/d0/34b0f59ac08de9c1e07876cfecd80aec650600177b4bd445124c755499a7/jsonpatch-1.26-py2.py3-none-any.whl\n",
            "Collecting torchfile\n",
            "  Downloading https://files.pythonhosted.org/packages/91/af/5b305f86f2d218091af657ddb53f984ecbd9518ca9fe8ef4103a007252c9/torchfile-0.1.0.tar.gz\n",
            "Collecting websocket-client\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 41.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from visdom->textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (7.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from bokeh>=2.0->pandas-bokeh->-r WNUT-2020-Task-2/requirements.txt (line 22)) (3.7.4.3)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->-r WNUT-2020-Task-2/requirements.txt (line 24)) (2.49.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r WNUT-2020-Task-2/requirements.txt (line 33)) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r WNUT-2020-Task-2/requirements.txt (line 33)) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r WNUT-2020-Task-2/requirements.txt (line 33)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r WNUT-2020-Task-2/requirements.txt (line 33)) (1.3.0)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair>=0.5.1->textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (3.11.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair>=0.5.1->textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (2.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->flair>=0.5.1->textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (0.2.5)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading https://files.pythonhosted.org/packages/18/b0/a80d29577c08eea401659254dfaed87f1af45272899e1812d7e01b679bc5/jsonpointer-2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard->-r WNUT-2020-Task-2/requirements.txt (line 33)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r WNUT-2020-Task-2/requirements.txt (line 33)) (3.1.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair>=0.5.1->textattack->-r WNUT-2020-Task-2/requirements.txt (line 21)) (4.4.2)\n",
            "Building wheels for collected packages: fairseq, fastBPE, pyLDAvis, sacremoses, PyYAML, future, watchdog, gql, subprocess32, jsonnet, overrides, lru-dict, terminaltables, visdom, sentence-transformers, funcy, pathtools, graphql-core, segtok, langdetect, ftfy, mpld3, sqlitedict, torchfile\n",
            "  Building wheel for fairseq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.9.0-cp36-cp36m-linux_x86_64.whl size=2046363 sha256=ad718c1e3ab5c1801c0505924f87518016e2292f3ad86b8971821075d4dc126a\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/3e/1b/0fa30695dcba41e4b0088067fa40f3328d1e8ee78c22cd4766\n",
            "  Building wheel for fastBPE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastBPE: filename=fastBPE-0.1.0-cp36-cp36m-linux_x86_64.whl size=481495 sha256=ba8e831a23b1665eb545c30a29c5f95dd6e05721ab47babf462b78847e2d34dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/0c/9c/fc62058b4d473a5602bcd3d3edfece796f123875379ea82d79\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97712 sha256=9f1004d5b2a4229d87db233c315c12ec2142c6bd905e018bd2713509db109f4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=18161bddf52d8a7b6d7e349cb8e5a75d713a79cb922f0c0f41f0a3f81a416708\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=fd22a95700a69653707ac352038b96175ad9ad423194a82d2415108f096345f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=5d458207ee3b78ea06199fa9a1b3317b33140942e7cd04cf78fb062c6b54859f\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.3-cp36-none-any.whl size=73873 sha256=f786cf85461bf605f953f4d295a1a81d3399d235599786a94026b2fe0c9a6bdc\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/1d/38/2c19bb311f67cc7b4d07a2ec5ea36ab1a0a0ea50db994a5bc7\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=041ba2b92aae6b23622c24625b4cfe3f7162f790f4884eb1005f2001ac862ca7\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=6c63def17cc277b86fed4bc3ca659ef50224b2196e623b6dfdcc53b77c8ce803\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.16.0-cp36-cp36m-linux_x86_64.whl size=3321587 sha256=14fc2880233af9fe4bf727d965dcaa08e4905a1d837e5b18f37ac34fcf287e64\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/a9/43/bc5e0463deeec89dfca928a2a64595f1bdb520c891f6fbd09c\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.0.0-cp36-none-any.whl size=5669 sha256=b69ed914822bbd4e28a3a4a0a374914b0b66400dc76ede6ac0eeb809e6d7242c\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/1b/ec/6c71a1eb823df7f850d956b2d8c50a6d49c191e1063d73b9be\n",
            "  Building wheel for lru-dict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lru-dict: filename=lru_dict-1.1.6-cp36-cp36m-linux_x86_64.whl size=25887 sha256=5bd62a4eacd6ba7b77728c6bf7c74c4a108859215675924aedf846d49eb022dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/ef/06/fbdd555907a7d438fb33e4c8675f771ff1cf41917284c51ebf\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15356 sha256=c96fc038a51d1ebf4d11a626d7567e8503f839279b4be24a148ba617063ee93c\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.9-cp36-none-any.whl size=655251 sha256=08cc4d41c5ab779c2861038ed84f89c92e70b512a5c704ee1bbe9b7d582206ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/19/a7/6d589ed967f4dfefd33bc166d081257bd4ed0cb618dccfd62a\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.5-cp36-none-any.whl size=100380 sha256=63cea9c8de09f09eeb92e2fcff9be074b768bccce2fc7e3cd8789c888d9aca0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/26/5e/4a2e30ae699cdb89cc83ee7fe46f7352371a23b769474cd898\n",
            "  Building wheel for funcy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for funcy: filename=funcy-1.14-py2.py3-none-any.whl size=32042 sha256=bd5f3cef4bfb041379e40f6407f608e9286d122f3afb443bc3e5c409c3c4637d\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/5a/d8/1d875df03deae6f178dfdf70238cca33f948ef8a6f5209f2eb\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8785 sha256=0f8727602db24cd24a244f12aac4d8415b033864be9ae9a1c4e249b3f5af8dc0\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104651 sha256=b428155547f668f3f6cae599a32c253a26ddde3a85133ce2cc365088027b91a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp36-none-any.whl size=25021 sha256=35d176405ebbbe7696db6bf6bc4371dd50a0f13261bbaa2fd02183c76342a811\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993195 sha256=5b0ebfcec86692b7a0a4bd08bc601057b3e1d607911331a7e429e3d4174555c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45612 sha256=4d9c53243e6c9e854e2d50b58d75390a18577b931afe1728926ff255e74b42e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116677 sha256=61dd0ff28536b5d0757bcd379634d468211b5e2972bce56e6659b37e46916a1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.6.0-cp36-none-any.whl size=14689 sha256=6e4d4ee75b2988bb954b2c1bb48f0aa609c08d3f55e81bb294721bf263181708\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-cp36-none-any.whl size=5711 sha256=be6b49f1cd7ec2f56e2161f8392d83abd82c949b138148fae1e8fff3cbad701d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/c3/d6/9a1cc8f3a99a0fc1124cae20153f36af59a6e683daca0a0814\n",
            "Successfully built fairseq fastBPE pyLDAvis sacremoses PyYAML future watchdog gql subprocess32 jsonnet overrides lru-dict terminaltables visdom sentence-transformers funcy pathtools graphql-core segtok langdetect ftfy mpld3 sqlitedict torchfile\n",
            "\u001b[31mERROR: transformers 2.8.0 has requirement tokenizers==0.5.2, but you'll have tokenizers 0.8.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pytorch-lightning 0.9.0 has requirement tensorboard==2.2.0, but you'll have tensorboard 2.3.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: allennlp 1.0.0 has requirement torch<1.6.0,>=1.5.0, but you'll have torch 1.6.0+cu101 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: allennlp 1.0.0 has requirement transformers<2.12,>=2.9, but you'll have transformers 2.8.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: bert-score 0.3.5 has requirement transformers>=3.0.0, but you'll have transformers 2.8.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: flair 0.6.0.post1 has requirement pytest>=5.3.2, but you'll have pytest 3.6.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: flair 0.6.0.post1 has requirement transformers>=3.0.0, but you'll have transformers 2.8.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: sentence-transformers 0.3.5 has requirement transformers==3.0.2, but you'll have transformers 2.8.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: textattack 0.2.8 has requirement tokenizers==0.8.1-rc1, but you'll have tokenizers 0.8.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: textattack 0.2.8 has requirement transformers==3.0.2, but you'll have transformers 2.8.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pycodestyle, mccabe, pyflakes, flake8, python-dotenv, sacremoses, tokenizers, sentencepiece, transformers, PyYAML, future, pytorch-lightning, sentry-sdk, pathtools, watchdog, docker-pycreds, smmap, gitdb, GitPython, shortuuid, configparser, graphql-core, gql, subprocess32, wandb, tensorboardX, jsonnet, overrides, jsonpickle, allennlp, portalocker, sacrebleu, fairseq, fastBPE, nlpaug, lemminflect, bert-score, pyarrow, xxhash, nlp, lru-dict, terminaltables, segtok, langdetect, konoha, bpemb, deprecated, ftfy, mpld3, janome, sqlitedict, flair, language-tool-python, jsonpointer, jsonpatch, torchfile, websocket-client, visdom, sentence-transformers, textattack, pandas-bokeh, funcy, pyLDAvis, captum, eli5, subword-nmt\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed GitPython-3.1.7 PyYAML-5.3.1 allennlp-1.0.0 bert-score-0.3.5 bpemb-0.3.2 captum-0.2.0 configparser-5.0.0 deprecated-1.2.10 docker-pycreds-0.4.0 eli5-0.10.1 fairseq-0.9.0 fastBPE-0.1.0 flair-0.6.0.post1 flake8-3.8.3 ftfy-5.8 funcy-1.14 future-0.18.2 gitdb-4.0.5 gql-0.2.0 graphql-core-1.1 janome-0.4.0 jsonnet-0.16.0 jsonpatch-1.26 jsonpickle-1.4.1 jsonpointer-2.0 konoha-4.6.1 langdetect-1.0.8 language-tool-python-2.4.0 lemminflect-0.2.1 lru-dict-1.1.6 mccabe-0.6.1 mpld3-0.3 nlp-0.4.0 nlpaug-0.0.20 overrides-3.0.0 pandas-bokeh-0.5 pathtools-0.1.2 portalocker-2.0.0 pyLDAvis-2.1.2 pyarrow-1.0.1 pycodestyle-2.6.0 pyflakes-2.2.0 python-dotenv-0.14.0 pytorch-lightning-0.9.0 sacrebleu-1.4.13 sacremoses-0.0.43 segtok-1.5.10 sentence-transformers-0.3.5 sentencepiece-0.1.91 sentry-sdk-0.17.2 shortuuid-1.0.1 smmap-3.0.4 sqlitedict-1.6.0 subprocess32-3.5.4 subword-nmt-0.3.7 tensorboardX-2.1 terminaltables-3.1.0 textattack-0.2.8 tokenizers-0.8.1 torchfile-0.1.0 transformers-2.8.0 visdom-0.1.8.9 wandb-0.9.6 watchdog-0.10.3 websocket-client-0.57.0 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr6HiOltgAvn",
        "colab_type": "text"
      },
      "source": [
        "### Mount Drive to store models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He6ARQ4r4ttq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "999b9892-397d-486f-cb8f-9b5a136e3552"
      },
      "source": [
        "from google.colab import files, drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0WtXpADddCN",
        "colab_type": "text"
      },
      "source": [
        "### Keras baseline models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyLlQtsjdgO7",
        "colab_type": "text"
      },
      "source": [
        "#### RoBERTa-base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDPNDlTOMNAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir =  '/content/drive/My\\ Drive/Models/WNUT-Task2/model3/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRZ3wywa6AqL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "76f9f65d-29e1-45da-ee80-41f7eb3e150c"
      },
      "source": [
        "!cd WNUT-2020-Task-2/experiments/ && python exp3.py --model_save_path {model_dir}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-08-28 08:32:32.883824: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "100% 7000/7000 [00:05<00:00, 1353.67it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1509.78it/s]\n",
            "Modelling\n",
            "2020-08-28 08:32:44.407019: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-28 08:32:44.431065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 08:32:44.431833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-28 08:32:44.431894: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 08:32:44.432020: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-28 08:32:44.432100: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-28 08:32:44.432169: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-28 08:32:44.432239: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-28 08:32:44.432316: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-28 08:32:44.432406: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-28 08:32:44.432578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 08:32:44.433412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 08:32:44.434130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-28 08:32:44.441905: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-08-28 08:32:44.442206: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8271c00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-28 08:32:44.442239: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-28 08:32:44.501391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 08:32:44.502270: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x9629b80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-28 08:32:44.502304: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2020-08-28 08:32:44.502551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 08:32:44.503297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-28 08:32:44.503370: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 08:32:44.503434: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-28 08:32:44.503480: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-28 08:32:44.503529: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-28 08:32:44.503579: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-28 08:32:44.503631: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-28 08:32:44.503682: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-28 08:32:44.503834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 08:32:44.504668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 08:32:44.505367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-28 08:32:44.505437: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 08:32:45.020155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-28 08:32:45.020223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-28 08:32:45.020243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-28 08:32:45.020531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 08:32:45.021396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 08:32:45.022101: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-28 08:32:45.022157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10628 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2020-08-28 08:32:45.268906: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_roberta_model (TFRobertaMode ((None, 100, 768), ( 124645632   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 768)          0           tf_roberta_model[0][1]           \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            769         dropout_38[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 124,646,401\n",
            "Trainable params: 124,646,401\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Evaluation\n",
            "description    roberta-base\n",
            "f1                 0.908722\n",
            "precision          0.871595\n",
            "recall             0.949153\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaVT3HP9dkOu",
        "colab_type": "text"
      },
      "source": [
        "#### BERT-base-cased"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ_BHdDehiEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir =  '/content/drive/My\\ Drive/Models/WNUT-Task2/model3_2/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QA5tlE-hjiu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c8aa309b-eb88-4bdd-e1a6-dbd1e0c14eb6"
      },
      "source": [
        "!cd WNUT-2020-Task-2/experiments/ && python exp3.py --model_save_path {model_dir} --transformer_model_name bert-base-cased"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-30 13:24:01.411402: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "Downloading: 100% 433/433 [00:00<00:00, 132kB/s]\n",
            "Downloading: 100% 213k/213k [00:00<00:00, 848kB/s] \n",
            "100% 7000/7000 [00:06<00:00, 1149.69it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1175.19it/s]\n",
            "Modelling\n",
            "Downloading: 100% 527M/527M [00:16<00:00, 31.6MB/s]\n",
            "2020-08-30 13:24:36.693298: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-30 13:24:36.715547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 13:24:36.716446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-30 13:24:36.716498: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-30 13:24:36.716611: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-30 13:24:36.716689: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-30 13:24:36.716775: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-30 13:24:36.716859: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-30 13:24:36.716932: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-30 13:24:36.717015: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-30 13:24:36.717225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 13:24:36.717996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 13:24:36.718770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-30 13:24:36.738720: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-08-30 13:24:36.738963: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x9125d40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-30 13:24:36.738999: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-30 13:24:36.844048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 13:24:36.844950: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x91252c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-30 13:24:36.844988: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2020-08-30 13:24:36.845337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 13:24:36.846049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-30 13:24:36.846119: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-30 13:24:36.846221: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-30 13:24:36.846273: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-30 13:24:36.846324: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-30 13:24:36.846394: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-30 13:24:36.846449: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-30 13:24:36.846503: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-30 13:24:36.846660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 13:24:36.847518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 13:24:36.848288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-30 13:24:36.853104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-30 13:24:49.883334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-30 13:24:49.883421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-30 13:24:49.883443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-30 13:24:49.888431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 13:24:49.889325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 13:24:49.890026: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-30 13:24:49.890087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10628 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2020-08-30 13:24:50.214425: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model (TFBertModel)     ((None, 100, 768), ( 108310272   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 768)          0           tf_bert_model[0][1]              \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            769         dropout_37[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 108,311,041\n",
            "Trainable params: 108,311,041\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3328 - accuracy: 0.8421\n",
            "Score 0.8563049853372434. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model3_2/model.h5.\n",
            "219/219 [==============================] - 270s 1s/step - loss: 0.3328 - accuracy: 0.8421 - val_loss: 0.3988 - val_accuracy: 0.8530\n",
            "Epoch 2/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1178 - accuracy: 0.9589\n",
            "Score 0.8771186440677965. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model3_2/model.h5.\n",
            "219/219 [==============================] - 264s 1s/step - loss: 0.1178 - accuracy: 0.9589 - val_loss: 0.3155 - val_accuracy: 0.8840\n",
            "Epoch 3/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9839\n",
            "Score 0.8819742489270386. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model3_2/model.h5.\n",
            "219/219 [==============================] - 264s 1s/step - loss: 0.0484 - accuracy: 0.9839 - val_loss: 0.3510 - val_accuracy: 0.8900\n",
            "Epoch 4/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9936\n",
            "Score 0.8653846153846154. Model not saved.\n",
            "219/219 [==============================] - 262s 1s/step - loss: 0.0201 - accuracy: 0.9936 - val_loss: 0.4356 - val_accuracy: 0.8740\n",
            "Epoch 5/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9939\n",
            "Score 0.8718466195761857. Model not saved.\n",
            "219/219 [==============================] - 262s 1s/step - loss: 0.0178 - accuracy: 0.9939 - val_loss: 0.6800 - val_accuracy: 0.8730\n",
            "Epoch 6/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9969\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.3999999646330251e-05.\n",
            "\n",
            "Score 0.8651452282157676. Model not saved.\n",
            "219/219 [==============================] - 262s 1s/step - loss: 0.0120 - accuracy: 0.9969 - val_loss: 0.7215 - val_accuracy: 0.8700\n",
            "Epoch 7/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0043 - accuracy: 0.9986\n",
            "Score 0.8797533401849948. Model not saved.\n",
            "219/219 [==============================] - 262s 1s/step - loss: 0.0043 - accuracy: 0.9986 - val_loss: 0.7418 - val_accuracy: 0.8830\n",
            "Epoch 8/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 0.9990Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Score 0.8819742489270386. Model not saved.\n",
            "\n",
            "Epoch 7: early stopping.\n",
            "219/219 [==============================] - 262s 1s/step - loss: 0.0026 - accuracy: 0.9990 - val_loss: 0.7081 - val_accuracy: 0.8780\n",
            "Epoch 00008: early stopping\n",
            "Evaluation\n",
            "description    bert-base-cased\n",
            "f1                    0.881974\n",
            "precision             0.893478\n",
            "recall                0.870763\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk6pgUXDduvM",
        "colab_type": "text"
      },
      "source": [
        "#### XLNet-base-cased"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3wsHJkohpED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir =  '/content/drive/My\\ Drive/Models/WNUT-Task2/model3_3/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VqKmnSjhqB-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3fd8bb32-6529-45e5-c054-c4393ca15f9c"
      },
      "source": [
        "!cd WNUT-2020-Task-2/experiments/ && python exp3.py --model_save_path {model_dir} --transformer_model_name xlnet-base-cased"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-30 14:03:50.741721: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "Downloading: 100% 760/760 [00:00<00:00, 524kB/s]\n",
            "Downloading: 100% 798k/798k [00:00<00:00, 1.90MB/s]\n",
            "100% 7000/7000 [00:02<00:00, 2896.89it/s]\n",
            "100% 1000/1000 [00:00<00:00, 3190.89it/s]\n",
            "Modelling\n",
            "Downloading: 100% 565M/565M [00:20<00:00, 27.5MB/s]\n",
            "2020-08-30 14:04:21.077469: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-30 14:04:21.100402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 14:04:21.101225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-30 14:04:21.101289: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-30 14:04:21.101398: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-30 14:04:21.101465: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-30 14:04:21.101550: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-30 14:04:21.101645: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-30 14:04:21.101710: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-30 14:04:21.101779: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-30 14:04:21.101969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 14:04:21.102824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 14:04:21.103576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-30 14:04:21.109574: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-08-30 14:04:21.109806: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8d0dd40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-30 14:04:21.109853: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-30 14:04:21.165315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 14:04:21.166133: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8d0d2c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-30 14:04:21.166187: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2020-08-30 14:04:21.166445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 14:04:21.167149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-30 14:04:21.167279: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-30 14:04:21.167349: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-30 14:04:21.167409: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-30 14:04:21.167458: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-30 14:04:21.167508: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-30 14:04:21.167553: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-30 14:04:21.167605: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-30 14:04:21.167755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 14:04:21.168581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 14:04:21.169308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-30 14:04:21.169384: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-30 14:04:21.682204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-30 14:04:21.682280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-30 14:04:21.682307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-30 14:04:21.682675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 14:04:21.683503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 14:04:21.684224: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-30 14:04:21.684294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10628 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2020-08-30 14:04:21.831617: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tfxl_net_model (TFXLNetModel)   ((None, 100, 768), ( 116718336   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [(None, 768)]        0           tfxl_net_model[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 768)          0           tf_op_layer_strided_slice[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            769         dropout_37[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 116,719,105\n",
            "Trainable params: 116,719,105\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss.\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.4250 - accuracy: 0.8269\n",
            "Score 0.8756121449559257. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model3_3/model.h5.\n",
            "219/219 [==============================] - 364s 2s/step - loss: 0.4250 - accuracy: 0.8269 - val_loss: 0.3651 - val_accuracy: 0.8730\n",
            "Epoch 2/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1342 - accuracy: 0.9531\n",
            "Score 0.8736842105263158. Model not saved.\n",
            "219/219 [==============================] - 358s 2s/step - loss: 0.1342 - accuracy: 0.9531 - val_loss: 0.3247 - val_accuracy: 0.8800\n",
            "Epoch 3/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9730\n",
            "Score 0.885593220338983. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model3_3/model.h5.\n",
            "219/219 [==============================] - 361s 2s/step - loss: 0.0787 - accuracy: 0.9730 - val_loss: 0.3374 - val_accuracy: 0.8920\n",
            "Epoch 4/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9873\n",
            "Score 0.8725274725274725. Model not saved.\n",
            "219/219 [==============================] - 358s 2s/step - loss: 0.0410 - accuracy: 0.9873 - val_loss: 0.4144 - val_accuracy: 0.8840\n",
            "Epoch 5/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9884\n",
            "Score 0.8941641938674579. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model3_3/model.h5.\n",
            "219/219 [==============================] - 360s 2s/step - loss: 0.0323 - accuracy: 0.9884 - val_loss: 0.6180 - val_accuracy: 0.8930\n",
            "Epoch 6/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9940\n",
            "Score 0.8844884488448844. Model not saved.\n",
            "219/219 [==============================] - 358s 2s/step - loss: 0.0219 - accuracy: 0.9940 - val_loss: 0.4428 - val_accuracy: 0.8950\n",
            "Epoch 7/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9927\n",
            "Score 0.8846960167714883. Model not saved.\n",
            "219/219 [==============================] - 358s 2s/step - loss: 0.0247 - accuracy: 0.9927 - val_loss: 0.5737 - val_accuracy: 0.8900\n",
            "Epoch 8/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9953\n",
            "Score 0.8813209494324046. Model not saved.\n",
            "219/219 [==============================] - 357s 2s/step - loss: 0.0116 - accuracy: 0.9953 - val_loss: 0.6550 - val_accuracy: 0.8850\n",
            "Epoch 9/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9940\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.3999999646330251e-05.\n",
            "\n",
            "Score 0.8911917098445595. Model not saved.\n",
            "219/219 [==============================] - 358s 2s/step - loss: 0.0171 - accuracy: 0.9940 - val_loss: 0.5233 - val_accuracy: 0.8950\n",
            "Epoch 10/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0044 - accuracy: 0.9986\n",
            "Score 0.8884254431699687. Model not saved.\n",
            "\n",
            "Epoch 9: early stopping.\n",
            "219/219 [==============================] - 357s 2s/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.6956 - val_accuracy: 0.8930\n",
            "Evaluation\n",
            "description    xlnet-base-cased\n",
            "f1                     0.894164\n",
            "precision               0.83859\n",
            "recall                 0.957627\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJpw7YTJdzF7",
        "colab_type": "text"
      },
      "source": [
        "#### ALBERT-base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9oNUGu95SXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model4/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgJBZEFqNebk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "21f13268-e998-479c-ea26-adeb929d15a0"
      },
      "source": [
        "!cd WNUT-2020-Task-2/experiments/ && python exp3.py --model_save_path {model_dir} --transformer_model_name albert-base-v2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-08-28 08:43:04.135826: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "Downloading: 100% 684/684 [00:00<00:00, 469kB/s]\n",
            "Downloading: 100% 760k/760k [00:00<00:00, 1.83MB/s]\n",
            "100% 7000/7000 [00:02<00:00, 2336.20it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2515.80it/s]\n",
            "Modelling\n",
            "Downloading: 100% 63.0M/63.0M [00:01<00:00, 46.5MB/s]\n",
            "2020-08-28 08:43:15.358393: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-28 08:43:15.377101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 08:43:15.377881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-28 08:43:15.377929: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 08:43:15.378067: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-28 08:43:15.378140: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-28 08:43:15.378213: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-28 08:43:15.378280: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-28 08:43:15.378354: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-28 08:43:15.378426: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-28 08:43:15.378590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 08:43:15.379421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 08:43:15.380134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-28 08:43:15.387177: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-08-28 08:43:15.387426: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x79b9c00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-28 08:43:15.387461: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-28 08:43:15.439852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 08:43:15.440706: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8d71b80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-28 08:43:15.440740: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2020-08-28 08:43:15.441215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 08:43:15.441916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-28 08:43:15.441983: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 08:43:15.442067: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-28 08:43:15.442123: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-28 08:43:15.442172: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-28 08:43:15.442223: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-28 08:43:15.442281: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-28 08:43:15.442367: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-28 08:43:15.442521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 08:43:15.443404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 08:43:15.444102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-28 08:43:15.444213: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 08:43:15.963750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-28 08:43:15.963825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-28 08:43:15.963852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-28 08:43:15.964228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 08:43:15.965104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 08:43:15.965805: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-28 08:43:15.965865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10628 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2020-08-28 08:43:15.996734: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "2020-08-28 08:43:16.044167: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing TFAlbertModel: ['predictions']\n",
            "- This IS expected if you are initializing TFAlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFAlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFAlbertModel were initialized from the model checkpoint at albert-base-v2.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_albert_model (TFAlbertModel) ((None, 100, 768), ( 11683584    input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 768)          0           tf_albert_model[0][1]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            769         dropout_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 11,684,353\n",
            "Trainable params: 11,684,353\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200828_084320-q12ba7yb\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmagic-water-5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2/runs/q12ba7yb\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "Epoch 1/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.2654 - accuracy: 0.8867\n",
            "Score 0.8774869109947644. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model4/model.h5.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: \n",
            "219/219 [==============================] - 290s 1s/step - loss: 0.2654 - accuracy: 0.8867 - val_loss: 0.2890 - val_accuracy: 0.8830\n",
            "Epoch 2/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1130 - accuracy: 0.9614\n",
            "Score 0.8741573033707866. Model not saved.\n",
            "219/219 [==============================] - 282s 1s/step - loss: 0.1130 - accuracy: 0.9614 - val_loss: 0.3176 - val_accuracy: 0.8880\n",
            "Epoch 3/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9799\n",
            "Score 0.865771812080537. Model not saved.\n",
            "219/219 [==============================] - 283s 1s/step - loss: 0.0647 - accuracy: 0.9799 - val_loss: 0.3725 - val_accuracy: 0.8800\n",
            "Epoch 4/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9843\n",
            "Score 0.8921465968586386. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model4/model.h5.\n",
            "219/219 [==============================] - 284s 1s/step - loss: 0.0459 - accuracy: 0.9843 - val_loss: 0.3604 - val_accuracy: 0.8970\n",
            "Epoch 5/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9924\n",
            "Score 0.8861788617886179. Model not saved.\n",
            "219/219 [==============================] - 283s 1s/step - loss: 0.0268 - accuracy: 0.9924 - val_loss: 0.4479 - val_accuracy: 0.8880\n",
            "Epoch 6/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9824\n",
            "Score 0.8698412698412697. Model not saved.\n",
            "219/219 [==============================] - 282s 1s/step - loss: 0.0528 - accuracy: 0.9824 - val_loss: 0.4194 - val_accuracy: 0.8770\n",
            "Epoch 7/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9864\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.3999999646330251e-05.\n",
            "\n",
            "Score 0.888888888888889. Model not saved.\n",
            "219/219 [==============================] - 283s 1s/step - loss: 0.0374 - accuracy: 0.9864 - val_loss: 0.3820 - val_accuracy: 0.8950\n",
            "Epoch 8/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 0.9977\n",
            "Score 0.8902439024390244. Model not saved.\n",
            "219/219 [==============================] - 282s 1s/step - loss: 0.0059 - accuracy: 0.9977 - val_loss: 0.6035 - val_accuracy: 0.8920\n",
            "Epoch 9/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0044 - accuracy: 0.9993Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Score 0.8921465968586386. Model not saved.\n",
            "\n",
            "Epoch 8: early stopping.\n",
            "219/219 [==============================] - 283s 1s/step - loss: 0.0044 - accuracy: 0.9993 - val_loss: 0.6037 - val_accuracy: 0.8940\n",
            "Epoch 00009: early stopping\n",
            "Evaluation\n",
            "description    albert-base-v2\n",
            "f1                   0.892147\n",
            "precision            0.881988\n",
            "recall               0.902542\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.6036532521247864\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1598606782.26255\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        accuracy 0.9992856979370117\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    val_accuracy 0.8939999938011169\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              lr 1.3999999282532372e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.004355133511126041\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 2598.9133796691895\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.28897014260292053\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced magic-water-5: https://app.wandb.ai/victor7246/wnut-task2/runs/q12ba7yb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOm-R-3ud2V_",
        "colab_type": "text"
      },
      "source": [
        "#### BERTweet model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZUXtfTrW2d3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model5/'\n",
        "transformer_model_name = '/content/drive/My\\ Drive/Models/BERTweet/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKIeiKIWW9f2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1823f008-75a2-44b0-d0f9-00700e50c4c5"
      },
      "source": [
        "!cd WNUT-2020-Task-2/experiments/ && python exp3.py --model_save_path {model_dir} --transformer_model_name {transformer_model_name}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-08-28 09:38:26.232090: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "Loading codes from /content/drive/My Drive/Models/BERTweet/bpe.codes ...\n",
            "Read 64000 codes from the codes file.\n",
            "100% 7000/7000 [00:04<00:00, 1544.43it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1585.32it/s]\n",
            "Modelling\n",
            "2020-08-28 09:38:35.539257: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-28 09:38:35.562784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 09:38:35.563604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-28 09:38:35.563658: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 09:38:35.563762: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-28 09:38:35.563834: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-28 09:38:35.563913: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-28 09:38:35.563984: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-28 09:38:35.564068: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-28 09:38:35.564147: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-28 09:38:35.564312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 09:38:35.565217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 09:38:35.565938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-28 09:38:35.571691: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-08-28 09:38:35.571936: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7cb99c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-28 09:38:35.571982: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-28 09:38:35.627664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 09:38:35.628600: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7cb6000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-28 09:38:35.628635: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2020-08-28 09:38:35.628884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 09:38:35.629637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-28 09:38:35.629704: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 09:38:35.629803: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-28 09:38:35.629867: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-28 09:38:35.629926: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-28 09:38:35.630018: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-28 09:38:35.630069: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-28 09:38:35.630124: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-28 09:38:35.630282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 09:38:35.631104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 09:38:35.631783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-28 09:38:35.631886: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 09:38:36.189188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-28 09:38:36.189266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-28 09:38:36.189296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-28 09:38:36.189624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 09:38:36.190501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 09:38:36.191210: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-28 09:38:36.191278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10628 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2020-08-28 09:38:48.492498: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "All PyTorch model weights were used when initializing TFRobertaModel.\n",
            "\n",
            "Some weights or buffers of the PyTorch model TFRobertaModel were not initialized from the TF 2.0 model and are newly initialized: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_roberta_model (TFRobertaMode ((None, 100, 768), ( 134899968   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 768)          0           tf_roberta_model[0][1]           \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            769         dropout_38[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 134,900,737\n",
            "Trainable params: 134,900,737\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200828_093853-39mtns5y\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meager-violet-8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2/runs/39mtns5y\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "Epoch 1/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.2754 - accuracy: 0.87362020-08-28 09:43:54.894321: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 196611072 exceeds 10% of free system memory.\n",
            "2020-08-28 09:44:11.215219: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 196611072 exceeds 10% of free system memory.\n",
            "\n",
            "Score 0.8777670837343599. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model5/model.h5.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: \n",
            "219/219 [==============================] - 299s 1s/step - loss: 0.2754 - accuracy: 0.8736 - val_loss: 0.3716 - val_accuracy: 0.8730\n",
            "Epoch 2/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1209 - accuracy: 0.95962020-08-28 09:48:51.172175: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 196611072 exceeds 10% of free system memory.\n",
            "2020-08-28 09:49:05.848047: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 196611072 exceeds 10% of free system memory.\n",
            "\n",
            "Score 0.8929663608562691. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model5/model.h5.\n",
            "219/219 [==============================] - 293s 1s/step - loss: 0.1209 - accuracy: 0.9596 - val_loss: 0.3264 - val_accuracy: 0.8950\n",
            "Epoch 3/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9770\n",
            "Score 0.8875621890547264. Model not saved.\n",
            "219/219 [==============================] - 288s 1s/step - loss: 0.0671 - accuracy: 0.9770 - val_loss: 0.3839 - val_accuracy: 0.8870\n",
            "Epoch 4/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.98962020-08-28 09:58:34.577162: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 196611072 exceeds 10% of free system memory.\n",
            "\n",
            "Score 0.899581589958159. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model5/model.h5.\n",
            "219/219 [==============================] - 293s 1s/step - loss: 0.0329 - accuracy: 0.9896 - val_loss: 0.3370 - val_accuracy: 0.9040\n",
            "Epoch 5/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9931\n",
            "Score 0.8895463510848126. Model not saved.\n",
            "219/219 [==============================] - 287s 1s/step - loss: 0.0220 - accuracy: 0.9931 - val_loss: 0.5032 - val_accuracy: 0.8880\n",
            "Epoch 6/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9933\n",
            "Score 0.8840125391849529. Model not saved.\n",
            "219/219 [==============================] - 288s 1s/step - loss: 0.0196 - accuracy: 0.9933 - val_loss: 0.5126 - val_accuracy: 0.8890\n",
            "Epoch 7/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9923\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.3999999646330251e-05.\n",
            "\n",
            "Score 0.8935323383084577. Model not saved.\n",
            "219/219 [==============================] - 287s 1s/step - loss: 0.0222 - accuracy: 0.9923 - val_loss: 0.5455 - val_accuracy: 0.8930\n",
            "Epoch 8/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9971\n",
            "Score 0.894681960375391. Model not saved.\n",
            "219/219 [==============================] - 287s 1s/step - loss: 0.0078 - accuracy: 0.9971 - val_loss: 0.4948 - val_accuracy: 0.8990\n",
            "Epoch 9/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.9984Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Score 0.899581589958159. Model not saved.\n",
            "\n",
            "Epoch 8: early stopping.\n",
            "219/219 [==============================] - 289s 1s/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.5668 - val_accuracy: 0.8990\n",
            "Epoch 00009: early stopping\n",
            "Evaluation\n",
            "description            \n",
            "f1             0.899582\n",
            "precision       0.88843\n",
            "recall         0.911017\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3055\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        accuracy 0.9984285831451416\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              lr 1.3999999282532372e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.005273506511002779\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1598610176.6281655\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.5667920708656311\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    val_accuracy 0.8989999890327454\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 2671.187094926834\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.32637953758239746\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced eager-violet-8: https://app.wandb.ai/victor7246/wnut-task2/runs/39mtns5y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfl_jhVed6Ia",
        "colab_type": "text"
      },
      "source": [
        "#### RoBERTa-base with mean pooling of all tokens from last layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOSO9oV1lzog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model6/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duYCEO12l1QX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "790a5a02-6fd1-4503-d704-1e4f133e7723"
      },
      "source": [
        "!cd WNUT-2020-Task-2/experiments/ && python exp4.py --model_save_path {model_dir} --transformer_model_name roberta-base --pooling_type mean"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-08-28 10:30:07.371308: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "100% 7000/7000 [00:05<00:00, 1324.48it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1451.67it/s]\n",
            "Modelling\n",
            "2020-08-28 10:30:18.949980: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-28 10:30:18.969297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 10:30:18.970085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-28 10:30:18.970143: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 10:30:18.970280: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-28 10:30:18.970352: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-28 10:30:18.970424: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-28 10:30:18.970507: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-28 10:30:18.970578: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-28 10:30:18.970654: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-28 10:30:18.970887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 10:30:18.971720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 10:30:18.972562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-28 10:30:18.979034: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-08-28 10:30:18.979396: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x841dc00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-28 10:30:18.979449: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-28 10:30:19.033451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 10:30:19.034345: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x97dfb80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-28 10:30:19.034383: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2020-08-28 10:30:19.034632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 10:30:19.035374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-28 10:30:19.035439: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 10:30:19.035516: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-28 10:30:19.035566: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-28 10:30:19.035628: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-28 10:30:19.035686: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-28 10:30:19.035743: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-28 10:30:19.035798: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-28 10:30:19.035956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 10:30:19.036767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 10:30:19.037461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-28 10:30:19.037547: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 10:30:19.567269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-28 10:30:19.567330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-28 10:30:19.567353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-28 10:30:19.567622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 10:30:19.568489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 10:30:19.569193: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-28 10:30:19.569256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10628 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2020-08-28 10:30:19.821590: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_roberta_model (TFRobertaMode ((None, 100, 768), ( 124645632   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d (Globa (None, 768)          0           tf_roberta_model[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 768)          0           global_average_pooling1d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            769         dropout_38[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 124,646,401\n",
            "Trainable params: 124,646,401\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200828_103025-13xedleu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mastral-totem-9\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2/runs/13xedleu\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.2124 - accuracy: 0.9121\n",
            "Score 0.882466281310212. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model6/model.h5.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: \n",
            "219/219 [==============================] - 298s 1s/step - loss: 0.2124 - accuracy: 0.9121 - val_loss: 0.3227 - val_accuracy: 0.8780\n",
            "Epoch 2/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0823 - accuracy: 0.9701\n",
            "Score 0.8938775510204081. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model6/model.h5.\n",
            "219/219 [==============================] - 293s 1s/step - loss: 0.0823 - accuracy: 0.9701 - val_loss: 0.2776 - val_accuracy: 0.8960\n",
            "Epoch 3/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9850\n",
            "Score 0.8897715988083417. Model not saved.\n",
            "219/219 [==============================] - 287s 1s/step - loss: 0.0418 - accuracy: 0.9850 - val_loss: 0.4651 - val_accuracy: 0.8890\n",
            "Epoch 4/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9930\n",
            "Score 0.8961303462321792. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model6/model.h5.\n",
            "219/219 [==============================] - 293s 1s/step - loss: 0.0219 - accuracy: 0.9930 - val_loss: 0.4938 - val_accuracy: 0.8980\n",
            "Epoch 5/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9961\n",
            "Score 0.8926441351888669. Model not saved.\n",
            "219/219 [==============================] - 287s 1s/step - loss: 0.0114 - accuracy: 0.9961 - val_loss: 0.7772 - val_accuracy: 0.8920\n",
            "Epoch 6/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9954\n",
            "Score 0.8918640576725027. Model not saved.\n",
            "219/219 [==============================] - 287s 1s/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.6102 - val_accuracy: 0.8950\n",
            "Epoch 7/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9946\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.3999999646330251e-05.\n",
            "\n",
            "Score 0.8911495422177008. Model not saved.\n",
            "219/219 [==============================] - 287s 1s/step - loss: 0.0144 - accuracy: 0.9946 - val_loss: 0.7011 - val_accuracy: 0.8930\n",
            "Epoch 8/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 0.9984\n",
            "Score 0.893796004206099. Model not saved.\n",
            "219/219 [==============================] - 288s 1s/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.6211 - val_accuracy: 0.8990\n",
            "Epoch 9/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 0.9981\n",
            "Score 0.8954918032786886. Model not saved.\n",
            "\n",
            "Epoch 8: early stopping.\n",
            "219/219 [==============================] - 286s 1s/step - loss: 0.0045 - accuracy: 0.9981 - val_loss: 0.6333 - val_accuracy: 0.8980\n",
            "Evaluation\n",
            "description    roberta-basemean of all tokens from last layer\n",
            "f1                                                    0.89613\n",
            "precision                                            0.862745\n",
            "recall                                               0.932203\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3945\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    val_accuracy 0.8980000019073486\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1598613260.9499884\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        accuracy 0.9981428384780884\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.633306086063385\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.0044916775077581406\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 2654.349056482315\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              lr 1.3999999282532372e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.27759242057800293\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced astral-totem-9: https://app.wandb.ai/victor7246/wnut-task2/runs/13xedleu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn2kgAwMeBQx",
        "colab_type": "text"
      },
      "source": [
        "#### RoBERTa-base with max pooling of all tokens from last layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CVx6gQlmEol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model7/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXVeiF-NmFpR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7c21dcc4-6a13-40c5-969a-b79cfe7eee8a"
      },
      "source": [
        "!cd WNUT-2020-Task-2/experiments/ && python exp4.py --model_save_path {model_dir} --transformer_model_name roberta-base --pooling_type max"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-08-28 11:16:46.870986: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "100% 7000/7000 [00:05<00:00, 1306.02it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1429.65it/s]\n",
            "Modelling\n",
            "2020-08-28 11:16:59.201952: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-28 11:16:59.225342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 11:16:59.226124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-28 11:16:59.226178: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 11:16:59.226290: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-28 11:16:59.226363: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-28 11:16:59.226438: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-28 11:16:59.226522: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-28 11:16:59.226591: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-28 11:16:59.226683: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-28 11:16:59.226848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 11:16:59.227727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 11:16:59.228545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-28 11:16:59.234923: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-08-28 11:16:59.235281: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6cbbc00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-28 11:16:59.235320: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-28 11:16:59.293327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 11:16:59.294236: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x807db80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-28 11:16:59.294272: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2020-08-28 11:16:59.294526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 11:16:59.295261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-28 11:16:59.295328: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 11:16:59.295453: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-28 11:16:59.295504: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-28 11:16:59.295566: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-28 11:16:59.295620: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-28 11:16:59.295668: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-28 11:16:59.295723: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-28 11:16:59.295890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 11:16:59.296735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 11:16:59.297427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-28 11:16:59.297497: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 11:16:59.847924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-28 11:16:59.848013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-28 11:16:59.848041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-28 11:16:59.848367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 11:16:59.849252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 11:16:59.850050: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-28 11:16:59.850108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10628 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2020-08-28 11:17:00.106230: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_roberta_model (TFRobertaMode ((None, 100, 768), ( 124645632   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d (GlobalMax (None, 768)          0           tf_roberta_model[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 768)          0           global_max_pooling1d[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            769         dropout_38[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 124,646,401\n",
            "Trainable params: 124,646,401\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200828_111705-1t614pr9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpolished-galaxy-10\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2/runs/1t614pr9\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.2697 - accuracy: 0.8784\n",
            "Score 0.8726591760299627. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model7/model.h5.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: \n",
            "219/219 [==============================] - 295s 1s/step - loss: 0.2697 - accuracy: 0.8784 - val_loss: 0.4261 - val_accuracy: 0.8640\n",
            "Epoch 2/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0993 - accuracy: 0.9646\n",
            "Score 0.8848223896663079. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model7/model.h5.\n",
            "219/219 [==============================] - 290s 1s/step - loss: 0.0993 - accuracy: 0.9646 - val_loss: 0.2861 - val_accuracy: 0.8930\n",
            "Epoch 3/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9833\n",
            "Score 0.8822393822393823. Model not saved.\n",
            "219/219 [==============================] - 286s 1s/step - loss: 0.0539 - accuracy: 0.9833 - val_loss: 0.3981 - val_accuracy: 0.8780\n",
            "Epoch 4/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9917\n",
            "Score 0.8953140578265205. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model7/model.h5.\n",
            "219/219 [==============================] - 291s 1s/step - loss: 0.0269 - accuracy: 0.9917 - val_loss: 0.4503 - val_accuracy: 0.8950\n",
            "Epoch 5/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9923\n",
            "Score 0.8930041152263376. Model not saved.\n",
            "219/219 [==============================] - 286s 1s/step - loss: 0.0250 - accuracy: 0.9923 - val_loss: 0.3885 - val_accuracy: 0.8960\n",
            "Epoch 6/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9946\n",
            "Score 0.8863198458574181. Model not saved.\n",
            "219/219 [==============================] - 286s 1s/step - loss: 0.0172 - accuracy: 0.9946 - val_loss: 0.6129 - val_accuracy: 0.8820\n",
            "Epoch 7/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9926\n",
            "Score 0.8839103869653767. Model not saved.\n",
            "219/219 [==============================] - 287s 1s/step - loss: 0.0203 - accuracy: 0.9926 - val_loss: 0.4920 - val_accuracy: 0.8860\n",
            "Epoch 8/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9961\n",
            "Score 0.8903088391906283. Model not saved.\n",
            "219/219 [==============================] - 287s 1s/step - loss: 0.0134 - accuracy: 0.9961 - val_loss: 0.4330 - val_accuracy: 0.8970\n",
            "Epoch 9/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9976\n",
            "Score 0.8850574712643677. Model not saved.\n",
            "\n",
            "Epoch 8: early stopping.\n",
            "219/219 [==============================] - 287s 1s/step - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.8035 - val_accuracy: 0.8800\n",
            "Evaluation\n",
            "description    roberta-basemax of all tokens from last layer\n",
            "f1                                                  0.895314\n",
            "precision                                           0.845574\n",
            "recall                                              0.951271\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 4813\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 2644.7139914035797\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    val_accuracy 0.8799999952316284\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.007954802364110947\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        accuracy 0.9975714087486267\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.8035029172897339\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              lr 1.9999999494757503e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1598616050.7472422\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.2861255705356598\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced polished-galaxy-10: https://app.wandb.ai/victor7246/wnut-task2/runs/1t614pr9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bfaYFtOeFJL",
        "colab_type": "text"
      },
      "source": [
        "#### RoBERTa-base with mean pooling of all tokens from all 12 layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THu4Fou78tZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model8/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaWoy1gOM3l5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94a8bd00-e45d-4db7-c234-55845921ce13"
      },
      "source": [
        "!cd WNUT-2020-Task-2/experiments/ && python exp5.py --model_save_path {model_dir} --transformer_model_name roberta-base --pooling_type mean"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-08-28 13:29:49.353727: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "100% 7000/7000 [00:04<00:00, 1602.44it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1921.45it/s]\n",
            "Modelling\n",
            "Downloading: 100% 657M/657M [00:21<00:00, 30.9MB/s]\n",
            "2020-08-28 13:30:22.097740: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-28 13:30:22.117574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 13:30:22.118360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-28 13:30:22.118431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 13:30:22.118543: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-28 13:30:22.118614: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-28 13:30:22.118684: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-28 13:30:22.118757: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-28 13:30:22.118830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-28 13:30:22.118912: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-28 13:30:22.119117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 13:30:22.119939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 13:30:22.120789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-28 13:30:22.129029: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-08-28 13:30:22.129312: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x916bd40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-28 13:30:22.129361: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-28 13:30:22.186373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 13:30:22.187301: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x916af40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-28 13:30:22.187338: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2020-08-28 13:30:22.187678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 13:30:22.188464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-28 13:30:22.188541: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 13:30:22.188613: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-28 13:30:22.188673: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-28 13:30:22.188725: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-28 13:30:22.188801: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-28 13:30:22.188895: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-28 13:30:22.188986: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-28 13:30:22.189218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 13:30:22.190297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 13:30:22.191194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-28 13:30:22.191284: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 13:30:22.769515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-28 13:30:22.769578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-28 13:30:22.769600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-28 13:30:22.769898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 13:30:22.770905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 13:30:22.771862: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-28 13:30:22.771937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10628 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2020-08-28 13:30:23.025344: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "3 (None, 100, 768) (None, 768)\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_roberta_model (TFRobertaMode ((None, 100, 768), ( 124645632   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 100, 9216)    0           tf_roberta_model[0][3]           \n",
            "                                                                 tf_roberta_model[0][4]           \n",
            "                                                                 tf_roberta_model[0][5]           \n",
            "                                                                 tf_roberta_model[0][6]           \n",
            "                                                                 tf_roberta_model[0][7]           \n",
            "                                                                 tf_roberta_model[0][8]           \n",
            "                                                                 tf_roberta_model[0][9]           \n",
            "                                                                 tf_roberta_model[0][10]          \n",
            "                                                                 tf_roberta_model[0][11]          \n",
            "                                                                 tf_roberta_model[0][12]          \n",
            "                                                                 tf_roberta_model[0][13]          \n",
            "                                                                 tf_roberta_model[0][14]          \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d (Globa (None, 9216)         0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 9216)         0           global_average_pooling1d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            9217        dropout_38[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 124,654,849\n",
            "Trainable params: 124,654,849\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200828_133028-1mfaauz8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfresh-gorge-11\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2/runs/1mfaauz8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.2622 - accuracy: 0.8851\n",
            "Score 0.8919182083739047. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model8/model.h5.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: \n",
            "219/219 [==============================] - 301s 1s/step - loss: 0.2622 - accuracy: 0.8851 - val_loss: 0.3484 - val_accuracy: 0.8890\n",
            "Epoch 2/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0899 - accuracy: 0.9703\n",
            "Score 0.8893320039880359. Model not saved.\n",
            "219/219 [==============================] - 291s 1s/step - loss: 0.0899 - accuracy: 0.9703 - val_loss: 0.3441 - val_accuracy: 0.8890\n",
            "Epoch 3/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9831\n",
            "Score 0.9076142131979696. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model8/model.h5.\n",
            "219/219 [==============================] - 297s 1s/step - loss: 0.0480 - accuracy: 0.9831 - val_loss: 0.3178 - val_accuracy: 0.9090\n",
            "Epoch 4/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9916\n",
            "Score 0.8899803536345776. Model not saved.\n",
            "219/219 [==============================] - 291s 1s/step - loss: 0.0257 - accuracy: 0.9916 - val_loss: 0.5474 - val_accuracy: 0.8880\n",
            "Epoch 5/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9920\n",
            "Score 0.8936170212765958. Model not saved.\n",
            "219/219 [==============================] - 288s 1s/step - loss: 0.0227 - accuracy: 0.9920 - val_loss: 0.3941 - val_accuracy: 0.8950\n",
            "Epoch 6/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9961\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.3999999646330251e-05.\n",
            "\n",
            "Score 0.8947927736450584. Model not saved.\n",
            "219/219 [==============================] - 287s 1s/step - loss: 0.0108 - accuracy: 0.9961 - val_loss: 0.5234 - val_accuracy: 0.9010\n",
            "Epoch 7/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.9969\n",
            "Score 0.9011446409989594. Model not saved.\n",
            "219/219 [==============================] - 287s 1s/step - loss: 0.0084 - accuracy: 0.9969 - val_loss: 0.5735 - val_accuracy: 0.9050\n",
            "Epoch 8/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9997Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Score 0.9076142131979696. Model not saved.\n",
            "\n",
            "Epoch 7: early stopping.\n",
            "219/219 [==============================] - 288s 1s/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.6928 - val_accuracy: 0.8980\n",
            "Epoch 00008: early stopping\n",
            "Evaluation\n",
            "description    roberta-basemean of all tokens from layers\n",
            "f1                                               0.907614\n",
            "precision                                        0.871345\n",
            "recall                                           0.947034\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 6921\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              lr 1.3999999282532372e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.0016533080488443375\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 2399.5312168598175\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    val_accuracy 0.8980000019073486\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        accuracy 0.9997143149375916\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1598623788.1053593\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.69281005859375\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.3177689015865326\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced fresh-gorge-11: https://app.wandb.ai/victor7246/wnut-task2/runs/1mfaauz8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G07xvM8meK9b",
        "colab_type": "text"
      },
      "source": [
        "#### RoBERTa-base with max pooling of all tokens from all 12 layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zOwgtnCRFzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model9/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuA4YsvkTveF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "560e2558-870a-4bb1-fe5d-2939dd9b1397"
      },
      "source": [
        "!cd WNUT-2020-Task-2/experiments/ && python exp5.py --model_save_path {model_dir} --transformer_model_name roberta-base --pooling_type max"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-08-28 14:10:49.238361: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "100% 7000/7000 [00:04<00:00, 1653.04it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1852.08it/s]\n",
            "Modelling\n",
            "2020-08-28 14:10:59.864785: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-28 14:10:59.895108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 14:10:59.895926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-28 14:10:59.895975: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 14:10:59.896087: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-28 14:10:59.896157: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-28 14:10:59.896241: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-28 14:10:59.896309: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-28 14:10:59.896374: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-28 14:10:59.896449: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-28 14:10:59.896610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 14:10:59.897556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 14:10:59.899446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-28 14:10:59.914297: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-08-28 14:10:59.916387: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x9325480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-28 14:10:59.916440: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-28 14:10:59.963828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 14:10:59.964779: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x9324f40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-28 14:10:59.964830: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2020-08-28 14:10:59.965150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 14:10:59.965859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-28 14:10:59.965923: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 14:10:59.966039: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-28 14:10:59.966106: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-28 14:10:59.966167: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-28 14:10:59.966219: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-28 14:10:59.966266: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-28 14:10:59.966320: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-28 14:10:59.966470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 14:10:59.967263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 14:10:59.967947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-28 14:10:59.968029: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 14:11:00.497464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-28 14:11:00.497542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-28 14:11:00.497567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-28 14:11:00.497873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 14:11:00.498744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 14:11:00.499446: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-28 14:11:00.499513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10628 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2020-08-28 14:11:00.735476: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "3 (None, 100, 768) (None, 768)\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_roberta_model (TFRobertaMode ((None, 100, 768), ( 124645632   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 100, 9216)    0           tf_roberta_model[0][3]           \n",
            "                                                                 tf_roberta_model[0][4]           \n",
            "                                                                 tf_roberta_model[0][5]           \n",
            "                                                                 tf_roberta_model[0][6]           \n",
            "                                                                 tf_roberta_model[0][7]           \n",
            "                                                                 tf_roberta_model[0][8]           \n",
            "                                                                 tf_roberta_model[0][9]           \n",
            "                                                                 tf_roberta_model[0][10]          \n",
            "                                                                 tf_roberta_model[0][11]          \n",
            "                                                                 tf_roberta_model[0][12]          \n",
            "                                                                 tf_roberta_model[0][13]          \n",
            "                                                                 tf_roberta_model[0][14]          \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d (GlobalMax (None, 9216)         0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 9216)         0           global_max_pooling1d[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            9217        dropout_38[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 124,654,849\n",
            "Trainable params: 124,654,849\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200828_141105-7zeqev3o\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlucky-fog-12\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2/runs/7zeqev3o\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3233 - accuracy: 0.8536\n",
            "Score 0.8733459357277883. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model9/model.h5.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: \n",
            "219/219 [==============================] - 297s 1s/step - loss: 0.3233 - accuracy: 0.8536 - val_loss: 0.4736 - val_accuracy: 0.8660\n",
            "Epoch 2/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1071 - accuracy: 0.9637\n",
            "Score 0.888888888888889. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model9/model.h5.\n",
            "219/219 [==============================] - 292s 1s/step - loss: 0.1071 - accuracy: 0.9637 - val_loss: 0.2787 - val_accuracy: 0.8910\n",
            "Epoch 3/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.9801\n",
            "Score 0.8959660297239915. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model9/model.h5.\n",
            "219/219 [==============================] - 294s 1s/step - loss: 0.0614 - accuracy: 0.9801 - val_loss: 0.2958 - val_accuracy: 0.9020\n",
            "Epoch 4/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9893\n",
            "Score 0.9064748201438849. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model9/model.h5.\n",
            "219/219 [==============================] - 294s 1s/step - loss: 0.0314 - accuracy: 0.9893 - val_loss: 0.3666 - val_accuracy: 0.9090\n",
            "Epoch 5/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9901\n",
            "Score 0.8930041152263376. Model not saved.\n",
            "219/219 [==============================] - 290s 1s/step - loss: 0.0293 - accuracy: 0.9901 - val_loss: 0.3869 - val_accuracy: 0.8960\n",
            "Epoch 6/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9924\n",
            "Score 0.9026178010471204. Model not saved.\n",
            "219/219 [==============================] - 289s 1s/step - loss: 0.0221 - accuracy: 0.9924 - val_loss: 0.3485 - val_accuracy: 0.9070\n",
            "Epoch 7/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9914\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.3999999646330251e-05.\n",
            "\n",
            "Score 0.8906720160481445. Model not saved.\n",
            "219/219 [==============================] - 289s 1s/step - loss: 0.0234 - accuracy: 0.9914 - val_loss: 0.5298 - val_accuracy: 0.8910\n",
            "Epoch 8/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9971\n",
            "Score 0.895397489539749. Model not saved.\n",
            "219/219 [==============================] - 290s 1s/step - loss: 0.0111 - accuracy: 0.9971 - val_loss: 0.4717 - val_accuracy: 0.9000\n",
            "Epoch 9/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.9993Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Score 0.9064748201438849. Model not saved.\n",
            "\n",
            "Epoch 8: early stopping.\n",
            "219/219 [==============================] - 290s 1s/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.5509 - val_accuracy: 0.9020\n",
            "Epoch 00009: early stopping\n",
            "Evaluation\n",
            "description    roberta-basemax of all tokens from layers\n",
            "f1                                              0.906475\n",
            "precision                                        0.88024\n",
            "recall                                          0.934322\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 7469\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    val_accuracy 0.9020000100135803\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.002388548804447055\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              lr 1.3999999282532372e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        accuracy 0.9992856979370117\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.5508975386619568\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 2673.592429161072\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1598626522.0625584\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.2786589562892914\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced lucky-fog-12: https://app.wandb.ai/victor7246/wnut-task2/runs/7zeqev3o\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk0A1KV3eOho",
        "colab_type": "text"
      },
      "source": [
        "#### RoBERTa-base with mean pooling of all tokens from last 4 layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzU2RKx2WqUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model10/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmddZjdgRG_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "edc6bdbb-0de2-4307-e86c-8269afc1df29"
      },
      "source": [
        "!cd WNUT-2020-Task-2/experiments/ && python exp5.py --model_save_path {model_dir} --transformer_model_name roberta-base --pooling_type mean --layers 9_10_11_12"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-08-28 14:56:15.524851: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "100% 7000/7000 [00:04<00:00, 1681.05it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1895.49it/s]\n",
            "Modelling\n",
            "2020-08-28 14:56:25.899927: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-28 14:56:25.922547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 14:56:25.923396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-28 14:56:25.923454: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 14:56:25.923574: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-28 14:56:25.923654: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-28 14:56:25.923724: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-28 14:56:25.923813: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-28 14:56:25.923923: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-28 14:56:25.924075: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-28 14:56:25.924265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 14:56:25.925281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 14:56:25.926014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-28 14:56:25.931941: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-08-28 14:56:25.932327: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ffb480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-28 14:56:25.932366: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-28 14:56:25.978980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 14:56:25.979985: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ffaf40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-28 14:56:25.980046: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2020-08-28 14:56:25.981478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 14:56:25.982336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-28 14:56:25.982420: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 14:56:25.982504: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-28 14:56:25.982574: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-28 14:56:25.982643: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-28 14:56:25.982743: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-28 14:56:25.982805: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-28 14:56:25.982865: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-28 14:56:25.983127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 14:56:25.984208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 14:56:25.985200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-28 14:56:25.985301: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-28 14:56:26.511145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-28 14:56:26.511248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-28 14:56:26.511273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-28 14:56:26.511711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 14:56:26.512585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-28 14:56:26.513336: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-28 14:56:26.513407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10628 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2020-08-28 14:56:26.750183: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "3 (None, 100, 768) (None, 768)\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_roberta_model (TFRobertaMode ((None, 100, 768), ( 124645632   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 100, 3072)    0           tf_roberta_model[0][11]          \n",
            "                                                                 tf_roberta_model[0][12]          \n",
            "                                                                 tf_roberta_model[0][13]          \n",
            "                                                                 tf_roberta_model[0][14]          \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d (Globa (None, 3072)         0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 3072)         0           global_average_pooling1d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            3073        dropout_38[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 124,648,705\n",
            "Trainable params: 124,648,705\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200828_145631-3i03urbd\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mamber-hill-13\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2/runs/3i03urbd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.2372 - accuracy: 0.8961\n",
            "Score 0.8911495422177008. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model10/model.h5.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: \n",
            "219/219 [==============================] - 296s 1s/step - loss: 0.2372 - accuracy: 0.8961 - val_loss: 0.3078 - val_accuracy: 0.8930\n",
            "Epoch 2/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0960 - accuracy: 0.9669\n",
            "Score 0.8857994041708044. Model not saved.\n",
            "219/219 [==============================] - 287s 1s/step - loss: 0.0960 - accuracy: 0.9669 - val_loss: 0.3833 - val_accuracy: 0.8850\n",
            "Epoch 3/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9819\n",
            "Score 0.9003083247687563. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model10/model.h5.\n",
            "219/219 [==============================] - 292s 1s/step - loss: 0.0509 - accuracy: 0.9819 - val_loss: 0.3456 - val_accuracy: 0.9030\n",
            "Epoch 4/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9904\n",
            "Score 0.896551724137931. Model not saved.\n",
            "219/219 [==============================] - 286s 1s/step - loss: 0.0281 - accuracy: 0.9904 - val_loss: 0.4508 - val_accuracy: 0.8980\n",
            "Epoch 5/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9943\n",
            "Score 0.9006085192697769. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model10/model.h5.\n",
            "219/219 [==============================] - 291s 1s/step - loss: 0.0181 - accuracy: 0.9943 - val_loss: 0.4721 - val_accuracy: 0.9020\n",
            "Epoch 6/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9940\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.3999999646330251e-05.\n",
            "\n",
            "Score 0.890282131661442. Model not saved.\n",
            "219/219 [==============================] - 286s 1s/step - loss: 0.0192 - accuracy: 0.9940 - val_loss: 0.4420 - val_accuracy: 0.8950\n",
            "Epoch 7/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.9976\n",
            "Score 0.9083585095669688. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model10/model.h5.\n",
            "219/219 [==============================] - 292s 1s/step - loss: 0.0085 - accuracy: 0.9976 - val_loss: 0.6018 - val_accuracy: 0.9090\n",
            "Epoch 8/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 0.9989\n",
            "Score 0.8954593453009504. Model not saved.\n",
            "219/219 [==============================] - 287s 1s/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 0.6129 - val_accuracy: 0.9010\n",
            "Epoch 9/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.9969\n",
            "Score 0.9029029029029029. Model not saved.\n",
            "219/219 [==============================] - 286s 1s/step - loss: 0.0084 - accuracy: 0.9969 - val_loss: 0.6665 - val_accuracy: 0.9030\n",
            "Epoch 10/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 0.9990\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.799999497772659e-06.\n",
            "\n",
            "Score 0.8946840521564694. Model not saved.\n",
            "219/219 [==============================] - 286s 1s/step - loss: 0.0035 - accuracy: 0.9990 - val_loss: 0.7785 - val_accuracy: 0.8950\n",
            "Epoch 11/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 0.9987\n",
            "Score 0.8970438328236494. Model not saved.\n",
            "219/219 [==============================] - 286s 1s/step - loss: 0.0033 - accuracy: 0.9987 - val_loss: 0.7417 - val_accuracy: 0.8990\n",
            "Epoch 12/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9974Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Score 0.9083585095669688. Model not saved.\n",
            "\n",
            "Epoch 11: early stopping.\n",
            "219/219 [==============================] - 286s 1s/step - loss: 0.0066 - accuracy: 0.9974 - val_loss: 0.5410 - val_accuracy: 0.9000\n",
            "Epoch 00012: early stopping\n",
            "Evaluation\n",
            "description    roberta-basemean of all tokens from layers\n",
            "f1                                               0.908359\n",
            "precision                                        0.865643\n",
            "recall                                           0.955508\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 7849\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 3513.7813515663147\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.006599026266485453\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        accuracy 0.9974285960197449\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1598630088.5514705\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    val_accuracy 0.8999999761581421\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.5410013794898987\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              lr 9.79999913397478e-06\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.30779245495796204\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced amber-hill-13: https://app.wandb.ai/victor7246/wnut-task2/runs/3i03urbd\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZX5y9AEeRnA",
        "colab_type": "text"
      },
      "source": [
        "#### RoBERTa-base with max pooling of all tokens from last 4 layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhKTAWzwRPcb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model11/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5WRHu__Wtyh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4973270-1287-4121-9196-0fd7eb2e4a6e"
      },
      "source": [
        "!cd WNUT-2020-Task-2/experiments/ && python exp5.py --model_save_path {model_dir} --transformer_model_name roberta-base --pooling_type max --layers 9_10_11_12"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-30 05:42:09.987238: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "Downloading: 100% 481/481 [00:00<00:00, 286kB/s]\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 823kB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 509kB/s]\n",
            "100% 7000/7000 [00:03<00:00, 2288.13it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2697.75it/s]\n",
            "Modelling\n",
            "Downloading: 100% 657M/657M [00:53<00:00, 12.3MB/s]\n",
            "2020-08-30 05:43:24.802273: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-30 05:43:24.815190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 05:43:24.815762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-08-30 05:43:24.815810: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-30 05:43:24.815899: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-30 05:43:24.815942: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-30 05:43:24.815982: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-30 05:43:24.816021: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-30 05:43:24.816074: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-30 05:43:24.816117: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-30 05:43:24.816264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 05:43:24.816863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 05:43:24.817359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-30 05:43:24.832928: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-08-30 05:43:24.833132: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7eafd40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-30 05:43:24.833157: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-30 05:43:24.961314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 05:43:24.961993: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7eaf2c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-30 05:43:24.962026: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-08-30 05:43:24.962303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 05:43:24.962823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-08-30 05:43:24.962981: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-30 05:43:24.963094: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-30 05:43:24.963127: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-30 05:43:24.963152: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-30 05:43:24.963183: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-30 05:43:24.963208: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-30 05:43:24.963238: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-30 05:43:24.963366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 05:43:24.963969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 05:43:24.964461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-30 05:43:24.967638: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-30 05:43:29.221171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-30 05:43:29.221229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-30 05:43:29.221242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-30 05:43:29.225206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 05:43:29.225875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 05:43:29.226474: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-30 05:43:29.226524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14962 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "2020-08-30 05:43:29.548828: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_roberta_model (TFRobertaMode ((None, 100, 768), ( 124645632   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 100, 3072)    0           tf_roberta_model[0][11]          \n",
            "                                                                 tf_roberta_model[0][12]          \n",
            "                                                                 tf_roberta_model[0][13]          \n",
            "                                                                 tf_roberta_model[0][14]          \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d (GlobalMax (None, 3072)         0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 3072)         0           global_max_pooling1d[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            3073        dropout_38[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 124,648,705\n",
            "Trainable params: 124,648,705\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Evaluation\n",
            "description    roberta-base max of all tokens from layers 9_1...\n",
            "f1                                                      0.911134\n",
            "precision                                               0.879684\n",
            "recall                                                  0.944915\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VxR_x2XeUj1",
        "colab_type": "text"
      },
      "source": [
        "#### RoBERTa-base with mean pooling and CNN (wavenet) on all tokens from last 4 layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cjqbj9A4ygt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model12/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV5aWTOu40dF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5eb98e1d-0a75-4922-8ec4-d57c7a280992"
      },
      "source": [
        "!cd WNUT-2020-Task-2/experiments/ && python exp6.py --model_save_path {model_dir} --transformer_model_name roberta-base --pooling_type mean"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-30 05:43:53.862861: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "100% 7000/7000 [00:03<00:00, 2258.66it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2682.17it/s]\n",
            "Modelling\n",
            "2020-08-30 05:44:04.231702: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-30 05:44:04.245166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 05:44:04.245713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-08-30 05:44:04.245759: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-30 05:44:04.245845: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-30 05:44:04.245890: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-30 05:44:04.245931: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-30 05:44:04.245970: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-30 05:44:04.246004: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-30 05:44:04.246064: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-30 05:44:04.246208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 05:44:04.246812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 05:44:04.247317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-30 05:44:04.252318: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-08-30 05:44:04.252510: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x9253100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-30 05:44:04.252542: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-30 05:44:04.338755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 05:44:04.339524: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x92532c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-30 05:44:04.339557: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-08-30 05:44:04.339811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 05:44:04.340435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-08-30 05:44:04.340488: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-30 05:44:04.340542: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-30 05:44:04.340568: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-30 05:44:04.340592: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-30 05:44:04.340616: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-30 05:44:04.340637: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-30 05:44:04.340665: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-30 05:44:04.340792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 05:44:04.341448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 05:44:04.341938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-30 05:44:04.341993: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-30 05:44:04.964355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-30 05:44:04.964419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-30 05:44:04.964431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-30 05:44:04.964702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 05:44:04.965402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 05:44:04.965897: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-30 05:44:04.965940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14962 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "2020-08-30 05:44:05.142599: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_roberta_model (TFRobertaMode ((None, 100, 768), ( 124645632   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 100, 3072)    0           tf_roberta_model[0][11]          \n",
            "                                                                 tf_roberta_model[0][12]          \n",
            "                                                                 tf_roberta_model[0][13]          \n",
            "                                                                 tf_roberta_model[0][14]          \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 100, 128)     393344      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 100, 128)     49280       conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 100, 128)     49280       conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 100, 128)     0           conv1d_1[0][0]                   \n",
            "                                                                 conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 100, 128)     16512       multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 100, 128)     49280       conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 100, 128)     49280       conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, 100, 128)     0           conv1d_4[0][0]                   \n",
            "                                                                 conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 100, 128)     16512       multiply_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 100, 128)     49280       conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 100, 128)     49280       conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "multiply_2 (Multiply)           (None, 100, 128)     0           conv1d_7[0][0]                   \n",
            "                                                                 conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, 100, 128)     16512       multiply_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 100, 128)     49280       conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, 100, 128)     49280       conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "multiply_3 (Multiply)           (None, 100, 128)     0           conv1d_10[0][0]                  \n",
            "                                                                 conv1d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_12 (Conv1D)              (None, 100, 128)     16512       multiply_3[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_13 (Conv1D)              (None, 100, 128)     49280       conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_14 (Conv1D)              (None, 100, 128)     49280       conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_4 (Multiply)           (None, 100, 128)     0           conv1d_13[0][0]                  \n",
            "                                                                 conv1d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_15 (Conv1D)              (None, 100, 128)     16512       multiply_4[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_16 (Conv1D)              (None, 100, 128)     49280       conv1d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_17 (Conv1D)              (None, 100, 128)     49280       conv1d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_5 (Multiply)           (None, 100, 128)     0           conv1d_16[0][0]                  \n",
            "                                                                 conv1d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 100, 128)     0           conv1d[0][0]                     \n",
            "                                                                 conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_18 (Conv1D)              (None, 100, 128)     16512       multiply_5[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 100, 128)     0           add[0][0]                        \n",
            "                                                                 conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_19 (Conv1D)              (None, 100, 128)     49280       conv1d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_20 (Conv1D)              (None, 100, 128)     49280       conv1d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 100, 128)     0           add_1[0][0]                      \n",
            "                                                                 conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "multiply_6 (Multiply)           (None, 100, 128)     0           conv1d_19[0][0]                  \n",
            "                                                                 conv1d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 100, 128)     0           add_2[0][0]                      \n",
            "                                                                 conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_21 (Conv1D)              (None, 100, 128)     16512       multiply_6[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 100, 128)     0           add_3[0][0]                      \n",
            "                                                                 conv1d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_22 (Conv1D)              (None, 100, 128)     49280       conv1d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_23 (Conv1D)              (None, 100, 128)     49280       conv1d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 100, 128)     0           add_4[0][0]                      \n",
            "                                                                 conv1d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_7 (Multiply)           (None, 100, 128)     0           conv1d_22[0][0]                  \n",
            "                                                                 conv1d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 100, 128)     0           add_5[0][0]                      \n",
            "                                                                 conv1d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_24 (Conv1D)              (None, 100, 128)     16512       multiply_7[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 100, 128)     0           add_6[0][0]                      \n",
            "                                                                 conv1d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization (LayerNorma (None, 100, 128)     256         add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_25 (Conv1D)              (None, 100, 64)      8256        layer_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_26 (Conv1D)              (None, 100, 64)      12352       conv1d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_27 (Conv1D)              (None, 100, 64)      12352       conv1d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_8 (Multiply)           (None, 100, 64)      0           conv1d_26[0][0]                  \n",
            "                                                                 conv1d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_28 (Conv1D)              (None, 100, 64)      4160        multiply_8[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_29 (Conv1D)              (None, 100, 64)      12352       conv1d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_30 (Conv1D)              (None, 100, 64)      12352       conv1d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_9 (Multiply)           (None, 100, 64)      0           conv1d_29[0][0]                  \n",
            "                                                                 conv1d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_31 (Conv1D)              (None, 100, 64)      4160        multiply_9[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_32 (Conv1D)              (None, 100, 64)      12352       conv1d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_33 (Conv1D)              (None, 100, 64)      12352       conv1d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_10 (Multiply)          (None, 100, 64)      0           conv1d_32[0][0]                  \n",
            "                                                                 conv1d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_34 (Conv1D)              (None, 100, 64)      4160        multiply_10[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 100, 64)      0           conv1d_25[0][0]                  \n",
            "                                                                 conv1d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_35 (Conv1D)              (None, 100, 64)      12352       conv1d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_36 (Conv1D)              (None, 100, 64)      12352       conv1d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 100, 64)      0           add_8[0][0]                      \n",
            "                                                                 conv1d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_11 (Multiply)          (None, 100, 64)      0           conv1d_35[0][0]                  \n",
            "                                                                 conv1d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 100, 64)      0           add_9[0][0]                      \n",
            "                                                                 conv1d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_37 (Conv1D)              (None, 100, 64)      4160        multiply_11[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 100, 64)      0           add_10[0][0]                     \n",
            "                                                                 conv1d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d (Globa (None, 64)           0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 64)           0           global_average_pooling1d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            65          dropout_38[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 126,083,585\n",
            "Trainable params: 126,083,585\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "2020-08-30 05:44:25.917063: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3002 - accuracy: 0.8731\n",
            "Score 0.8908382066276802. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model12/model.h5.\n",
            "219/219 [==============================] - 109s 499ms/step - loss: 0.3002 - accuracy: 0.8731 - val_loss: 0.3592 - val_accuracy: 0.8880\n",
            "Epoch 2/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0969 - accuracy: 0.9641\n",
            "Score 0.8909090909090909. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model12/model.h5.\n",
            "219/219 [==============================] - 105s 481ms/step - loss: 0.0969 - accuracy: 0.9641 - val_loss: 0.3076 - val_accuracy: 0.8920\n",
            "Epoch 3/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9836\n",
            "Score 0.9001996007984031. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model12/model.h5.\n",
            "219/219 [==============================] - 106s 483ms/step - loss: 0.0479 - accuracy: 0.9836 - val_loss: 0.3881 - val_accuracy: 0.9000\n",
            "Epoch 4/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9921\n",
            "Score 0.9051987767584098. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model12/model.h5.\n",
            "219/219 [==============================] - 106s 484ms/step - loss: 0.0254 - accuracy: 0.9921 - val_loss: 0.4437 - val_accuracy: 0.9070\n",
            "Epoch 5/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9963\n",
            "Score 0.9014675052410902. Model not saved.\n",
            "219/219 [==============================] - 100s 459ms/step - loss: 0.0147 - accuracy: 0.9963 - val_loss: 0.4139 - val_accuracy: 0.9060\n",
            "Epoch 6/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9950\n",
            "Score 0.8639455782312925. Model not saved.\n",
            "219/219 [==============================] - 101s 459ms/step - loss: 0.0132 - accuracy: 0.9950 - val_loss: 0.6881 - val_accuracy: 0.8800\n",
            "Epoch 7/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9943\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.3999999646330251e-05.\n",
            "\n",
            "Score 0.8930323846908735. Model not saved.\n",
            "219/219 [==============================] - 100s 459ms/step - loss: 0.0179 - accuracy: 0.9943 - val_loss: 0.5291 - val_accuracy: 0.8910\n",
            "Epoch 8/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0043 - accuracy: 0.9990\n",
            "Score 0.8900308324768756. Model not saved.\n",
            "219/219 [==============================] - 100s 458ms/step - loss: 0.0043 - accuracy: 0.9990 - val_loss: 0.6392 - val_accuracy: 0.8930\n",
            "Epoch 9/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0044 - accuracy: 0.9990\n",
            "Score 0.9057377049180328. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model12/model.h5.\n",
            "219/219 [==============================] - 106s 482ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 0.5029 - val_accuracy: 0.9080\n",
            "Epoch 10/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 0.9993\n",
            "Score 0.8742004264392325. Model not saved.\n",
            "219/219 [==============================] - 100s 459ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.6364 - val_accuracy: 0.8820\n",
            "Epoch 11/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9966\n",
            "Score 0.8872017353579176. Model not saved.\n",
            "219/219 [==============================] - 100s 458ms/step - loss: 0.0126 - accuracy: 0.9966 - val_loss: 0.4699 - val_accuracy: 0.8960\n",
            "Epoch 12/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 0.9981\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.799999497772659e-06.\n",
            "\n",
            "Score 0.8886597938144329. Model not saved.\n",
            "219/219 [==============================] - 101s 459ms/step - loss: 0.0057 - accuracy: 0.9981 - val_loss: 0.7030 - val_accuracy: 0.8920\n",
            "Epoch 13/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9996\n",
            "Score 0.8937875751503006. Model not saved.\n",
            "219/219 [==============================] - 101s 459ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.8552 - val_accuracy: 0.8940\n",
            "Epoch 14/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 0.9989Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Score 0.9057377049180328. Model not saved.\n",
            "\n",
            "Epoch 13: early stopping.\n",
            "219/219 [==============================] - 101s 459ms/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 0.7349 - val_accuracy: 0.8890\n",
            "Epoch 00014: early stopping\n",
            "Evaluation\n",
            "Snapshot Evaluation\n",
            "description    roberta-base mean pooling with CNN\n",
            "f1                                       0.905738\n",
            "precision                                0.876984\n",
            "recall                                   0.936441\n",
            "Name: 0, dtype: object\n",
            "Snapshot scores: F1 0.8975409836065574, Precision 0.8690476190476191 and Recall 0.9279661016949152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdbkU-q9ecXj",
        "colab_type": "text"
      },
      "source": [
        "#### RoBERTa-base with max pooling and CNN (wavenet) on all tokens from last 4 layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D46Zzxv9-UQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model13/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QINKjCTL9_Fz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e6c05d34-e1f0-4cb8-bfa9-4caedc8e5a55"
      },
      "source": [
        "!cd WNUT-2020-Task-2/experiments/ && python exp6.py --model_save_path {model_dir} --transformer_model_name roberta-base --pooling_type max"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-08-30 06:09:37.563607: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "100% 7000/7000 [00:03<00:00, 2256.20it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2310.42it/s]\n",
            "Modelling\n",
            "2020-08-30 06:09:48.262363: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-30 06:09:48.279660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 06:09:48.280246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-08-30 06:09:48.280302: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-30 06:09:48.280389: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-30 06:09:48.280429: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-30 06:09:48.280470: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-30 06:09:48.280508: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-30 06:09:48.280543: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-30 06:09:48.280584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-30 06:09:48.280723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 06:09:48.281340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 06:09:48.281819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-30 06:09:48.286794: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-08-30 06:09:48.286988: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x85f5100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-30 06:09:48.287017: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-30 06:09:48.375715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 06:09:48.376389: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x85f52c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-30 06:09:48.376434: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-08-30 06:09:48.376724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 06:09:48.377291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-08-30 06:09:48.377348: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-30 06:09:48.377402: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-30 06:09:48.377430: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-30 06:09:48.377454: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-30 06:09:48.377478: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-30 06:09:48.377500: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-30 06:09:48.377529: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-30 06:09:48.377664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 06:09:48.378285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 06:09:48.378782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-30 06:09:48.378842: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-30 06:09:48.998149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-30 06:09:48.998210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-30 06:09:48.998221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-30 06:09:48.998489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 06:09:48.999188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 06:09:48.999723: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-30 06:09:48.999769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14962 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "2020-08-30 06:09:49.166563: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_roberta_model (TFRobertaMode ((None, 100, 768), ( 124645632   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 100, 3072)    0           tf_roberta_model[0][11]          \n",
            "                                                                 tf_roberta_model[0][12]          \n",
            "                                                                 tf_roberta_model[0][13]          \n",
            "                                                                 tf_roberta_model[0][14]          \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 100, 128)     393344      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 100, 128)     49280       conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 100, 128)     49280       conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 100, 128)     0           conv1d_1[0][0]                   \n",
            "                                                                 conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 100, 128)     16512       multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 100, 128)     49280       conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 100, 128)     49280       conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, 100, 128)     0           conv1d_4[0][0]                   \n",
            "                                                                 conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 100, 128)     16512       multiply_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 100, 128)     49280       conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 100, 128)     49280       conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "multiply_2 (Multiply)           (None, 100, 128)     0           conv1d_7[0][0]                   \n",
            "                                                                 conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, 100, 128)     16512       multiply_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 100, 128)     49280       conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, 100, 128)     49280       conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "multiply_3 (Multiply)           (None, 100, 128)     0           conv1d_10[0][0]                  \n",
            "                                                                 conv1d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_12 (Conv1D)              (None, 100, 128)     16512       multiply_3[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_13 (Conv1D)              (None, 100, 128)     49280       conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_14 (Conv1D)              (None, 100, 128)     49280       conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_4 (Multiply)           (None, 100, 128)     0           conv1d_13[0][0]                  \n",
            "                                                                 conv1d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_15 (Conv1D)              (None, 100, 128)     16512       multiply_4[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_16 (Conv1D)              (None, 100, 128)     49280       conv1d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_17 (Conv1D)              (None, 100, 128)     49280       conv1d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_5 (Multiply)           (None, 100, 128)     0           conv1d_16[0][0]                  \n",
            "                                                                 conv1d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 100, 128)     0           conv1d[0][0]                     \n",
            "                                                                 conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_18 (Conv1D)              (None, 100, 128)     16512       multiply_5[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 100, 128)     0           add[0][0]                        \n",
            "                                                                 conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_19 (Conv1D)              (None, 100, 128)     49280       conv1d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_20 (Conv1D)              (None, 100, 128)     49280       conv1d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 100, 128)     0           add_1[0][0]                      \n",
            "                                                                 conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "multiply_6 (Multiply)           (None, 100, 128)     0           conv1d_19[0][0]                  \n",
            "                                                                 conv1d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 100, 128)     0           add_2[0][0]                      \n",
            "                                                                 conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_21 (Conv1D)              (None, 100, 128)     16512       multiply_6[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 100, 128)     0           add_3[0][0]                      \n",
            "                                                                 conv1d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_22 (Conv1D)              (None, 100, 128)     49280       conv1d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_23 (Conv1D)              (None, 100, 128)     49280       conv1d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 100, 128)     0           add_4[0][0]                      \n",
            "                                                                 conv1d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_7 (Multiply)           (None, 100, 128)     0           conv1d_22[0][0]                  \n",
            "                                                                 conv1d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 100, 128)     0           add_5[0][0]                      \n",
            "                                                                 conv1d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_24 (Conv1D)              (None, 100, 128)     16512       multiply_7[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 100, 128)     0           add_6[0][0]                      \n",
            "                                                                 conv1d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization (LayerNorma (None, 100, 128)     256         add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_25 (Conv1D)              (None, 100, 64)      8256        layer_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_26 (Conv1D)              (None, 100, 64)      12352       conv1d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_27 (Conv1D)              (None, 100, 64)      12352       conv1d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_8 (Multiply)           (None, 100, 64)      0           conv1d_26[0][0]                  \n",
            "                                                                 conv1d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_28 (Conv1D)              (None, 100, 64)      4160        multiply_8[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_29 (Conv1D)              (None, 100, 64)      12352       conv1d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_30 (Conv1D)              (None, 100, 64)      12352       conv1d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_9 (Multiply)           (None, 100, 64)      0           conv1d_29[0][0]                  \n",
            "                                                                 conv1d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_31 (Conv1D)              (None, 100, 64)      4160        multiply_9[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_32 (Conv1D)              (None, 100, 64)      12352       conv1d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_33 (Conv1D)              (None, 100, 64)      12352       conv1d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_10 (Multiply)          (None, 100, 64)      0           conv1d_32[0][0]                  \n",
            "                                                                 conv1d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_34 (Conv1D)              (None, 100, 64)      4160        multiply_10[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 100, 64)      0           conv1d_25[0][0]                  \n",
            "                                                                 conv1d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_35 (Conv1D)              (None, 100, 64)      12352       conv1d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_36 (Conv1D)              (None, 100, 64)      12352       conv1d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 100, 64)      0           add_8[0][0]                      \n",
            "                                                                 conv1d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_11 (Multiply)          (None, 100, 64)      0           conv1d_35[0][0]                  \n",
            "                                                                 conv1d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 100, 64)      0           add_9[0][0]                      \n",
            "                                                                 conv1d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_37 (Conv1D)              (None, 100, 64)      4160        multiply_11[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 100, 64)      0           add_10[0][0]                     \n",
            "                                                                 conv1d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d (GlobalMax (None, 64)           0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 64)           0           global_max_pooling1d[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            65          dropout_38[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 126,083,585\n",
            "Trainable params: 126,083,585\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200830_060953-35aajx26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlight-plant-16\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2/runs/35aajx26\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "2020-08-30 06:10:11.664947: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.3542 - accuracy: 0.8399\n",
            "Score 0.888235294117647. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model13/model.h5.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: \n",
            "219/219 [==============================] - 110s 504ms/step - loss: 0.3542 - accuracy: 0.8399 - val_loss: 0.3190 - val_accuracy: 0.8860\n",
            "Epoch 2/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.1095 - accuracy: 0.9614\n",
            "Score 0.8935721812434141. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model13/model.h5.\n",
            "219/219 [==============================] - 106s 485ms/step - loss: 0.1095 - accuracy: 0.9614 - val_loss: 0.2540 - val_accuracy: 0.8990\n",
            "Epoch 3/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9790\n",
            "Score 0.8972162740899358. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model13/model.h5.\n",
            "219/219 [==============================] - 106s 486ms/step - loss: 0.0595 - accuracy: 0.9790 - val_loss: 0.2986 - val_accuracy: 0.9040\n",
            "Epoch 4/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9909\n",
            "Score 0.8967674661105319. Model not saved.\n",
            "219/219 [==============================] - 101s 460ms/step - loss: 0.0268 - accuracy: 0.9909 - val_loss: 0.4160 - val_accuracy: 0.9010\n",
            "Epoch 5/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9926\n",
            "Score 0.8939999999999999. Model not saved.\n",
            "219/219 [==============================] - 101s 460ms/step - loss: 0.0223 - accuracy: 0.9926 - val_loss: 0.5139 - val_accuracy: 0.8940\n",
            "Epoch 6/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9941\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.3999999646330251e-05.\n",
            "\n",
            "Score 0.887722980062959. Model not saved.\n",
            "219/219 [==============================] - 101s 461ms/step - loss: 0.0182 - accuracy: 0.9941 - val_loss: 0.5330 - val_accuracy: 0.8930\n",
            "Epoch 7/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 0.9980\n",
            "Score 0.8924949290060852. Model not saved.\n",
            "219/219 [==============================] - 101s 460ms/step - loss: 0.0076 - accuracy: 0.9980 - val_loss: 0.5434 - val_accuracy: 0.8940\n",
            "Epoch 8/15\n",
            "219/219 [==============================] - ETA: 0s - loss: 0.0055 - accuracy: 0.9987Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Score 0.8972162740899358. Model not saved.\n",
            "\n",
            "Epoch 7: early stopping.\n",
            "219/219 [==============================] - 101s 461ms/step - loss: 0.0055 - accuracy: 0.9987 - val_loss: 0.5187 - val_accuracy: 0.8980\n",
            "Epoch 00008: early stopping\n",
            "Evaluation\n",
            "Snapshot Evaluation\n",
            "description    roberta-base max pooling with CNN\n",
            "f1                                      0.897216\n",
            "precision                               0.906926\n",
            "recall                                  0.887712\n",
            "Name: 0, dtype: object\n",
            "Snapshot scores: F1 0.9026915113871635, Precision 0.8825910931174089 and Recall 0.923728813559322\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1072\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.5186788439750671\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              lr 1.3999999282532372e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 862.4800825119019\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.005533323623239994\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        accuracy 0.99871426820755\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    val_accuracy 0.8980000019073486\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1598768639.4396966\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.2539614140987396\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced light-plant-16: https://app.wandb.ai/victor7246/wnut-task2/runs/35aajx26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B0SKMz5efw1",
        "colab_type": "text"
      },
      "source": [
        "#### RoBERTa-base-NLI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvtCg9grHjQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model14/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_KfjRJNY6fR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c12a1b7a-cb95-45ac-f94c-2e206e7112da"
      },
      "source": [
        "!cd WNUT-2020-Task-2/experiments/ && python exp7.py --model_save_path {model_dir}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-08-30 08:08:07.050141: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "100% 7000/7000 [00:12<00:00, 582.35it/s]\n",
            "100% 1000/1000 [00:01<00:00, 685.46it/s]\n",
            "Tokenization\n",
            "Downloading: 100% 678/678 [00:00<00:00, 484kB/s]\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 2.13MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.36MB/s]\n",
            "Downloading: 100% 150/150 [00:00<00:00, 110kB/s]\n",
            "Downloading: 100% 25.0/25.0 [00:00<00:00, 16.7kB/s]\n",
            "100% 14000/14000 [00:08<00:00, 1700.53it/s]\n",
            "100% 2000/2000 [00:01<00:00, 1843.22it/s]\n",
            "Modelling\n",
            "2020-08-30 08:08:44.576796: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-30 08:08:44.595256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 08:08:44.596169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-30 08:08:44.596235: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-30 08:08:44.596386: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-30 08:08:44.596509: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-30 08:08:44.596591: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-30 08:08:44.596671: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-30 08:08:44.596742: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-30 08:08:44.596819: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-30 08:08:44.596989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 08:08:44.597870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 08:08:44.598733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-30 08:08:44.616125: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-08-30 08:08:44.616399: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x9ea8540 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-30 08:08:44.616437: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-30 08:08:44.707309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 08:08:44.708182: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x9ea8700 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-30 08:08:44.708217: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2020-08-30 08:08:44.708511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 08:08:44.709204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2020-08-30 08:08:44.709274: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-30 08:08:44.709348: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-30 08:08:44.709405: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-30 08:08:44.709469: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-30 08:08:44.709524: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-30 08:08:44.709583: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-30 08:08:44.709649: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-30 08:08:44.709808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 08:08:44.710624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 08:08:44.711285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-30 08:08:44.716400: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-30 08:08:57.661611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-30 08:08:57.661682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-30 08:08:57.661702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-30 08:08:57.726391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 08:08:57.727260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-30 08:08:57.727999: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-30 08:08:57.728057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10628 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2020-08-30 08:08:58.100111: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "Downloading: 100% 501M/501M [00:18<00:00, 26.5MB/s]\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_roberta_model_1 (TFRobertaMo ((None, 100, 768), ( 124645632   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_76 (Dropout)            (None, 768)          0           tf_roberta_model_1[0][1]         \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            769         dropout_76[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 124,646,401\n",
            "Trainable params: 124,646,401\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200830_080925-37kidvjz\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlemon-tree-17\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2/runs/37kidvjz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "Epoch 1/15\n",
            "438/438 [==============================] - ETA: 0s - loss: 0.2205 - accuracy: 0.8995\n",
            "Score 0.8864321608040201. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model14/model.h5.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: \n",
            "438/438 [==============================] - 612s 1s/step - loss: 0.2205 - accuracy: 0.8995 - val_loss: 0.3272 - val_accuracy: 0.8870\n",
            "Epoch 2/15\n",
            "438/438 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9804\n",
            "Score 0.8935532233883058. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model14/model.h5.\n",
            "438/438 [==============================] - 608s 1s/step - loss: 0.0612 - accuracy: 0.9804 - val_loss: 0.4806 - val_accuracy: 0.8935\n",
            "Epoch 3/15\n",
            "438/438 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9932\n",
            "Score 0.8932527693857. Model not saved.\n",
            "438/438 [==============================] - 602s 1s/step - loss: 0.0235 - accuracy: 0.9932 - val_loss: 0.5169 - val_accuracy: 0.8940\n",
            "Epoch 4/15\n",
            "438/438 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9954\n",
            "Score 0.8874371859296483. Model not saved.\n",
            "438/438 [==============================] - 601s 1s/step - loss: 0.0154 - accuracy: 0.9954 - val_loss: 0.5459 - val_accuracy: 0.8880\n",
            "Epoch 5/15\n",
            "438/438 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9966\n",
            "Score 0.9061102831594635. Model saved in /content/drive/My Drive/Models/WNUT-Task2/model14/model.h5.\n",
            "438/438 [==============================] - 606s 1s/step - loss: 0.0114 - accuracy: 0.9966 - val_loss: 0.4980 - val_accuracy: 0.9055\n",
            "Epoch 6/15\n",
            "438/438 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9946\n",
            "Score 0.8902077151335311. Model not saved.\n",
            "438/438 [==============================] - 601s 1s/step - loss: 0.0189 - accuracy: 0.9946 - val_loss: 0.4659 - val_accuracy: 0.8890\n",
            "Epoch 7/15\n",
            "438/438 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9959\n",
            "Score 0.901352028042063. Model not saved.\n",
            "438/438 [==============================] - 600s 1s/step - loss: 0.0134 - accuracy: 0.9959 - val_loss: 0.5462 - val_accuracy: 0.9015\n",
            "Epoch 8/15\n",
            "438/438 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.9976\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.3999999646330251e-05.\n",
            "\n",
            "Score 0.9013386217154189. Model not saved.\n",
            "438/438 [==============================] - 600s 1s/step - loss: 0.0084 - accuracy: 0.9976 - val_loss: 0.6217 - val_accuracy: 0.9005\n",
            "Epoch 9/15\n",
            "438/438 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9984\n",
            "Score 0.9048330842052815. Model not saved.\n",
            "438/438 [==============================] - 602s 1s/step - loss: 0.0064 - accuracy: 0.9984 - val_loss: 0.7488 - val_accuracy: 0.9045\n",
            "Epoch 10/15\n",
            "438/438 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 0.9983Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Score 0.9061102831594635. Model not saved.\n",
            "\n",
            "Epoch 9: early stopping.\n",
            "438/438 [==============================] - 602s 1s/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.6470 - val_accuracy: 0.8910\n",
            "Epoch 00010: early stopping\n",
            "NLI Evaluation\n",
            "NLI scores: \n",
            "F1 0.9061102831594635, Precision 0.9002961500493584 and Recall 0.912\n",
            "Original Evaluation\n",
            "Traceback (most recent call last):\n",
            "  File \"exp7.py\", line 247, in <module>\n",
            "    main(args)\n",
            "  File \"exp7.py\", line 163, in main\n",
            "    assert nli_val_df_.shape[0] == val_df.shape[0]\n",
            "AssertionError\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1032\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program failed with code 1. Press ctrl-c to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    val_accuracy 0.890999972820282\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      _timestamp 1598781006.438064\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss 0.005208892747759819\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              lr 1.3999999282532372e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _runtime 6120.172523498535\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           _step 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_loss 0.6469828486442566\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        accuracy 0.998285710811615\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_loss 0.32718712091445923\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_epoch 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced lemon-tree-17: https://app.wandb.ai/victor7246/wnut-task2/runs/37kidvjz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAIoXPNl1e0b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba0fe030-52bd-4fee-b4a9-f3a8772a640e"
      },
      "source": [
        "!!cd WNUT-2020-Task-2/experiments/ && python exp7.py --model_save_path {model_dir}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"\\x1b[34m\\x1b[1mwandb\\x1b[0m: \\x1b[33mWARNING\\x1b[0m If you're specifying your api key in code, ensure this code is not shared publically.\",\n",
              " '\\x1b[34m\\x1b[1mwandb\\x1b[0m: \\x1b[33mWARNING\\x1b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.',\n",
              " '\\x1b[34m\\x1b[1mwandb\\x1b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc',\n",
              " '\\x1b[34m\\x1b[1mwandb\\x1b[0m: \\x1b[33mWARNING\\x1b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras',\n",
              " '2020-08-30 12:32:45.324606: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1',\n",
              " 'exp7.py:40: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.',\n",
              " '  pd.options.display.max_colwidth=-1',\n",
              " '',\n",
              " '  0% 0/7000 [00:00<?, ?it/s]',\n",
              " '  1% 68/7000 [00:00<00:10, 679.93it/s]',\n",
              " '  2% 141/7000 [00:00<00:09, 692.48it/s]',\n",
              " '  3% 209/7000 [00:00<00:09, 686.71it/s]',\n",
              " '  4% 275/7000 [00:00<00:09, 675.97it/s]',\n",
              " '  5% 346/7000 [00:00<00:09, 685.52it/s]',\n",
              " '  6% 416/7000 [00:00<00:09, 688.29it/s]',\n",
              " '  7% 487/7000 [00:00<00:09, 691.98it/s]',\n",
              " '  8% 555/7000 [00:00<00:09, 688.14it/s]',\n",
              " '  9% 623/7000 [00:00<00:09, 683.95it/s]',\n",
              " ' 10% 689/7000 [00:01<00:09, 664.60it/s]',\n",
              " ' 11% 757/7000 [00:01<00:09, 667.02it/s]',\n",
              " ' 12% 823/7000 [00:01<00:09, 646.95it/s]',\n",
              " ' 13% 888/7000 [00:01<00:09, 647.58it/s]',\n",
              " ' 14% 953/7000 [00:01<00:09, 634.94it/s]',\n",
              " ' 15% 1019/7000 [00:01<00:09, 640.99it/s]',\n",
              " ' 16% 1088/7000 [00:01<00:09, 654.63it/s]',\n",
              " ' 16% 1154/7000 [00:01<00:09, 645.20it/s]',\n",
              " ' 17% 1221/7000 [00:01<00:08, 651.20it/s]',\n",
              " ' 18% 1287/7000 [00:01<00:08, 649.12it/s]',\n",
              " ' 19% 1352/7000 [00:02<00:08, 632.52it/s]',\n",
              " ' 20% 1416/7000 [00:02<00:08, 634.29it/s]',\n",
              " ' 21% 1480/7000 [00:02<00:08, 628.24it/s]',\n",
              " ' 22% 1543/7000 [00:02<00:08, 625.05it/s]',\n",
              " ' 23% 1606/7000 [00:02<00:08, 612.16it/s]',\n",
              " ' 24% 1668/7000 [00:02<00:08, 613.47it/s]',\n",
              " ' 25% 1730/7000 [00:02<00:08, 610.01it/s]',\n",
              " ' 26% 1793/7000 [00:02<00:08, 614.90it/s]',\n",
              " ' 26% 1855/7000 [00:02<00:08, 614.25it/s]',\n",
              " ' 27% 1920/7000 [00:02<00:08, 623.28it/s]',\n",
              " ' 28% 1983/7000 [00:03<00:08, 613.97it/s]',\n",
              " ' 29% 2045/7000 [00:03<00:08, 610.18it/s]',\n",
              " ' 30% 2108/7000 [00:03<00:07, 613.19it/s]',\n",
              " ' 31% 2172/7000 [00:03<00:07, 618.37it/s]',\n",
              " ' 32% 2234/7000 [00:03<00:07, 606.98it/s]',\n",
              " ' 33% 2297/7000 [00:03<00:07, 610.85it/s]',\n",
              " ' 34% 2359/7000 [00:03<00:07, 612.05it/s]',\n",
              " ' 35% 2421/7000 [00:03<00:07, 612.43it/s]',\n",
              " ' 35% 2484/7000 [00:03<00:07, 616.45it/s]',\n",
              " ' 36% 2546/7000 [00:03<00:07, 616.46it/s]',\n",
              " ' 37% 2608/7000 [00:04<00:07, 589.75it/s]',\n",
              " ' 38% 2668/7000 [00:04<00:07, 581.32it/s]',\n",
              " ' 39% 2731/7000 [00:04<00:07, 593.82it/s]',\n",
              " ' 40% 2791/7000 [00:04<00:07, 587.49it/s]',\n",
              " ' 41% 2850/7000 [00:04<00:07, 586.26it/s]',\n",
              " ' 42% 2912/7000 [00:04<00:06, 595.92it/s]',\n",
              " ' 42% 2972/7000 [00:04<00:06, 592.09it/s]',\n",
              " ' 43% 3034/7000 [00:04<00:06, 600.19it/s]',\n",
              " ' 44% 3096/7000 [00:04<00:06, 603.66it/s]',\n",
              " ' 45% 3157/7000 [00:05<00:06, 598.46it/s]',\n",
              " ' 46% 3217/7000 [00:05<00:06, 574.35it/s]',\n",
              " ' 47% 3275/7000 [00:05<00:06, 572.80it/s]',\n",
              " ' 48% 3333/7000 [00:05<00:06, 572.84it/s]',\n",
              " ' 48% 3391/7000 [00:05<00:06, 552.30it/s]',\n",
              " ' 49% 3447/7000 [00:05<00:06, 550.47it/s]',\n",
              " ' 50% 3504/7000 [00:05<00:06, 554.23it/s]',\n",
              " ' 51% 3560/7000 [00:05<00:06, 554.49it/s]',\n",
              " ' 52% 3618/7000 [00:05<00:06, 560.43it/s]',\n",
              " ' 53% 3677/7000 [00:05<00:05, 566.89it/s]',\n",
              " ' 53% 3734/7000 [00:06<00:05, 562.63it/s]',\n",
              " ' 54% 3791/7000 [00:06<00:05, 561.60it/s]',\n",
              " ' 55% 3849/7000 [00:06<00:05, 565.75it/s]',\n",
              " ' 56% 3906/7000 [00:06<00:05, 564.01it/s]',\n",
              " ' 57% 3963/7000 [00:06<00:05, 554.11it/s]',\n",
              " ' 57% 4020/7000 [00:06<00:05, 557.66it/s]',\n",
              " ' 58% 4076/7000 [00:06<00:05, 553.62it/s]',\n",
              " ' 59% 4133/7000 [00:06<00:05, 557.29it/s]',\n",
              " ' 60% 4190/7000 [00:06<00:05, 560.51it/s]',\n",
              " ' 61% 4247/7000 [00:07<00:05, 537.65it/s]',\n",
              " ' 61% 4301/7000 [00:07<00:05, 533.75it/s]',\n",
              " ' 62% 4355/7000 [00:07<00:05, 522.21it/s]',\n",
              " ' 63% 4408/7000 [00:07<00:04, 521.42it/s]',\n",
              " ' 64% 4465/7000 [00:07<00:04, 534.19it/s]',\n",
              " ' 65% 4519/7000 [00:07<00:04, 531.80it/s]',\n",
              " ' 65% 4574/7000 [00:07<00:04, 535.36it/s]',\n",
              " ' 66% 4632/7000 [00:07<00:04, 545.47it/s]',\n",
              " ' 67% 4690/7000 [00:07<00:04, 553.28it/s]',\n",
              " ' 68% 4746/7000 [00:07<00:04, 549.03it/s]',\n",
              " ' 69% 4801/7000 [00:08<00:04, 545.17it/s]',\n",
              " ' 69% 4856/7000 [00:08<00:03, 537.01it/s]',\n",
              " ' 70% 4910/7000 [00:08<00:03, 536.88it/s]',\n",
              " ' 71% 4965/7000 [00:08<00:03, 539.07it/s]',\n",
              " ' 72% 5020/7000 [00:08<00:03, 541.82it/s]',\n",
              " ' 72% 5075/7000 [00:08<00:03, 522.42it/s]',\n",
              " ' 73% 5131/7000 [00:08<00:03, 530.79it/s]',\n",
              " ' 74% 5186/7000 [00:08<00:03, 534.27it/s]',\n",
              " ' 75% 5240/7000 [00:08<00:03, 533.10it/s]',\n",
              " ' 76% 5294/7000 [00:08<00:03, 527.66it/s]',\n",
              " ' 76% 5348/7000 [00:09<00:03, 529.81it/s]',\n",
              " ' 77% 5402/7000 [00:09<00:03, 521.86it/s]',\n",
              " ' 78% 5458/7000 [00:09<00:02, 531.37it/s]',\n",
              " ' 79% 5513/7000 [00:09<00:02, 536.33it/s]',\n",
              " ' 80% 5567/7000 [00:09<00:02, 534.91it/s]',\n",
              " ' 80% 5621/7000 [00:09<00:02, 522.15it/s]',\n",
              " ' 81% 5676/7000 [00:09<00:02, 527.43it/s]',\n",
              " ' 82% 5729/7000 [00:09<00:02, 525.88it/s]',\n",
              " ' 83% 5784/7000 [00:09<00:02, 530.85it/s]',\n",
              " ' 83% 5838/7000 [00:09<00:02, 521.34it/s]',\n",
              " ' 84% 5891/7000 [00:10<00:02, 522.38it/s]',\n",
              " ' 85% 5944/7000 [00:10<00:02, 517.03it/s]',\n",
              " ' 86% 6001/7000 [00:10<00:01, 529.69it/s]',\n",
              " ' 87% 6058/7000 [00:10<00:01, 540.37it/s]',\n",
              " ' 87% 6113/7000 [00:10<00:01, 515.81it/s]',\n",
              " ' 88% 6165/7000 [00:10<00:01, 499.55it/s]',\n",
              " ' 89% 6219/7000 [00:10<00:01, 509.48it/s]',\n",
              " ' 90% 6272/7000 [00:10<00:01, 514.77it/s]',\n",
              " ' 90% 6326/7000 [00:10<00:01, 519.51it/s]',\n",
              " ' 91% 6379/7000 [00:11<00:01, 505.86it/s]',\n",
              " ' 92% 6430/7000 [00:11<00:01, 505.00it/s]',\n",
              " ' 93% 6481/7000 [00:11<00:01, 496.18it/s]',\n",
              " ' 93% 6531/7000 [00:11<00:00, 496.62it/s]',\n",
              " ' 94% 6584/7000 [00:11<00:00, 504.03it/s]',\n",
              " ' 95% 6635/7000 [00:11<00:00, 501.44it/s]',\n",
              " ' 96% 6686/7000 [00:11<00:00, 493.06it/s]',\n",
              " ' 96% 6739/7000 [00:11<00:00, 503.32it/s]',\n",
              " ' 97% 6791/7000 [00:11<00:00, 506.17it/s]',\n",
              " ' 98% 6842/7000 [00:11<00:00, 502.37it/s]',\n",
              " ' 98% 6893/7000 [00:12<00:00, 494.41it/s]',\n",
              " ' 99% 6943/7000 [00:12<00:00, 493.02it/s]',\n",
              " '100% 6993/7000 [00:12<00:00, 485.70it/s]',\n",
              " '100% 7000/7000 [00:12<00:00, 569.05it/s]',\n",
              " '',\n",
              " '  0% 0/1000 [00:00<?, ?it/s]',\n",
              " '  8% 75/1000 [00:00<00:01, 746.86it/s]',\n",
              " ' 15% 148/1000 [00:00<00:01, 740.77it/s]',\n",
              " ' 21% 206/1000 [00:00<00:01, 681.09it/s]',\n",
              " ' 28% 278/1000 [00:00<00:01, 691.02it/s]',\n",
              " ' 35% 351/1000 [00:00<00:00, 700.00it/s]',\n",
              " ' 42% 422/1000 [00:00<00:00, 702.94it/s]',\n",
              " ' 49% 489/1000 [00:00<00:00, 691.64it/s]',\n",
              " ' 56% 561/1000 [00:00<00:00, 699.61it/s]',\n",
              " ' 63% 629/1000 [00:00<00:00, 689.46it/s]',\n",
              " ' 70% 699/1000 [00:01<00:00, 689.81it/s]',\n",
              " ' 77% 770/1000 [00:01<00:00, 693.51it/s]',\n",
              " ' 84% 838/1000 [00:01<00:00, 675.05it/s]',\n",
              " ' 90% 905/1000 [00:01<00:00, 653.41it/s]',\n",
              " ' 98% 975/1000 [00:01<00:00, 665.48it/s]',\n",
              " '100% 1000/1000 [00:01<00:00, 681.86it/s]',\n",
              " 'Tokenization',\n",
              " '',\n",
              " '  0% 0/14000 [00:00<?, ?it/s]',\n",
              " '  1% 123/14000 [00:00<00:11, 1212.75it/s]',\n",
              " '  2% 263/14000 [00:00<00:10, 1262.74it/s]',\n",
              " '  3% 419/14000 [00:00<00:10, 1336.24it/s]',\n",
              " '  4% 581/14000 [00:00<00:09, 1409.98it/s]',\n",
              " '  5% 745/14000 [00:00<00:09, 1471.58it/s]',\n",
              " '  6% 889/14000 [00:00<00:08, 1459.42it/s]',\n",
              " '  7% 1045/14000 [00:00<00:08, 1486.01it/s]',\n",
              " '  9% 1209/14000 [00:00<00:08, 1527.96it/s]',\n",
              " ' 10% 1356/14000 [00:00<00:08, 1505.11it/s]',\n",
              " ' 11% 1523/14000 [00:01<00:08, 1547.60it/s]',\n",
              " ' 12% 1686/14000 [00:01<00:07, 1570.95it/s]',\n",
              " ' 13% 1842/14000 [00:01<00:07, 1544.33it/s]',\n",
              " ' 14% 2009/14000 [00:01<00:07, 1576.64it/s]',\n",
              " ' 15% 2167/14000 [00:01<00:07, 1565.00it/s]',\n",
              " ' 17% 2333/14000 [00:01<00:07, 1590.65it/s]',\n",
              " ' 18% 2492/14000 [00:01<00:07, 1588.22it/s]',\n",
              " ' 19% 2664/14000 [00:01<00:06, 1623.76it/s]',\n",
              " ' 20% 2828/14000 [00:01<00:06, 1627.26it/s]',\n",
              " ' 21% 2991/14000 [00:01<00:06, 1582.34it/s]',\n",
              " ' 23% 3156/14000 [00:02<00:06, 1601.20it/s]',\n",
              " ' 24% 3336/14000 [00:02<00:06, 1653.79it/s]',\n",
              " ' 25% 3503/14000 [00:02<00:06, 1634.82it/s]',\n",
              " ' 26% 3673/14000 [00:02<00:06, 1651.25it/s]',\n",
              " ' 27% 3843/14000 [00:02<00:06, 1663.62it/s]',\n",
              " ' 29% 4013/14000 [00:02<00:05, 1673.47it/s]',\n",
              " ' 30% 4181/14000 [00:02<00:05, 1640.78it/s]',\n",
              " ' 31% 4350/14000 [00:02<00:05, 1654.84it/s]',\n",
              " ' 32% 4521/14000 [00:02<00:05, 1668.50it/s]',\n",
              " ' 33% 4689/14000 [00:02<00:05, 1640.43it/s]',\n",
              " ' 35% 4858/14000 [00:03<00:05, 1654.31it/s]',\n",
              " ' 36% 5030/14000 [00:03<00:05, 1670.29it/s]',\n",
              " ' 37% 5198/14000 [00:03<00:08, 1095.61it/s]',\n",
              " ' 38% 5379/14000 [00:03<00:06, 1241.25it/s]',\n",
              " ' 40% 5545/14000 [00:03<00:06, 1342.52it/s]',\n",
              " ' 41% 5717/14000 [00:03<00:05, 1434.25it/s]',\n",
              " ' 42% 5880/14000 [00:03<00:05, 1486.58it/s]',\n",
              " ' 43% 6055/14000 [00:03<00:05, 1556.25it/s]',\n",
              " ' 44% 6229/14000 [00:04<00:04, 1604.96it/s]',\n",
              " ' 46% 6403/14000 [00:04<00:04, 1640.46it/s]',\n",
              " ' 47% 6575/14000 [00:04<00:04, 1660.85it/s]',\n",
              " ' 48% 6748/14000 [00:04<00:04, 1680.27it/s]',\n",
              " ' 49% 6920/14000 [00:04<00:04, 1691.93it/s]',\n",
              " ' 51% 7100/14000 [00:04<00:04, 1721.74it/s]',\n",
              " ' 52% 7274/14000 [00:04<00:03, 1707.37it/s]',\n",
              " ' 53% 7455/14000 [00:04<00:03, 1735.52it/s]',\n",
              " ' 55% 7639/14000 [00:04<00:03, 1764.58it/s]',\n",
              " ' 56% 7817/14000 [00:04<00:03, 1768.25it/s]',\n",
              " ' 57% 7995/14000 [00:05<00:03, 1769.77it/s]',\n",
              " ' 58% 8173/14000 [00:05<00:03, 1676.55it/s]',\n",
              " ' 60% 8348/14000 [00:05<00:03, 1695.96it/s]',\n",
              " ' 61% 8525/14000 [00:05<00:03, 1716.31it/s]',\n",
              " ' 62% 8703/14000 [00:05<00:03, 1734.06it/s]',\n",
              " ' 63% 8886/14000 [00:05<00:02, 1761.28it/s]',\n",
              " ' 65% 9063/14000 [00:05<00:02, 1722.29it/s]',\n",
              " ' 66% 9246/14000 [00:05<00:02, 1750.75it/s]',\n",
              " ' 67% 9422/14000 [00:05<00:02, 1749.24it/s]',\n",
              " ' 69% 9603/14000 [00:05<00:02, 1766.02it/s]',\n",
              " ' 70% 9780/14000 [00:06<00:02, 1733.36it/s]',\n",
              " ' 71% 9966/14000 [00:06<00:02, 1767.47it/s]',\n",
              " ' 72% 10144/14000 [00:06<00:02, 1755.10it/s]',\n",
              " ' 74% 10325/14000 [00:06<00:02, 1770.47it/s]',\n",
              " ' 75% 10503/14000 [00:06<00:01, 1772.19it/s]',\n",
              " ' 76% 10695/14000 [00:06<00:01, 1810.85it/s]',\n",
              " ' 78% 10877/14000 [00:06<00:01, 1760.52it/s]',\n",
              " ' 79% 11065/14000 [00:06<00:01, 1792.84it/s]',\n",
              " ' 80% 11245/14000 [00:06<00:01, 1768.41it/s]',\n",
              " ' 82% 11423/14000 [00:06<00:01, 1756.53it/s]',\n",
              " ' 83% 11608/14000 [00:07<00:01, 1782.95it/s]',\n",
              " ' 84% 11796/14000 [00:07<00:01, 1810.05it/s]',\n",
              " ' 86% 11978/14000 [00:07<00:01, 1787.31it/s]',\n",
              " ' 87% 12158/14000 [00:07<00:01, 1787.79it/s]',\n",
              " ' 88% 12346/14000 [00:07<00:00, 1814.31it/s]',\n",
              " ' 90% 12538/14000 [00:07<00:00, 1842.71it/s]',\n",
              " ' 91% 12723/14000 [00:07<00:00, 1812.11it/s]',\n",
              " ' 92% 12905/14000 [00:07<00:00, 1799.80it/s]',\n",
              " ' 94% 13094/14000 [00:07<00:00, 1824.45it/s]',\n",
              " ' 95% 13277/14000 [00:08<00:00, 1811.93it/s]',\n",
              " ' 96% 13459/14000 [00:08<00:00, 1811.68it/s]',\n",
              " ' 98% 13652/14000 [00:08<00:00, 1844.96it/s]',\n",
              " ' 99% 13837/14000 [00:08<00:00, 1796.01it/s]',\n",
              " '100% 14000/14000 [00:08<00:00, 1663.82it/s]',\n",
              " '',\n",
              " '  0% 0/2000 [00:00<?, ?it/s]',\n",
              " '  9% 177/2000 [00:00<00:01, 1768.53it/s]',\n",
              " ' 18% 369/2000 [00:00<00:00, 1810.52it/s]',\n",
              " ' 27% 547/2000 [00:00<00:00, 1800.30it/s]',\n",
              " ' 37% 739/2000 [00:00<00:00, 1832.34it/s]',\n",
              " ' 46% 915/2000 [00:00<00:00, 1808.66it/s]',\n",
              " ' 55% 1103/2000 [00:00<00:00, 1828.64it/s]',\n",
              " ' 65% 1291/2000 [00:00<00:00, 1843.51it/s]',\n",
              " ' 74% 1481/2000 [00:00<00:00, 1856.17it/s]',\n",
              " ' 84% 1673/2000 [00:00<00:00, 1873.66it/s]',\n",
              " ' 93% 1854/2000 [00:01<00:00, 1817.07it/s]',\n",
              " '100% 2000/2000 [00:01<00:00, 1832.64it/s]',\n",
              " 'Modelling',\n",
              " '2020-08-30 12:33:15.771709: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1',\n",
              " '2020-08-30 12:33:15.791954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero',\n",
              " '2020-08-30 12:33:15.792772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: ',\n",
              " 'pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7',\n",
              " 'coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s',\n",
              " '2020-08-30 12:33:15.792818: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1',\n",
              " '2020-08-30 12:33:15.792935: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10',\n",
              " '2020-08-30 12:33:15.793027: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10',\n",
              " '2020-08-30 12:33:15.793122: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10',\n",
              " '2020-08-30 12:33:15.793202: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10',\n",
              " '2020-08-30 12:33:15.793314: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10',\n",
              " '2020-08-30 12:33:15.793391: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7',\n",
              " '2020-08-30 12:33:15.793588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero',\n",
              " '2020-08-30 12:33:15.794398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero',\n",
              " '2020-08-30 12:33:15.795097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0',\n",
              " '2020-08-30 12:33:15.801409: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz',\n",
              " '2020-08-30 12:33:15.801751: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x95bc380 initialized for platform Host (this does not guarantee that XLA will be used). Devices:',\n",
              " '2020-08-30 12:33:15.801809: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version',\n",
              " '2020-08-30 12:33:15.865119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero',\n",
              " '2020-08-30 12:33:15.866025: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x95bc540 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:',\n",
              " '2020-08-30 12:33:15.866060: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7',\n",
              " '2020-08-30 12:33:15.866345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero',\n",
              " '2020-08-30 12:33:15.867124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: ',\n",
              " 'pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7',\n",
              " 'coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s',\n",
              " '2020-08-30 12:33:15.867217: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1',\n",
              " '2020-08-30 12:33:15.867301: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10',\n",
              " '2020-08-30 12:33:15.867363: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10',\n",
              " '2020-08-30 12:33:15.867409: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10',\n",
              " '2020-08-30 12:33:15.867481: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10',\n",
              " '2020-08-30 12:33:15.867538: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10',\n",
              " '2020-08-30 12:33:15.867604: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7',\n",
              " '2020-08-30 12:33:15.867767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero',\n",
              " '2020-08-30 12:33:15.868667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero',\n",
              " '2020-08-30 12:33:15.869397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0',\n",
              " '2020-08-30 12:33:15.869479: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1',\n",
              " '2020-08-30 12:33:16.418365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:',\n",
              " '2020-08-30 12:33:16.418463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 ',\n",
              " '2020-08-30 12:33:16.418497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N ',\n",
              " '2020-08-30 12:33:16.418772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero',\n",
              " '2020-08-30 12:33:16.419657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero',\n",
              " '2020-08-30 12:33:16.420335: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.',\n",
              " '2020-08-30 12:33:16.420393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10628 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)',\n",
              " '2020-08-30 12:33:16.681141: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10',\n",
              " 'Model: \"functional_1\"',\n",
              " '__________________________________________________________________________________________________',\n",
              " 'Layer (type)                    Output Shape         Param #     Connected to                     ',\n",
              " '==================================================================================================',\n",
              " 'input_1 (InputLayer)            [(None, 100)]        0                                            ',\n",
              " '__________________________________________________________________________________________________',\n",
              " 'input_2 (InputLayer)            [(None, 100)]        0                                            ',\n",
              " '__________________________________________________________________________________________________',\n",
              " 'tf_roberta_model_1 (TFRobertaMo ((None, 100, 768), ( 124645632   input_1[0][0]                    ',\n",
              " '                                                                 input_2[0][0]                    ',\n",
              " '__________________________________________________________________________________________________',\n",
              " 'dropout_76 (Dropout)            (None, 768)          0           tf_roberta_model_1[0][1]         ',\n",
              " '__________________________________________________________________________________________________',\n",
              " 'dense (Dense)                   (None, 1)            769         dropout_76[0][0]                 ',\n",
              " '==================================================================================================',\n",
              " 'Total params: 124,646,401',\n",
              " 'Trainable params: 124,646,401',\n",
              " 'Non-trainable params: 0',\n",
              " '__________________________________________________________________________________________________',\n",
              " 'None',\n",
              " 'NLI Evaluation',\n",
              " 'NLI scores: ',\n",
              " 'F1 0.9061102831594635, Precision 0.9002961500493584 and Recall 0.912',\n",
              " 'Original Evaluation',\n",
              " 'Id            1241728922192142336                                                                                                                                                                                                                                                               ',\n",
              " 'words         For those saying Pakistan isn’t Italy; After 3 weeks of the first confirmed case, Pakistan has a higher #Coronavirus growth rate than Italy. Experts on the issue say Italy was too slow to lockdown. Even after lockdown, it took time for it to properly have an impact. #Corona',\n",
              " 'labels        UNINFORMATIVE                                                                                                                                                                                                                                                                     ',\n",
              " 'orig_label    INFORMATIVE                                                                                                                                                                                                                                                                       ',\n",
              " 'Name: 0, dtype: object',\n",
              " 'Id            1241728922192142336                                                                                                                                                                                                                                                               ',\n",
              " 'words         For those saying Pakistan isn’t Italy; After 3 weeks of the first confirmed case, Pakistan has a higher #Coronavirus growth rate than Italy. Experts on the issue say Italy was too slow to lockdown. Even after lockdown, it took time for it to properly have an impact. #Corona',\n",
              " 'labels        0                                                                                                                                                                                                                                                                                 ',\n",
              " 'orig_label    1                                                                                                                                                                                                                                                                                 ',\n",
              " 'Name: 0, dtype: object',\n",
              " 'description    textattack/roberta-base-MNLI with NLI',\n",
              " 'f1             0.904222                             ',\n",
              " 'precision      0.87976                              ',\n",
              " 'recall         0.930085                             ',\n",
              " 'Name: 0, dtype: object']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ztz6P7zeyz7",
        "colab_type": "text"
      },
      "source": [
        "### Torch models with explicit regularizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdiXgZ0be43Z",
        "colab_type": "text"
      },
      "source": [
        "#### Dropouts - 0, 0.1, .2, .3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNefK-Ig3Hfp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c2ccb0d9-5bc7-4d70-e2a9-6fc47ea191e9"
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model15_1/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --model_save_path {model_dir} --dropout 0\n",
        "\n",
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model15_2/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --model_save_path {model_dir} --dropout .1\n",
        "\n",
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model15_3/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --model_save_path {model_dir} --dropout .2\n",
        "\n",
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model15_4/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --model_save_path {model_dir} --dropout .3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-08-31 12:50:19.198453: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "Tokenization\n",
            "100% 7000/7000 [00:03<00:00, 2113.64it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2576.67it/s]\n",
            "Train and val loader length 219 and 32\n",
            "Modelling\n",
            "Transformer(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (drop): Dropout(p=0.0, inplace=False)\n",
            "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
            ")\n",
            "Device: cuda\n",
            "[LOG] Total number of parameters to learn 124646401\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200831_125036-1aagqzu3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33methereal-gorge-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization/runs/1aagqzu3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "  0%|                                                   | 0/219 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Current training Loss 0.073: 100%|████████████| 219/219 [01:08<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.016: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.06: 100%|███████████████████| 32/32 [00:03<00:00, 10.03it/s]\n",
            "Train loss = 0.05 Train metric = 0.984 Val loss = 0.275 Val metric = 0.893\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model15_1/\n",
            "Current training Loss 0.009: 100%|████████████| 219/219 [01:08<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.002: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.034: 100%|██████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.016 Train metric = 0.996 Val loss = 0.332 Val metric = 0.893\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model15_1/\n",
            "Current training Loss 0.007: 100%|████████████| 219/219 [01:08<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.001: 100%|██████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.009 Train metric = 0.997 Val loss = 0.409 Val metric = 0.895\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model15_1/\n",
            "Current training Loss 0.0: 100%|██████████████| 219/219 [01:08<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.004: 100%|██████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.005 Train metric = 0.999 Val loss = 0.388 Val metric = 0.901\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model15_1/\n",
            "Current training Loss 0.108: 100%|████████████| 219/219 [01:09<00:00,  3.16it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100%|████████████████| 219/219 [00:22<00:00,  9.84it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.403: 100%|██████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.019 Train metric = 0.994 Val loss = 0.484 Val metric = 0.895\n",
            "Current training Loss 0.001: 100%|████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.002: 100%|████████████████| 219/219 [00:22<00:00,  9.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.001: 100%|██████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.002 Train metric = 1.0 Val loss = 0.497 Val metric = 0.894\n",
            "Current training Loss 0.003: 100%|████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100%|████████████████| 219/219 [00:22<00:00,  9.85it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.001: 100%|██████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.003 Train metric = 0.999 Val loss = 0.499 Val metric = 0.877\n",
            "Current training Loss 0.0: 100%|██████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [00:22<00:00,  9.85it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.002: 100%|██████████████████| 32/32 [00:03<00:00, 10.03it/s]\n",
            "Train loss = 0.001 Train metric = 1.0 Val loss = 0.552 Val metric = 0.901\n",
            "Current training Loss 0.0: 100%|██████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [00:22<00:00,  9.85it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.001: 100%|██████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.001 Train metric = 1.0 Val loss = 0.574 Val metric = 0.895\n",
            "Early stopping\n",
            "100%|███████████████████████████████████████████| 32/32 [00:03<00:00, 10.26it/s]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0.0, mixout prob 0, ...\n",
            "f1                                                      0.901031\n",
            "precision                                                0.87751\n",
            "recall                                                  0.925847\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1893\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                train_loss 0.0008806790574453771\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_metric 0.8946280991735537\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _timestamp 1598879096.30878\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  val_loss 0.5741199254989624\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_metric 0.9996972449288526\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                     _step 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  _runtime 877.7258067131042\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced ethereal-gorge-1: https://app.wandb.ai/victor7246/wnut-task2-regularization/runs/1aagqzu3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-08-31 13:05:05.496039: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "Tokenization\n",
            "100% 7000/7000 [00:03<00:00, 2174.18it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2628.92it/s]\n",
            "Train and val loader length 219 and 32\n",
            "Modelling\n",
            "Transformer(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (drop): Dropout(p=0.1, inplace=False)\n",
            "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
            ")\n",
            "Device: cuda\n",
            "[LOG] Total number of parameters to learn 124646401\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200831_130521-e09oide3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mearthy-darkness-2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization/runs/e09oide3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "  0%|                                                   | 0/219 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Current training Loss 0.183: 100%|████████████| 219/219 [01:08<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.077: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.08: 100%|███████████████████| 32/32 [00:03<00:00, 10.03it/s]\n",
            "Train loss = 0.075 Train metric = 0.976 Val loss = 0.246 Val metric = 0.888\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model15_2/\n",
            "Current training Loss 0.049: 100%|████████████| 219/219 [01:09<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.023: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.081: 100%|██████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.027 Train metric = 0.991 Val loss = 0.281 Val metric = 0.895\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model15_2/\n",
            "Current training Loss 0.002: 100%|████████████| 219/219 [01:09<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.06: 100%|███████████████████| 32/32 [00:03<00:00, 10.03it/s]\n",
            "Train loss = 0.007 Train metric = 0.998 Val loss = 0.39 Val metric = 0.903\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model15_2/\n",
            "Current training Loss 0.002: 100%|████████████| 219/219 [01:09<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.014: 100%|██████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.008 Train metric = 0.997 Val loss = 0.45 Val metric = 0.902\n",
            "Current training Loss 0.002: 100%|████████████| 219/219 [01:09<00:00,  3.15it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.002: 100%|██████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.008 Train metric = 0.998 Val loss = 0.393 Val metric = 0.912\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model15_2/\n",
            "Current training Loss 0.002: 100%|████████████| 219/219 [01:09<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [00:22<00:00,  9.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.015: 100%|██████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.02 Train metric = 0.993 Val loss = 0.538 Val metric = 0.896\n",
            "Current training Loss 0.001: 100%|████████████| 219/219 [01:08<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [00:22<00:00,  9.84it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.001: 100%|██████████████████| 32/32 [00:03<00:00, 10.03it/s]\n",
            "Train loss = 0.003 Train metric = 0.999 Val loss = 0.451 Val metric = 0.899\n",
            "Current training Loss 0.0: 100%|██████████████| 219/219 [01:08<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [00:22<00:00,  9.84it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.0: 100%|████████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.003 Train metric = 0.999 Val loss = 0.457 Val metric = 0.908\n",
            "Current training Loss 0.0: 100%|██████████████| 219/219 [01:09<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.0: 100%|████████████████████| 32/32 [00:03<00:00, 10.02it/s]\n",
            "Train loss = 0.003 Train metric = 0.999 Val loss = 0.474 Val metric = 0.898\n",
            "Current training Loss 0.0: 100%|██████████████| 219/219 [01:09<00:00,  3.15it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [00:22<00:00,  9.84it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.0: 100%|████████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.001 Train metric = 0.999 Val loss = 0.489 Val metric = 0.91\n",
            "Early stopping\n",
            "100%|███████████████████████████████████████████| 32/32 [00:03<00:00, 10.26it/s]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0.1, mixout prob 0, ...\n",
            "f1                                                      0.911917\n",
            "precision                                               0.892495\n",
            "recall                                                  0.932203\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2031\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_metric 0.9098360655737705\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_metric 0.9993944898577051\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _timestamp 1598880081.9505172\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  val_loss 0.4886886775493622\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                     _step 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                train_loss 0.0012674171011894941\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  _runtime 977.1055665016174\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced earthy-darkness-2: https://app.wandb.ai/victor7246/wnut-task2-regularization/runs/e09oide3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-08-31 13:21:31.529130: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "Tokenization\n",
            "100% 7000/7000 [00:03<00:00, 2145.31it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2689.35it/s]\n",
            "Train and val loader length 219 and 32\n",
            "Modelling\n",
            "Transformer(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (drop): Dropout(p=0.2, inplace=False)\n",
            "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
            ")\n",
            "Device: cuda\n",
            "[LOG] Total number of parameters to learn 124646401\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200831_132147-2pabmmdq\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcerulean-feather-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization/runs/2pabmmdq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "  0%|                                                   | 0/219 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Current training Loss 0.116: 100%|████████████| 219/219 [01:09<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.137: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.453: 100%|██████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.139 Train metric = 0.95 Val loss = 0.351 Val metric = 0.881\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model15_3/\n",
            "Current training Loss 0.034: 100%|████████████| 219/219 [01:09<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.007: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.243: 100%|██████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.046 Train metric = 0.983 Val loss = 0.298 Val metric = 0.891\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model15_3/\n",
            "Current training Loss 0.008: 100%|████████████| 219/219 [01:09<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.004: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.025: 100%|██████████████████| 32/32 [00:03<00:00, 10.03it/s]\n",
            "Train loss = 0.031 Train metric = 0.99 Val loss = 0.297 Val metric = 0.885\n",
            "Current training Loss 0.004: 100%|████████████| 219/219 [01:08<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.009: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.137: 100%|██████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.028 Train metric = 0.991 Val loss = 0.375 Val metric = 0.91\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model15_3/\n",
            "Current training Loss 0.02: 100%|█████████████| 219/219 [01:09<00:00,  3.15it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.01: 100%|█████████████████| 219/219 [00:22<00:00,  9.84it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.459: 100%|██████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.031 Train metric = 0.989 Val loss = 0.471 Val metric = 0.906\n",
            "Current training Loss 0.001: 100%|████████████| 219/219 [01:08<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100%|████████████████| 219/219 [00:22<00:00,  9.76it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.003: 100%|██████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.021 Train metric = 0.993 Val loss = 0.491 Val metric = 0.905\n",
            "Current training Loss 0.001: 100%|████████████| 219/219 [01:08<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100%|████████████████| 219/219 [00:22<00:00,  9.85it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.004: 100%|██████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.021 Train metric = 0.992 Val loss = 0.506 Val metric = 0.905\n",
            "Current training Loss 0.0: 100%|██████████████| 219/219 [01:08<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [00:22<00:00,  9.84it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.004: 100%|██████████████████| 32/32 [00:03<00:00, 10.03it/s]\n",
            "Train loss = 0.008 Train metric = 0.997 Val loss = 0.492 Val metric = 0.907\n",
            "Current training Loss 0.011: 100%|████████████| 219/219 [01:08<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [00:22<00:00,  9.84it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.0: 100%|████████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.002 Train metric = 0.999 Val loss = 0.452 Val metric = 0.901\n",
            "Early stopping\n",
            "100%|███████████████████████████████████████████| 32/32 [00:03<00:00, 10.25it/s]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0.2, mixout prob 0, ...\n",
            "f1                                                      0.909639\n",
            "precision                                               0.864504\n",
            "recall                                                  0.959746\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2331\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                     _step 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                train_loss 0.0024022578727453947\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_metric 0.9992427684385886\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_metric 0.9014675052410902\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _timestamp 1598880969.4094472\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  _runtime 878.5089972019196\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  val_loss 0.45223724842071533\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced cerulean-feather-3: https://app.wandb.ai/victor7246/wnut-task2-regularization/runs/2pabmmdq\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-08-31 13:36:18.155603: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "Tokenization\n",
            "100% 7000/7000 [00:03<00:00, 2173.37it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2629.52it/s]\n",
            "Train and val loader length 219 and 32\n",
            "Modelling\n",
            "Transformer(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (drop): Dropout(p=0.3, inplace=False)\n",
            "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
            ")\n",
            "Device: cuda\n",
            "[LOG] Total number of parameters to learn 124646401\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200831_133634-2dmmb23v\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbrisk-sky-4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization/runs/2dmmb23v\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "  0%|                                                   | 0/219 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Current training Loss 0.13: 100%|█████████████| 219/219 [01:08<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.08: 100%|█████████████████| 219/219 [00:22<00:00,  9.85it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.383: 100%|██████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.22 Train metric = 0.931 Val loss = 0.458 Val metric = 0.875\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model15_4/\n",
            "Current training Loss 0.052: 100%|████████████| 219/219 [01:09<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.034: 100%|████████████████| 219/219 [00:22<00:00,  9.84it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.125: 100%|██████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.107 Train metric = 0.964 Val loss = 0.358 Val metric = 0.891\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model15_4/\n",
            "Current training Loss 0.021: 100%|████████████| 219/219 [01:09<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.02: 100%|█████████████████| 219/219 [00:22<00:00,  9.84it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.15: 100%|███████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.074 Train metric = 0.975 Val loss = 0.385 Val metric = 0.885\n",
            "Current training Loss 0.055: 100%|████████████| 219/219 [01:08<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.006: 100%|████████████████| 219/219 [00:22<00:00,  9.84it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.03: 100%|███████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.047 Train metric = 0.984 Val loss = 0.349 Val metric = 0.891\n",
            "Current training Loss 0.019: 100%|████████████| 219/219 [01:09<00:00,  3.16it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.057: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.138: 100%|██████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.085 Train metric = 0.975 Val loss = 0.479 Val metric = 0.899\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model15_4/\n",
            "Current training Loss 0.006: 100%|████████████| 219/219 [01:09<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.002: 100%|████████████████| 219/219 [00:22<00:00,  9.76it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.025: 100%|██████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.057 Train metric = 0.984 Val loss = 0.492 Val metric = 0.897\n",
            "Current training Loss 0.003: 100%|████████████| 219/219 [01:08<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.133: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.025: 100%|██████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.057 Train metric = 0.985 Val loss = 0.538 Val metric = 0.898\n",
            "Current training Loss 0.214: 100%|████████████| 219/219 [01:08<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.094: 100%|████████████████| 219/219 [00:22<00:00,  9.84it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.003: 100%|██████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.049 Train metric = 0.988 Val loss = 0.473 Val metric = 0.902\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model15_4/\n",
            "Current training Loss 0.003: 100%|████████████| 219/219 [01:09<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100%|████████████████| 219/219 [00:22<00:00,  9.84it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.003: 100%|██████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.044 Train metric = 0.988 Val loss = 0.5 Val metric = 0.9\n",
            "Current training Loss 0.011: 100%|████████████| 219/219 [01:09<00:00,  3.15it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [00:22<00:00,  9.84it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.017: 100%|██████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.041 Train metric = 0.987 Val loss = 0.534 Val metric = 0.902\n",
            "Current training Loss 0.004: 100%|████████████| 219/219 [01:08<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [00:22<00:00,  9.84it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.005: 100%|██████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.024 Train metric = 0.992 Val loss = 0.515 Val metric = 0.902\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model15_4/\n",
            "Current training Loss 0.004: 100%|████████████| 219/219 [01:09<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [00:22<00:00,  9.84it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.001: 100%|██████████████████| 32/32 [00:03<00:00, 10.06it/s]\n",
            "Train loss = 0.026 Train metric = 0.992 Val loss = 0.513 Val metric = 0.902\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model15_4/\n",
            "Current training Loss 0.002: 100%|████████████| 219/219 [01:09<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [00:22<00:00,  9.77it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.003: 100%|██████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.034 Train metric = 0.99 Val loss = 0.599 Val metric = 0.899\n",
            "Current training Loss 0.001: 100%|████████████| 219/219 [01:09<00:00,  3.16it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [00:22<00:00,  9.84it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.002: 100%|██████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.029 Train metric = 0.992 Val loss = 0.601 Val metric = 0.898\n",
            "Current training Loss 0.001: 100%|████████████| 219/219 [01:08<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [00:22<00:00,  9.85it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.003: 100%|██████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.035 Train metric = 0.991 Val loss = 0.632 Val metric = 0.896\n",
            "100%|███████████████████████████████████████████| 32/32 [00:03<00:00, 10.28it/s]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0.3, mixout prob 0, ...\n",
            "f1                                                      0.902316\n",
            "precision                                               0.859885\n",
            "recall                                                  0.949153\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2574\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                train_loss 0.0348108746111393\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_metric 0.8957298907646475\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_metric 0.9911477869467368\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                     _step 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  _runtime 1457.8660266399384\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  val_loss 0.631822943687439\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _timestamp 1598882435.3783221\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced brisk-sky-4: https://app.wandb.ai/victor7246/wnut-task2-regularization/runs/2dmmb23v\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lam9YfZUfHVZ",
        "colab_type": "text"
      },
      "source": [
        "#### Mixout - .5, .6, .7, .8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnJHwksa_9R1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "894371c3-8f16-4a49-def0-b883d9b3affa"
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model16/1/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --model_save_path {model_dir} --mixout_prob .5 --lightning true --epochs 2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-02 08:47:41.216649: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "100% 7000/7000 [00:04<00:00, 1702.81it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1914.42it/s]\n",
            "Train and val loader length 219 and 32\n",
            "Modelling\n",
            "Device: cuda\n",
            "TransformerWithMixout(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (drop): Dropout(p=0, inplace=False)\n",
            "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory /content/drive/My Drive/Models/WNUT-Task2/model16/1/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "[LOG] Total number of parameters to learn 124646401\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name   | Type                  | Params\n",
            "-------------------------------------------------\n",
            "0 | model  | TransformerWithMixout | 124 M \n",
            "1 | metric | PLAccuracy            | 0     \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  2.43it/s]val loss = 0.703 val metric = 0.438 \n",
            "Epoch 0:   0% 0/251 [00:00<?, ?it/s] /usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Epoch 0:  87% 219/251 [03:46<00:33,  1.04s/it, loss=0.162, v_num=2]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  88% 220/251 [03:47<00:32,  1.03s/it, loss=0.162, v_num=2]\n",
            "Epoch 0:  88% 221/251 [03:47<00:30,  1.03s/it, loss=0.162, v_num=2]\n",
            "Epoch 0:  88% 222/251 [03:47<00:29,  1.03s/it, loss=0.162, v_num=2]\n",
            "Epoch 0:  89% 223/251 [03:48<00:28,  1.02s/it, loss=0.162, v_num=2]\n",
            "Epoch 0:  89% 224/251 [03:48<00:27,  1.02s/it, loss=0.162, v_num=2]\n",
            "Epoch 0:  90% 225/251 [03:48<00:26,  1.02s/it, loss=0.162, v_num=2]\n",
            "Epoch 0:  90% 226/251 [03:49<00:25,  1.01s/it, loss=0.162, v_num=2]\n",
            "Epoch 0:  90% 227/251 [03:49<00:24,  1.01s/it, loss=0.162, v_num=2]\n",
            "Epoch 0:  91% 228/251 [03:50<00:23,  1.01s/it, loss=0.162, v_num=2]\n",
            "Epoch 0:  91% 229/251 [03:50<00:22,  1.01s/it, loss=0.162, v_num=2]\n",
            "Epoch 0:  92% 230/251 [03:50<00:21,  1.00s/it, loss=0.162, v_num=2]\n",
            "Epoch 0:  92% 231/251 [03:51<00:20,  1.00s/it, loss=0.162, v_num=2]\n",
            "Epoch 0:  92% 232/251 [03:51<00:18,  1.00it/s, loss=0.162, v_num=2]\n",
            "Epoch 0:  93% 233/251 [03:51<00:17,  1.00it/s, loss=0.162, v_num=2]\n",
            "Epoch 0:  93% 234/251 [03:52<00:16,  1.01it/s, loss=0.162, v_num=2]\n",
            "Epoch 0:  94% 235/251 [03:52<00:15,  1.01it/s, loss=0.162, v_num=2]\n",
            "Epoch 0:  94% 236/251 [03:52<00:14,  1.01it/s, loss=0.162, v_num=2]\n",
            "Epoch 0:  94% 237/251 [03:53<00:13,  1.02it/s, loss=0.162, v_num=2]\n",
            "Epoch 0:  95% 238/251 [03:53<00:12,  1.02it/s, loss=0.162, v_num=2]\n",
            "Epoch 0:  95% 239/251 [03:54<00:11,  1.02it/s, loss=0.162, v_num=2]\n",
            "Epoch 0:  96% 240/251 [03:54<00:10,  1.02it/s, loss=0.162, v_num=2]\n",
            "Epoch 0:  96% 241/251 [03:54<00:09,  1.03it/s, loss=0.162, v_num=2]\n",
            "Epoch 0:  96% 242/251 [03:55<00:08,  1.03it/s, loss=0.162, v_num=2]\n",
            "Epoch 0:  97% 243/251 [03:55<00:07,  1.03it/s, loss=0.162, v_num=2]\n",
            "Epoch 0:  97% 244/251 [03:55<00:06,  1.03it/s, loss=0.162, v_num=2]\n",
            "Epoch 0:  98% 245/251 [03:56<00:05,  1.04it/s, loss=0.162, v_num=2]\n",
            "Epoch 0:  98% 246/251 [03:56<00:04,  1.04it/s, loss=0.162, v_num=2]\n",
            "Epoch 0:  98% 247/251 [03:56<00:03,  1.04it/s, loss=0.162, v_num=2]\n",
            "Epoch 0:  99% 248/251 [03:57<00:02,  1.04it/s, loss=0.162, v_num=2]\n",
            "Epoch 0:  99% 249/251 [03:57<00:01,  1.05it/s, loss=0.162, v_num=2]\n",
            "Epoch 0: 100% 250/251 [03:58<00:00,  1.05it/s, loss=0.162, v_num=2]\n",
            "Epoch 0: 100% 251/251 [03:58<00:00,  1.05it/s, loss=0.162, v_num=2]val loss = 0.264 val metric = 0.892 \n",
            "\n",
            "Epoch 00000: val_metric reached 0.89160 (best 0.89160), saving model to /content/drive/My Drive/Models/WNUT-Task2/model16/1/epoch=0.ckpt as top 1\n",
            "Epoch 0: 100% 251/251 [04:00<00:00,  1.04it/s, loss=0.162, v_num=2]\n",
            "                                               \u001b[ATrain loss = 0.267 Train metric = 0.889\n",
            "Epoch 1:  87% 219/251 [03:45<00:32,  1.03s/it, loss=0.087, v_num=2]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  88% 220/251 [03:46<00:31,  1.03s/it, loss=0.087, v_num=2]\n",
            "Epoch 1:  88% 221/251 [03:46<00:30,  1.03s/it, loss=0.087, v_num=2]\n",
            "Epoch 1:  88% 222/251 [03:46<00:29,  1.02s/it, loss=0.087, v_num=2]\n",
            "Epoch 1:  89% 223/251 [03:47<00:28,  1.02s/it, loss=0.087, v_num=2]\n",
            "Epoch 1:  89% 224/251 [03:47<00:27,  1.02s/it, loss=0.087, v_num=2]\n",
            "Epoch 1:  90% 225/251 [03:47<00:26,  1.01s/it, loss=0.087, v_num=2]\n",
            "Epoch 1:  90% 226/251 [03:48<00:25,  1.01s/it, loss=0.087, v_num=2]\n",
            "Epoch 1:  90% 227/251 [03:48<00:24,  1.01s/it, loss=0.087, v_num=2]\n",
            "Epoch 1:  91% 228/251 [03:49<00:23,  1.00s/it, loss=0.087, v_num=2]\n",
            "Epoch 1:  91% 229/251 [03:49<00:22,  1.00s/it, loss=0.087, v_num=2]\n",
            "Epoch 1:  92% 230/251 [03:49<00:20,  1.00it/s, loss=0.087, v_num=2]\n",
            "Epoch 1:  92% 231/251 [03:50<00:19,  1.00it/s, loss=0.087, v_num=2]\n",
            "Epoch 1:  92% 232/251 [03:50<00:18,  1.01it/s, loss=0.087, v_num=2]\n",
            "Epoch 1:  93% 233/251 [03:50<00:17,  1.01it/s, loss=0.087, v_num=2]\n",
            "Epoch 1:  93% 234/251 [03:51<00:16,  1.01it/s, loss=0.087, v_num=2]\n",
            "Epoch 1:  94% 235/251 [03:51<00:15,  1.01it/s, loss=0.087, v_num=2]\n",
            "Epoch 1:  94% 236/251 [03:51<00:14,  1.02it/s, loss=0.087, v_num=2]\n",
            "Epoch 1:  94% 237/251 [03:52<00:13,  1.02it/s, loss=0.087, v_num=2]\n",
            "Epoch 1:  95% 238/251 [03:52<00:12,  1.02it/s, loss=0.087, v_num=2]\n",
            "Epoch 1:  95% 239/251 [03:53<00:11,  1.03it/s, loss=0.087, v_num=2]\n",
            "Epoch 1:  96% 240/251 [03:53<00:10,  1.03it/s, loss=0.087, v_num=2]\n",
            "Epoch 1:  96% 241/251 [03:53<00:09,  1.03it/s, loss=0.087, v_num=2]\n",
            "Epoch 1:  96% 242/251 [03:54<00:08,  1.03it/s, loss=0.087, v_num=2]\n",
            "Epoch 1:  97% 243/251 [03:54<00:07,  1.04it/s, loss=0.087, v_num=2]\n",
            "Epoch 1:  97% 244/251 [03:54<00:06,  1.04it/s, loss=0.087, v_num=2]\n",
            "Epoch 1:  98% 245/251 [03:55<00:05,  1.04it/s, loss=0.087, v_num=2]\n",
            "Epoch 1:  98% 246/251 [03:55<00:04,  1.04it/s, loss=0.087, v_num=2]\n",
            "Epoch 1:  98% 247/251 [03:55<00:03,  1.05it/s, loss=0.087, v_num=2]\n",
            "Epoch 1:  99% 248/251 [03:56<00:02,  1.05it/s, loss=0.087, v_num=2]\n",
            "Epoch 1:  99% 249/251 [03:56<00:01,  1.05it/s, loss=0.087, v_num=2]\n",
            "Epoch 1: 100% 250/251 [03:56<00:00,  1.05it/s, loss=0.087, v_num=2]\n",
            "Epoch 1: 100% 251/251 [03:57<00:00,  1.06it/s, loss=0.087, v_num=2]val loss = 0.274 val metric = 0.899 \n",
            "\n",
            "Epoch 00001: val_metric reached 0.89941 (best 0.89941), saving model to /content/drive/My Drive/Models/WNUT-Task2/model16/1/epoch=1.ckpt as top 1\n",
            "Epoch 1: 100% 251/251 [03:59<00:00,  1.05it/s, loss=0.087, v_num=2]\n",
            "                                               \u001b[ATrain loss = 0.076 Train metric = 0.973\n",
            "Saving latest checkpoint..\n",
            "Epoch 1: 100% 251/251 [03:59<00:00,  1.05it/s, loss=0.087, v_num=2]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0, mixout prob 0.5, ...\n",
            "f1                                                      0.875978\n",
            "precision                                               0.926714\n",
            "recall                                                  0.830508\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWfLe7LsqNoq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6cc15dd5-49d4-4188-d907-0b56ad9b2c43"
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model16/1/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --model_save_path {model_dir} --mixout_prob .5 --lightning true\n",
        "\n",
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model16/2/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --model_save_path {model_dir} --mixout_prob .6 --lightning true\n",
        "\n",
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model16/3/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --model_save_path {model_dir} --mixout_prob .7 --lightning true\n",
        "\n",
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model16/4/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --model_save_path {model_dir} --mixout_prob .8 --lightning true"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-02 09:09:29.542937: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "100% 7000/7000 [00:04<00:00, 1678.59it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1971.71it/s]\n",
            "Train and val loader length 219 and 32\n",
            "Modelling\n",
            "Device: cuda\n",
            "TransformerWithMixout(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.5, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (drop): Dropout(p=0, inplace=False)\n",
            "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory /content/drive/My Drive/Models/WNUT-Task2/model16/1/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "[LOG] Total number of parameters to learn 124646401\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name   | Type                  | Params\n",
            "-------------------------------------------------\n",
            "0 | model  | TransformerWithMixout | 124 M \n",
            "1 | metric | PLAccuracy            | 0     \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  2.30it/s]val loss = 0.703 val metric = 0.438 \n",
            "Epoch 0:   0% 0/251 [00:00<?, ?it/s] /usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Epoch 0:  87% 219/251 [03:43<00:32,  1.02s/it, loss=0.162, v_num=3]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  88% 220/251 [03:43<00:31,  1.02s/it, loss=0.162, v_num=3]\n",
            "Epoch 0:  88% 221/251 [03:44<00:30,  1.01s/it, loss=0.162, v_num=3]\n",
            "Epoch 0:  88% 222/251 [03:44<00:29,  1.01s/it, loss=0.162, v_num=3]\n",
            "Epoch 0:  89% 223/251 [03:44<00:28,  1.01s/it, loss=0.162, v_num=3]\n",
            "Epoch 0:  89% 224/251 [03:45<00:27,  1.01s/it, loss=0.162, v_num=3]\n",
            "Epoch 0:  90% 225/251 [03:45<00:26,  1.00s/it, loss=0.162, v_num=3]\n",
            "Epoch 0:  90% 226/251 [03:45<00:24,  1.00it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  90% 227/251 [03:46<00:23,  1.00it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  91% 228/251 [03:46<00:22,  1.01it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  91% 229/251 [03:47<00:21,  1.01it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  92% 230/251 [03:47<00:20,  1.01it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  92% 231/251 [03:47<00:19,  1.01it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  92% 232/251 [03:48<00:18,  1.02it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  93% 233/251 [03:48<00:17,  1.02it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  93% 234/251 [03:48<00:16,  1.02it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  94% 235/251 [03:49<00:15,  1.03it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  94% 236/251 [03:49<00:14,  1.03it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  94% 237/251 [03:49<00:13,  1.03it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  95% 238/251 [03:50<00:12,  1.03it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  95% 239/251 [03:50<00:11,  1.04it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  96% 240/251 [03:51<00:10,  1.04it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  96% 241/251 [03:51<00:09,  1.04it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  96% 242/251 [03:51<00:08,  1.04it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  97% 243/251 [03:52<00:07,  1.05it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  97% 244/251 [03:52<00:06,  1.05it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  98% 245/251 [03:52<00:05,  1.05it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  98% 246/251 [03:53<00:04,  1.05it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  98% 247/251 [03:53<00:03,  1.06it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  99% 248/251 [03:53<00:02,  1.06it/s, loss=0.162, v_num=3]\n",
            "Epoch 0:  99% 249/251 [03:54<00:01,  1.06it/s, loss=0.162, v_num=3]\n",
            "Epoch 0: 100% 250/251 [03:54<00:00,  1.07it/s, loss=0.162, v_num=3]\n",
            "Epoch 0: 100% 251/251 [03:54<00:00,  1.07it/s, loss=0.162, v_num=3]val loss = 0.264 val metric = 0.892 \n",
            "\n",
            "Epoch 00000: val_metric reached 0.89160 (best 0.89160), saving model to /content/drive/My Drive/Models/WNUT-Task2/model16/1/epoch=0.ckpt as top 1\n",
            "Epoch 0: 100% 251/251 [03:57<00:00,  1.06it/s, loss=0.162, v_num=3]\n",
            "                                               \u001b[ATrain loss = 0.267 Train metric = 0.889\n",
            "Epoch 1:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.087, v_num=3]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  88% 220/251 [03:44<00:31,  1.02s/it, loss=0.087, v_num=3]\n",
            "Epoch 1:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.087, v_num=3]\n",
            "Epoch 1:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.087, v_num=3]\n",
            "Epoch 1:  89% 223/251 [03:46<00:28,  1.01s/it, loss=0.087, v_num=3]\n",
            "Epoch 1:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.087, v_num=3]\n",
            "Epoch 1:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.087, v_num=3]\n",
            "Epoch 1:  90% 226/251 [03:47<00:25,  1.01s/it, loss=0.087, v_num=3]\n",
            "Epoch 1:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.087, v_num=3]\n",
            "Epoch 1:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  92% 231/251 [03:48<00:19,  1.01it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  93% 233/251 [03:49<00:17,  1.01it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  93% 234/251 [03:50<00:16,  1.02it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  94% 237/251 [03:51<00:13,  1.03it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  97% 244/251 [03:53<00:06,  1.04it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  98% 245/251 [03:53<00:05,  1.05it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  99% 248/251 [03:55<00:02,  1.06it/s, loss=0.087, v_num=3]\n",
            "Epoch 1:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.087, v_num=3]\n",
            "Epoch 1: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.087, v_num=3]\n",
            "Epoch 1: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.087, v_num=3]val loss = 0.283 val metric = 0.902 \n",
            "\n",
            "Epoch 00001: val_metric reached 0.90234 (best 0.90234), saving model to /content/drive/My Drive/Models/WNUT-Task2/model16/1/epoch=1.ckpt as top 1\n",
            "Epoch 1: 100% 251/251 [03:58<00:00,  1.05it/s, loss=0.087, v_num=3]\n",
            "                                               \u001b[ATrain loss = 0.076 Train metric = 0.973\n",
            "Epoch 2:  87% 219/251 [03:43<00:32,  1.02s/it, loss=0.016, v_num=3]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 2:  88% 220/251 [03:44<00:31,  1.02s/it, loss=0.016, v_num=3]\n",
            "Epoch 2:  88% 221/251 [03:44<00:30,  1.02s/it, loss=0.016, v_num=3]\n",
            "Epoch 2:  88% 222/251 [03:45<00:29,  1.01s/it, loss=0.016, v_num=3]\n",
            "Epoch 2:  89% 223/251 [03:45<00:28,  1.01s/it, loss=0.016, v_num=3]\n",
            "Epoch 2:  89% 224/251 [03:45<00:27,  1.01s/it, loss=0.016, v_num=3]\n",
            "Epoch 2:  90% 225/251 [03:46<00:26,  1.00s/it, loss=0.016, v_num=3]\n",
            "Epoch 2:  90% 226/251 [03:46<00:25,  1.00s/it, loss=0.016, v_num=3]\n",
            "Epoch 2:  90% 227/251 [03:46<00:23,  1.00it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  91% 229/251 [03:47<00:21,  1.01it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  92% 230/251 [03:47<00:20,  1.01it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  92% 231/251 [03:48<00:19,  1.01it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  92% 232/251 [03:48<00:18,  1.01it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  93% 233/251 [03:48<00:17,  1.02it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  93% 234/251 [03:49<00:16,  1.02it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  94% 235/251 [03:49<00:15,  1.02it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  94% 236/251 [03:50<00:14,  1.03it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  94% 237/251 [03:50<00:13,  1.03it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  95% 238/251 [03:50<00:12,  1.03it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  96% 240/251 [03:51<00:10,  1.04it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  96% 241/251 [03:51<00:09,  1.04it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  97% 243/251 [03:52<00:07,  1.05it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  97% 244/251 [03:52<00:06,  1.05it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  98% 245/251 [03:53<00:05,  1.05it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  98% 246/251 [03:53<00:04,  1.05it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  98% 247/251 [03:53<00:03,  1.06it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  99% 248/251 [03:54<00:02,  1.06it/s, loss=0.016, v_num=3]\n",
            "Epoch 2:  99% 249/251 [03:54<00:01,  1.06it/s, loss=0.016, v_num=3]\n",
            "Epoch 2: 100% 250/251 [03:54<00:00,  1.06it/s, loss=0.016, v_num=3]\n",
            "Epoch 2: 100% 251/251 [03:55<00:00,  1.07it/s, loss=0.016, v_num=3]val loss = 0.421 val metric = 0.896 \n",
            "\n",
            "Epoch 00002: val_metric  was not in top 1\n",
            "Epoch 2: 100% 251/251 [03:55<00:00,  1.07it/s, loss=0.016, v_num=3]\n",
            "                                               \u001b[ATrain loss = 0.031 Train metric = 0.99\n",
            "Epoch 3:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.012, v_num=3]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  88% 220/251 [03:45<00:31,  1.02s/it, loss=0.012, v_num=3]\n",
            "Epoch 3:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.012, v_num=3]\n",
            "Epoch 3:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.012, v_num=3]\n",
            "Epoch 3:  89% 223/251 [03:46<00:28,  1.01s/it, loss=0.012, v_num=3]\n",
            "Epoch 3:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.012, v_num=3]\n",
            "Epoch 3:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.012, v_num=3]\n",
            "Epoch 3:  90% 226/251 [03:47<00:25,  1.01s/it, loss=0.012, v_num=3]\n",
            "Epoch 3:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.012, v_num=3]\n",
            "Epoch 3:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  92% 231/251 [03:48<00:19,  1.01it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  93% 233/251 [03:49<00:17,  1.01it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  93% 234/251 [03:50<00:16,  1.02it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  94% 237/251 [03:51<00:13,  1.03it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  97% 244/251 [03:53<00:06,  1.04it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  98% 245/251 [03:54<00:05,  1.05it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  99% 248/251 [03:55<00:02,  1.05it/s, loss=0.012, v_num=3]\n",
            "Epoch 3:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.012, v_num=3]\n",
            "Epoch 3: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.012, v_num=3]\n",
            "Epoch 3: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.012, v_num=3]val loss = 0.478 val metric = 0.906 \n",
            "\n",
            "Epoch 00003: val_metric reached 0.90625 (best 0.90625), saving model to /content/drive/My Drive/Models/WNUT-Task2/model16/1/epoch=3.ckpt as top 1\n",
            "Epoch 3: 100% 251/251 [03:58<00:00,  1.05it/s, loss=0.012, v_num=3]\n",
            "                                               \u001b[ATrain loss = 0.016 Train metric = 0.995\n",
            "Epoch 4:  87% 219/251 [03:44<00:32,  1.02s/it, loss=0.032, v_num=3]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 4:  88% 220/251 [03:44<00:31,  1.02s/it, loss=0.032, v_num=3]\n",
            "Epoch 4:  88% 221/251 [03:44<00:30,  1.02s/it, loss=0.032, v_num=3]\n",
            "Epoch 4:  88% 222/251 [03:45<00:29,  1.01s/it, loss=0.032, v_num=3]\n",
            "Epoch 4:  89% 223/251 [03:45<00:28,  1.01s/it, loss=0.032, v_num=3]\n",
            "Epoch 4:  89% 224/251 [03:45<00:27,  1.01s/it, loss=0.032, v_num=3]\n",
            "Epoch 4:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.032, v_num=3]\n",
            "Epoch 4:  90% 226/251 [03:46<00:25,  1.00s/it, loss=0.032, v_num=3]\n",
            "Epoch 4:  90% 227/251 [03:46<00:23,  1.00it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  91% 229/251 [03:47<00:21,  1.01it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  92% 231/251 [03:48<00:19,  1.01it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  92% 232/251 [03:48<00:18,  1.01it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  93% 233/251 [03:49<00:17,  1.02it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  93% 234/251 [03:49<00:16,  1.02it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  94% 235/251 [03:49<00:15,  1.02it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  94% 236/251 [03:50<00:14,  1.03it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  94% 237/251 [03:50<00:13,  1.03it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  95% 238/251 [03:50<00:12,  1.03it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  96% 240/251 [03:51<00:10,  1.04it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  96% 241/251 [03:51<00:09,  1.04it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  97% 243/251 [03:52<00:07,  1.04it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  97% 244/251 [03:52<00:06,  1.05it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  98% 245/251 [03:53<00:05,  1.05it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  98% 246/251 [03:53<00:04,  1.05it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  98% 247/251 [03:54<00:03,  1.06it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  99% 248/251 [03:54<00:02,  1.06it/s, loss=0.032, v_num=3]\n",
            "Epoch 4:  99% 249/251 [03:54<00:01,  1.06it/s, loss=0.032, v_num=3]\n",
            "Epoch 4: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.032, v_num=3]\n",
            "Epoch 4: 100% 251/251 [03:55<00:00,  1.07it/s, loss=0.032, v_num=3]val loss = 0.386 val metric = 0.906 \n",
            "\n",
            "Epoch 00004: val_metric  was not in top 1\n",
            "Epoch 4: 100% 251/251 [03:55<00:00,  1.07it/s, loss=0.032, v_num=3]\n",
            "                                               \u001b[ATrain loss = 0.015 Train metric = 0.996\n",
            "Epoch 5:  87% 219/251 [03:44<00:32,  1.02s/it, loss=0.008, v_num=3]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  88% 220/251 [03:44<00:31,  1.02s/it, loss=0.008, v_num=3]\n",
            "Epoch 5:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.008, v_num=3]\n",
            "Epoch 5:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.008, v_num=3]\n",
            "Epoch 5:  89% 223/251 [03:45<00:28,  1.01s/it, loss=0.008, v_num=3]\n",
            "Epoch 5:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.008, v_num=3]\n",
            "Epoch 5:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.008, v_num=3]\n",
            "Epoch 5:  90% 226/251 [03:46<00:25,  1.00s/it, loss=0.008, v_num=3]\n",
            "Epoch 5:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.008, v_num=3]\n",
            "Epoch 5:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  92% 231/251 [03:48<00:19,  1.01it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  93% 233/251 [03:49<00:17,  1.02it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  93% 234/251 [03:49<00:16,  1.02it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  94% 237/251 [03:50<00:13,  1.03it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  96% 240/251 [03:51<00:10,  1.03it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  97% 244/251 [03:53<00:06,  1.05it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  98% 245/251 [03:53<00:05,  1.05it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  99% 248/251 [03:54<00:02,  1.06it/s, loss=0.008, v_num=3]\n",
            "Epoch 5:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.008, v_num=3]\n",
            "Epoch 5: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.008, v_num=3]\n",
            "Epoch 5: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.008, v_num=3]val loss = 0.487 val metric = 0.906 \n",
            "\n",
            "Epoch 00005: val_metric  was not in top 1\n",
            "Epoch 5: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.008, v_num=3]\n",
            "                                               \u001b[ATrain loss = 0.012 Train metric = 0.996\n",
            "Epoch 6:  87% 219/251 [03:45<00:32,  1.03s/it, loss=0.032, v_num=3]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  88% 220/251 [03:46<00:31,  1.03s/it, loss=0.032, v_num=3]\n",
            "Epoch 6:  88% 221/251 [03:46<00:30,  1.03s/it, loss=0.032, v_num=3]\n",
            "Epoch 6:  88% 222/251 [03:46<00:29,  1.02s/it, loss=0.032, v_num=3]\n",
            "Epoch 6:  89% 223/251 [03:47<00:28,  1.02s/it, loss=0.032, v_num=3]\n",
            "Epoch 6:  89% 224/251 [03:47<00:27,  1.02s/it, loss=0.032, v_num=3]\n",
            "Epoch 6:  90% 225/251 [03:47<00:26,  1.01s/it, loss=0.032, v_num=3]\n",
            "Epoch 6:  90% 226/251 [03:48<00:25,  1.01s/it, loss=0.032, v_num=3]\n",
            "Epoch 6:  90% 227/251 [03:48<00:24,  1.01s/it, loss=0.032, v_num=3]\n",
            "Epoch 6:  91% 228/251 [03:49<00:23,  1.00s/it, loss=0.032, v_num=3]\n",
            "Epoch 6:  91% 229/251 [03:49<00:22,  1.00s/it, loss=0.032, v_num=3]\n",
            "Epoch 6:  92% 230/251 [03:49<00:20,  1.00it/s, loss=0.032, v_num=3]\n",
            "Epoch 6:  92% 231/251 [03:50<00:19,  1.00it/s, loss=0.032, v_num=3]\n",
            "Epoch 6:  92% 232/251 [03:50<00:18,  1.01it/s, loss=0.032, v_num=3]\n",
            "Epoch 6:  93% 233/251 [03:50<00:17,  1.01it/s, loss=0.032, v_num=3]\n",
            "Epoch 6:  93% 234/251 [03:51<00:16,  1.01it/s, loss=0.032, v_num=3]\n",
            "Epoch 6:  94% 235/251 [03:51<00:15,  1.02it/s, loss=0.032, v_num=3]\n",
            "Epoch 6:  94% 236/251 [03:51<00:14,  1.02it/s, loss=0.032, v_num=3]\n",
            "Epoch 6:  94% 237/251 [03:52<00:13,  1.02it/s, loss=0.032, v_num=3]\n",
            "Epoch 6:  95% 238/251 [03:52<00:12,  1.02it/s, loss=0.032, v_num=3]\n",
            "Epoch 6:  95% 239/251 [03:52<00:11,  1.03it/s, loss=0.032, v_num=3]\n",
            "Epoch 6:  96% 240/251 [03:53<00:10,  1.03it/s, loss=0.032, v_num=3]\n",
            "Epoch 6:  96% 241/251 [03:53<00:09,  1.03it/s, loss=0.032, v_num=3]\n",
            "Epoch 6:  96% 242/251 [03:53<00:08,  1.03it/s, loss=0.032, v_num=3]\n",
            "Epoch 6:  97% 243/251 [03:54<00:07,  1.04it/s, loss=0.032, v_num=3]\n",
            "Epoch 6:  97% 244/251 [03:54<00:06,  1.04it/s, loss=0.032, v_num=3]\n",
            "Epoch 6:  98% 245/251 [03:54<00:05,  1.04it/s, loss=0.032, v_num=3]\n",
            "Epoch 6:  98% 246/251 [03:55<00:04,  1.05it/s, loss=0.032, v_num=3]\n",
            "Epoch 6:  98% 247/251 [03:55<00:03,  1.05it/s, loss=0.032, v_num=3]\n",
            "Epoch 6:  99% 248/251 [03:56<00:02,  1.05it/s, loss=0.032, v_num=3]\n",
            "Epoch 6:  99% 249/251 [03:56<00:01,  1.05it/s, loss=0.032, v_num=3]\n",
            "Epoch 6: 100% 250/251 [03:56<00:00,  1.06it/s, loss=0.032, v_num=3]\n",
            "Epoch 6: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.032, v_num=3]val loss = 0.559 val metric = 0.888 \n",
            "\n",
            "Epoch 00006: val_metric  was not in top 1\n",
            "Epoch 6: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.032, v_num=3]\n",
            "                                               \u001b[ATrain loss = 0.012 Train metric = 0.996\n",
            "Epoch 7:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.001, v_num=3]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  88% 220/251 [03:44<00:31,  1.02s/it, loss=0.001, v_num=3]\n",
            "Epoch 7:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.001, v_num=3]\n",
            "Epoch 7:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.001, v_num=3]\n",
            "Epoch 7:  89% 223/251 [03:45<00:28,  1.01s/it, loss=0.001, v_num=3]\n",
            "Epoch 7:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.001, v_num=3]\n",
            "Epoch 7:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.001, v_num=3]\n",
            "Epoch 7:  90% 226/251 [03:47<00:25,  1.00s/it, loss=0.001, v_num=3]\n",
            "Epoch 7:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.001, v_num=3]\n",
            "Epoch 7:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  92% 231/251 [03:48<00:19,  1.01it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  93% 233/251 [03:49<00:17,  1.01it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  93% 234/251 [03:49<00:16,  1.02it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  94% 237/251 [03:51<00:13,  1.03it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  97% 244/251 [03:53<00:06,  1.04it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  98% 245/251 [03:53<00:05,  1.05it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  99% 248/251 [03:54<00:02,  1.06it/s, loss=0.001, v_num=3]\n",
            "Epoch 7:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.001, v_num=3]\n",
            "Epoch 7: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.001, v_num=3]\n",
            "Epoch 7: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.001, v_num=3]val loss = 0.537 val metric = 0.902 \n",
            "\n",
            "Epoch 00007: val_metric  was not in top 1\n",
            "Epoch 7: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.001, v_num=3]\n",
            "                                               \u001b[ATrain loss = 0.008 Train metric = 0.998\n",
            "Epoch 8:  87% 219/251 [03:44<00:32,  1.02s/it, loss=0.000, v_num=3]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 8:  88% 220/251 [03:44<00:31,  1.02s/it, loss=0.000, v_num=3]\n",
            "Epoch 8:  88% 221/251 [03:44<00:30,  1.02s/it, loss=0.000, v_num=3]\n",
            "Epoch 8:  88% 222/251 [03:45<00:29,  1.01s/it, loss=0.000, v_num=3]\n",
            "Epoch 8:  89% 223/251 [03:45<00:28,  1.01s/it, loss=0.000, v_num=3]\n",
            "Epoch 8:  89% 224/251 [03:45<00:27,  1.01s/it, loss=0.000, v_num=3]\n",
            "Epoch 8:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.000, v_num=3]\n",
            "Epoch 8:  90% 226/251 [03:46<00:25,  1.00s/it, loss=0.000, v_num=3]\n",
            "Epoch 8:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.000, v_num=3]\n",
            "Epoch 8:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  91% 229/251 [03:47<00:21,  1.01it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  92% 231/251 [03:48<00:19,  1.01it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  92% 232/251 [03:48<00:18,  1.01it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  93% 233/251 [03:49<00:17,  1.02it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  93% 234/251 [03:49<00:16,  1.02it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  94% 235/251 [03:49<00:15,  1.02it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  94% 236/251 [03:50<00:14,  1.03it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  94% 237/251 [03:50<00:13,  1.03it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  95% 238/251 [03:50<00:12,  1.03it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  96% 240/251 [03:51<00:10,  1.04it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  96% 241/251 [03:51<00:09,  1.04it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  97% 243/251 [03:52<00:07,  1.04it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  97% 244/251 [03:53<00:06,  1.05it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  98% 245/251 [03:53<00:05,  1.05it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  98% 246/251 [03:53<00:04,  1.05it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  98% 247/251 [03:54<00:03,  1.06it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  99% 248/251 [03:54<00:02,  1.06it/s, loss=0.000, v_num=3]\n",
            "Epoch 8:  99% 249/251 [03:54<00:01,  1.06it/s, loss=0.000, v_num=3]\n",
            "Epoch 8: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.000, v_num=3]\n",
            "Epoch 8: 100% 251/251 [03:55<00:00,  1.07it/s, loss=0.000, v_num=3]val loss = 0.642 val metric = 0.904 \n",
            "\n",
            "Epoch 00008: val_metric  was not in top 1\n",
            "Epoch 8: 100% 251/251 [03:55<00:00,  1.07it/s, loss=0.000, v_num=3]\n",
            "                                               \u001b[ATrain loss = 0.001 Train metric = 1.0\n",
            "Saving latest checkpoint..\n",
            "Epoch 8: 100% 251/251 [03:55<00:00,  1.07it/s, loss=0.000, v_num=3]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0, mixout prob 0.5, ...\n",
            "f1                                                      0.889804\n",
            "precision                                               0.865731\n",
            "recall                                                  0.915254\n",
            "Name: 0, dtype: object\n",
            "2020-09-02 09:45:24.667230: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "100% 7000/7000 [00:04<00:00, 1645.33it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1943.63it/s]\n",
            "Train and val loader length 219 and 32\n",
            "Modelling\n",
            "Device: cuda\n",
            "TransformerWithMixout(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.6, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (drop): Dropout(p=0, inplace=False)\n",
            "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory /content/drive/My Drive/Models/WNUT-Task2/model16/2/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "[LOG] Total number of parameters to learn 124646401\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name   | Type                  | Params\n",
            "-------------------------------------------------\n",
            "0 | model  | TransformerWithMixout | 124 M \n",
            "1 | metric | PLAccuracy            | 0     \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  2.31it/s]val loss = 0.703 val metric = 0.438 \n",
            "Epoch 0:   0% 0/251 [00:00<?, ?it/s] /usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Epoch 0:  87% 219/251 [03:44<00:32,  1.02s/it, loss=0.130, v_num=4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  88% 220/251 [03:44<00:31,  1.02s/it, loss=0.130, v_num=4]\n",
            "Epoch 0:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.130, v_num=4]\n",
            "Epoch 0:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.130, v_num=4]\n",
            "Epoch 0:  89% 223/251 [03:45<00:28,  1.01s/it, loss=0.130, v_num=4]\n",
            "Epoch 0:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.130, v_num=4]\n",
            "Epoch 0:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.130, v_num=4]\n",
            "Epoch 0:  90% 226/251 [03:46<00:25,  1.00s/it, loss=0.130, v_num=4]\n",
            "Epoch 0:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.130, v_num=4]\n",
            "Epoch 0:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  91% 229/251 [03:47<00:21,  1.00it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  92% 231/251 [03:48<00:19,  1.01it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  93% 233/251 [03:49<00:17,  1.02it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  93% 234/251 [03:49<00:16,  1.02it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  94% 237/251 [03:50<00:13,  1.03it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  96% 240/251 [03:51<00:10,  1.03it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  97% 244/251 [03:53<00:06,  1.05it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  98% 245/251 [03:53<00:05,  1.05it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  99% 248/251 [03:54<00:02,  1.06it/s, loss=0.130, v_num=4]\n",
            "Epoch 0:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.130, v_num=4]\n",
            "Epoch 0: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.130, v_num=4]\n",
            "Epoch 0: 100% 251/251 [03:55<00:00,  1.07it/s, loss=0.130, v_num=4]val loss = 0.276 val metric = 0.896 \n",
            "\n",
            "Epoch 00000: val_metric reached 0.89648 (best 0.89648), saving model to /content/drive/My Drive/Models/WNUT-Task2/model16/2/epoch=0.ckpt as top 1\n",
            "Epoch 0: 100% 251/251 [03:58<00:00,  1.05it/s, loss=0.130, v_num=4]\n",
            "                                               \u001b[ATrain loss = 0.272 Train metric = 0.885\n",
            "Epoch 1:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.034, v_num=4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  88% 220/251 [03:45<00:31,  1.02s/it, loss=0.034, v_num=4]\n",
            "Epoch 1:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.034, v_num=4]\n",
            "Epoch 1:  88% 222/251 [03:46<00:29,  1.02s/it, loss=0.034, v_num=4]\n",
            "Epoch 1:  89% 223/251 [03:46<00:28,  1.02s/it, loss=0.034, v_num=4]\n",
            "Epoch 1:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.034, v_num=4]\n",
            "Epoch 1:  90% 225/251 [03:47<00:26,  1.01s/it, loss=0.034, v_num=4]\n",
            "Epoch 1:  90% 226/251 [03:47<00:25,  1.01s/it, loss=0.034, v_num=4]\n",
            "Epoch 1:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.034, v_num=4]\n",
            "Epoch 1:  91% 228/251 [03:48<00:23,  1.00s/it, loss=0.034, v_num=4]\n",
            "Epoch 1:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.034, v_num=4]\n",
            "Epoch 1:  92% 230/251 [03:48<00:20,  1.00it/s, loss=0.034, v_num=4]\n",
            "Epoch 1:  92% 231/251 [03:49<00:19,  1.01it/s, loss=0.034, v_num=4]\n",
            "Epoch 1:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.034, v_num=4]\n",
            "Epoch 1:  93% 233/251 [03:49<00:17,  1.01it/s, loss=0.034, v_num=4]\n",
            "Epoch 1:  93% 234/251 [03:50<00:16,  1.02it/s, loss=0.034, v_num=4]\n",
            "Epoch 1:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.034, v_num=4]\n",
            "Epoch 1:  94% 236/251 [03:51<00:14,  1.02it/s, loss=0.034, v_num=4]\n",
            "Epoch 1:  94% 237/251 [03:51<00:13,  1.02it/s, loss=0.034, v_num=4]\n",
            "Epoch 1:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.034, v_num=4]\n",
            "Epoch 1:  95% 239/251 [03:52<00:11,  1.03it/s, loss=0.034, v_num=4]\n",
            "Epoch 1:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.034, v_num=4]\n",
            "Epoch 1:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.034, v_num=4]\n",
            "Epoch 1:  96% 242/251 [03:53<00:08,  1.04it/s, loss=0.034, v_num=4]\n",
            "Epoch 1:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.034, v_num=4]\n",
            "Epoch 1:  97% 244/251 [03:53<00:06,  1.04it/s, loss=0.034, v_num=4]\n",
            "Epoch 1:  98% 245/251 [03:54<00:05,  1.05it/s, loss=0.034, v_num=4]\n",
            "Epoch 1:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.034, v_num=4]\n",
            "Epoch 1:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.034, v_num=4]\n",
            "Epoch 1:  99% 248/251 [03:55<00:02,  1.05it/s, loss=0.034, v_num=4]\n",
            "Epoch 1:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.034, v_num=4]\n",
            "Epoch 1: 100% 250/251 [03:56<00:00,  1.06it/s, loss=0.034, v_num=4]\n",
            "Epoch 1: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.034, v_num=4]val loss = 0.331 val metric = 0.9 \n",
            "\n",
            "Epoch 00001: val_metric reached 0.90039 (best 0.90039), saving model to /content/drive/My Drive/Models/WNUT-Task2/model16/2/epoch=1.ckpt as top 1\n",
            "Epoch 1: 100% 251/251 [03:58<00:00,  1.05it/s, loss=0.034, v_num=4]\n",
            "                                               \u001b[ATrain loss = 0.076 Train metric = 0.973\n",
            "Epoch 2:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.014, v_num=4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 2:  88% 220/251 [03:44<00:31,  1.02s/it, loss=0.014, v_num=4]\n",
            "Epoch 2:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.014, v_num=4]\n",
            "Epoch 2:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.014, v_num=4]\n",
            "Epoch 2:  89% 223/251 [03:45<00:28,  1.01s/it, loss=0.014, v_num=4]\n",
            "Epoch 2:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.014, v_num=4]\n",
            "Epoch 2:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.014, v_num=4]\n",
            "Epoch 2:  90% 226/251 [03:46<00:25,  1.00s/it, loss=0.014, v_num=4]\n",
            "Epoch 2:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.014, v_num=4]\n",
            "Epoch 2:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  91% 229/251 [03:47<00:21,  1.00it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  92% 231/251 [03:48<00:19,  1.01it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  93% 233/251 [03:49<00:17,  1.02it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  93% 234/251 [03:49<00:16,  1.02it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  94% 237/251 [03:50<00:13,  1.03it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  96% 240/251 [03:51<00:10,  1.04it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  97% 243/251 [03:52<00:07,  1.04it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  97% 244/251 [03:53<00:06,  1.05it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  98% 245/251 [03:53<00:05,  1.05it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  98% 246/251 [03:53<00:04,  1.05it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  99% 248/251 [03:54<00:02,  1.06it/s, loss=0.014, v_num=4]\n",
            "Epoch 2:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.014, v_num=4]\n",
            "Epoch 2: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.014, v_num=4]\n",
            "Epoch 2: 100% 251/251 [03:55<00:00,  1.07it/s, loss=0.014, v_num=4]val loss = 0.383 val metric = 0.908 \n",
            "\n",
            "Epoch 00002: val_metric reached 0.90820 (best 0.90820), saving model to /content/drive/My Drive/Models/WNUT-Task2/model16/2/epoch=2.ckpt as top 1\n",
            "Epoch 2: 100% 251/251 [03:57<00:00,  1.06it/s, loss=0.014, v_num=4]\n",
            "                                               \u001b[ATrain loss = 0.033 Train metric = 0.991\n",
            "Epoch 3:  87% 219/251 [03:45<00:32,  1.03s/it, loss=0.007, v_num=4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  88% 220/251 [03:45<00:31,  1.03s/it, loss=0.007, v_num=4]\n",
            "Epoch 3:  88% 221/251 [03:46<00:30,  1.02s/it, loss=0.007, v_num=4]\n",
            "Epoch 3:  88% 222/251 [03:46<00:29,  1.02s/it, loss=0.007, v_num=4]\n",
            "Epoch 3:  89% 223/251 [03:46<00:28,  1.02s/it, loss=0.007, v_num=4]\n",
            "Epoch 3:  89% 224/251 [03:47<00:27,  1.01s/it, loss=0.007, v_num=4]\n",
            "Epoch 3:  90% 225/251 [03:47<00:26,  1.01s/it, loss=0.007, v_num=4]\n",
            "Epoch 3:  90% 226/251 [03:47<00:25,  1.01s/it, loss=0.007, v_num=4]\n",
            "Epoch 3:  90% 227/251 [03:48<00:24,  1.01s/it, loss=0.007, v_num=4]\n",
            "Epoch 3:  91% 228/251 [03:48<00:23,  1.00s/it, loss=0.007, v_num=4]\n",
            "Epoch 3:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.007, v_num=4]\n",
            "Epoch 3:  92% 230/251 [03:49<00:20,  1.00it/s, loss=0.007, v_num=4]\n",
            "Epoch 3:  92% 231/251 [03:49<00:19,  1.01it/s, loss=0.007, v_num=4]\n",
            "Epoch 3:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.007, v_num=4]\n",
            "Epoch 3:  93% 233/251 [03:50<00:17,  1.01it/s, loss=0.007, v_num=4]\n",
            "Epoch 3:  93% 234/251 [03:50<00:16,  1.01it/s, loss=0.007, v_num=4]\n",
            "Epoch 3:  94% 235/251 [03:51<00:15,  1.02it/s, loss=0.007, v_num=4]\n",
            "Epoch 3:  94% 236/251 [03:51<00:14,  1.02it/s, loss=0.007, v_num=4]\n",
            "Epoch 3:  94% 237/251 [03:51<00:13,  1.02it/s, loss=0.007, v_num=4]\n",
            "Epoch 3:  95% 238/251 [03:52<00:12,  1.03it/s, loss=0.007, v_num=4]\n",
            "Epoch 3:  95% 239/251 [03:52<00:11,  1.03it/s, loss=0.007, v_num=4]\n",
            "Epoch 3:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.007, v_num=4]\n",
            "Epoch 3:  96% 241/251 [03:53<00:09,  1.03it/s, loss=0.007, v_num=4]\n",
            "Epoch 3:  96% 242/251 [03:53<00:08,  1.04it/s, loss=0.007, v_num=4]\n",
            "Epoch 3:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.007, v_num=4]\n",
            "Epoch 3:  97% 244/251 [03:54<00:06,  1.04it/s, loss=0.007, v_num=4]\n",
            "Epoch 3:  98% 245/251 [03:54<00:05,  1.04it/s, loss=0.007, v_num=4]\n",
            "Epoch 3:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.007, v_num=4]\n",
            "Epoch 3:  98% 247/251 [03:55<00:03,  1.05it/s, loss=0.007, v_num=4]\n",
            "Epoch 3:  99% 248/251 [03:55<00:02,  1.05it/s, loss=0.007, v_num=4]\n",
            "Epoch 3:  99% 249/251 [03:56<00:01,  1.05it/s, loss=0.007, v_num=4]\n",
            "Epoch 3: 100% 250/251 [03:56<00:00,  1.06it/s, loss=0.007, v_num=4]\n",
            "Epoch 3: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.007, v_num=4]val loss = 0.45 val metric = 0.905 \n",
            "\n",
            "Epoch 00003: val_metric  was not in top 1\n",
            "Epoch 3: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.007, v_num=4]\n",
            "                                               \u001b[ATrain loss = 0.022 Train metric = 0.993\n",
            "Epoch 4:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.003, v_num=4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 4:  88% 220/251 [03:44<00:31,  1.02s/it, loss=0.003, v_num=4]\n",
            "Epoch 4:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.003, v_num=4]\n",
            "Epoch 4:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.003, v_num=4]\n",
            "Epoch 4:  89% 223/251 [03:46<00:28,  1.01s/it, loss=0.003, v_num=4]\n",
            "Epoch 4:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.003, v_num=4]\n",
            "Epoch 4:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.003, v_num=4]\n",
            "Epoch 4:  90% 226/251 [03:47<00:25,  1.00s/it, loss=0.003, v_num=4]\n",
            "Epoch 4:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.003, v_num=4]\n",
            "Epoch 4:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  92% 231/251 [03:48<00:19,  1.01it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  93% 233/251 [03:49<00:17,  1.02it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  93% 234/251 [03:49<00:16,  1.02it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  94% 237/251 [03:50<00:13,  1.03it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  96% 240/251 [03:51<00:10,  1.03it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  97% 244/251 [03:53<00:06,  1.05it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  98% 245/251 [03:53<00:05,  1.05it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  99% 248/251 [03:54<00:02,  1.06it/s, loss=0.003, v_num=4]\n",
            "Epoch 4:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.003, v_num=4]\n",
            "Epoch 4: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.003, v_num=4]\n",
            "Epoch 4: 100% 251/251 [03:55<00:00,  1.07it/s, loss=0.003, v_num=4]val loss = 0.454 val metric = 0.911 \n",
            "\n",
            "Epoch 00004: val_metric reached 0.91113 (best 0.91113), saving model to /content/drive/My Drive/Models/WNUT-Task2/model16/2/epoch=4.ckpt as top 1\n",
            "Epoch 4: 100% 251/251 [03:57<00:00,  1.06it/s, loss=0.003, v_num=4]\n",
            "                                               \u001b[ATrain loss = 0.015 Train metric = 0.996\n",
            "Epoch 5:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.002, v_num=4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  88% 220/251 [03:45<00:31,  1.02s/it, loss=0.002, v_num=4]\n",
            "Epoch 5:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.002, v_num=4]\n",
            "Epoch 5:  88% 222/251 [03:46<00:29,  1.02s/it, loss=0.002, v_num=4]\n",
            "Epoch 5:  89% 223/251 [03:46<00:28,  1.02s/it, loss=0.002, v_num=4]\n",
            "Epoch 5:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.002, v_num=4]\n",
            "Epoch 5:  90% 225/251 [03:47<00:26,  1.01s/it, loss=0.002, v_num=4]\n",
            "Epoch 5:  90% 226/251 [03:47<00:25,  1.01s/it, loss=0.002, v_num=4]\n",
            "Epoch 5:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.002, v_num=4]\n",
            "Epoch 5:  91% 228/251 [03:48<00:23,  1.00s/it, loss=0.002, v_num=4]\n",
            "Epoch 5:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.002, v_num=4]\n",
            "Epoch 5:  92% 230/251 [03:48<00:20,  1.00it/s, loss=0.002, v_num=4]\n",
            "Epoch 5:  92% 231/251 [03:49<00:19,  1.01it/s, loss=0.002, v_num=4]\n",
            "Epoch 5:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.002, v_num=4]\n",
            "Epoch 5:  93% 233/251 [03:50<00:17,  1.01it/s, loss=0.002, v_num=4]\n",
            "Epoch 5:  93% 234/251 [03:50<00:16,  1.02it/s, loss=0.002, v_num=4]\n",
            "Epoch 5:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.002, v_num=4]\n",
            "Epoch 5:  94% 236/251 [03:51<00:14,  1.02it/s, loss=0.002, v_num=4]\n",
            "Epoch 5:  94% 237/251 [03:51<00:13,  1.02it/s, loss=0.002, v_num=4]\n",
            "Epoch 5:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.002, v_num=4]\n",
            "Epoch 5:  95% 239/251 [03:52<00:11,  1.03it/s, loss=0.002, v_num=4]\n",
            "Epoch 5:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.002, v_num=4]\n",
            "Epoch 5:  96% 241/251 [03:52<00:09,  1.03it/s, loss=0.002, v_num=4]\n",
            "Epoch 5:  96% 242/251 [03:53<00:08,  1.04it/s, loss=0.002, v_num=4]\n",
            "Epoch 5:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.002, v_num=4]\n",
            "Epoch 5:  97% 244/251 [03:53<00:06,  1.04it/s, loss=0.002, v_num=4]\n",
            "Epoch 5:  98% 245/251 [03:54<00:05,  1.05it/s, loss=0.002, v_num=4]\n",
            "Epoch 5:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.002, v_num=4]\n",
            "Epoch 5:  98% 247/251 [03:55<00:03,  1.05it/s, loss=0.002, v_num=4]\n",
            "Epoch 5:  99% 248/251 [03:55<00:02,  1.05it/s, loss=0.002, v_num=4]\n",
            "Epoch 5:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.002, v_num=4]\n",
            "Epoch 5: 100% 250/251 [03:56<00:00,  1.06it/s, loss=0.002, v_num=4]\n",
            "Epoch 5: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.002, v_num=4]val loss = 0.457 val metric = 0.91 \n",
            "\n",
            "Epoch 00005: val_metric  was not in top 1\n",
            "Epoch 5: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.002, v_num=4]\n",
            "                                               \u001b[ATrain loss = 0.015 Train metric = 0.995\n",
            "Epoch 6:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.001, v_num=4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  88% 220/251 [03:44<00:31,  1.02s/it, loss=0.001, v_num=4]\n",
            "Epoch 6:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.001, v_num=4]\n",
            "Epoch 6:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.001, v_num=4]\n",
            "Epoch 6:  89% 223/251 [03:45<00:28,  1.01s/it, loss=0.001, v_num=4]\n",
            "Epoch 6:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.001, v_num=4]\n",
            "Epoch 6:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.001, v_num=4]\n",
            "Epoch 6:  90% 226/251 [03:46<00:25,  1.00s/it, loss=0.001, v_num=4]\n",
            "Epoch 6:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.001, v_num=4]\n",
            "Epoch 6:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  91% 229/251 [03:47<00:21,  1.00it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  92% 231/251 [03:48<00:19,  1.01it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  93% 233/251 [03:49<00:17,  1.02it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  93% 234/251 [03:49<00:16,  1.02it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  94% 237/251 [03:50<00:13,  1.03it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  96% 240/251 [03:51<00:10,  1.04it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  97% 243/251 [03:52<00:07,  1.04it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  97% 244/251 [03:53<00:06,  1.05it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  98% 245/251 [03:53<00:05,  1.05it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  98% 246/251 [03:53<00:04,  1.05it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  99% 248/251 [03:54<00:02,  1.06it/s, loss=0.001, v_num=4]\n",
            "Epoch 6:  99% 249/251 [03:54<00:01,  1.06it/s, loss=0.001, v_num=4]\n",
            "Epoch 6: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.001, v_num=4]\n",
            "Epoch 6: 100% 251/251 [03:55<00:00,  1.07it/s, loss=0.001, v_num=4]val loss = 0.442 val metric = 0.909 \n",
            "\n",
            "Epoch 00006: val_metric  was not in top 1\n",
            "Epoch 6: 100% 251/251 [03:55<00:00,  1.07it/s, loss=0.001, v_num=4]\n",
            "                                               \u001b[ATrain loss = 0.011 Train metric = 0.997\n",
            "Epoch 7:  87% 219/251 [03:45<00:32,  1.03s/it, loss=0.001, v_num=4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  88% 220/251 [03:45<00:31,  1.03s/it, loss=0.001, v_num=4]\n",
            "Epoch 7:  88% 221/251 [03:46<00:30,  1.02s/it, loss=0.001, v_num=4]\n",
            "Epoch 7:  88% 222/251 [03:46<00:29,  1.02s/it, loss=0.001, v_num=4]\n",
            "Epoch 7:  89% 223/251 [03:46<00:28,  1.02s/it, loss=0.001, v_num=4]\n",
            "Epoch 7:  89% 224/251 [03:47<00:27,  1.01s/it, loss=0.001, v_num=4]\n",
            "Epoch 7:  90% 225/251 [03:47<00:26,  1.01s/it, loss=0.001, v_num=4]\n",
            "Epoch 7:  90% 226/251 [03:47<00:25,  1.01s/it, loss=0.001, v_num=4]\n",
            "Epoch 7:  90% 227/251 [03:48<00:24,  1.01s/it, loss=0.001, v_num=4]\n",
            "Epoch 7:  91% 228/251 [03:48<00:23,  1.00s/it, loss=0.001, v_num=4]\n",
            "Epoch 7:  91% 229/251 [03:49<00:22,  1.00s/it, loss=0.001, v_num=4]\n",
            "Epoch 7:  92% 230/251 [03:49<00:20,  1.00it/s, loss=0.001, v_num=4]\n",
            "Epoch 7:  92% 231/251 [03:49<00:19,  1.01it/s, loss=0.001, v_num=4]\n",
            "Epoch 7:  92% 232/251 [03:50<00:18,  1.01it/s, loss=0.001, v_num=4]\n",
            "Epoch 7:  93% 233/251 [03:50<00:17,  1.01it/s, loss=0.001, v_num=4]\n",
            "Epoch 7:  93% 234/251 [03:50<00:16,  1.01it/s, loss=0.001, v_num=4]\n",
            "Epoch 7:  94% 235/251 [03:51<00:15,  1.02it/s, loss=0.001, v_num=4]\n",
            "Epoch 7:  94% 236/251 [03:51<00:14,  1.02it/s, loss=0.001, v_num=4]\n",
            "Epoch 7:  94% 237/251 [03:51<00:13,  1.02it/s, loss=0.001, v_num=4]\n",
            "Epoch 7:  95% 238/251 [03:52<00:12,  1.02it/s, loss=0.001, v_num=4]\n",
            "Epoch 7:  95% 239/251 [03:52<00:11,  1.03it/s, loss=0.001, v_num=4]\n",
            "Epoch 7:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.001, v_num=4]\n",
            "Epoch 7:  96% 241/251 [03:53<00:09,  1.03it/s, loss=0.001, v_num=4]\n",
            "Epoch 7:  96% 242/251 [03:53<00:08,  1.04it/s, loss=0.001, v_num=4]\n",
            "Epoch 7:  97% 243/251 [03:54<00:07,  1.04it/s, loss=0.001, v_num=4]\n",
            "Epoch 7:  97% 244/251 [03:54<00:06,  1.04it/s, loss=0.001, v_num=4]\n",
            "Epoch 7:  98% 245/251 [03:54<00:05,  1.04it/s, loss=0.001, v_num=4]\n",
            "Epoch 7:  98% 246/251 [03:55<00:04,  1.05it/s, loss=0.001, v_num=4]\n",
            "Epoch 7:  98% 247/251 [03:55<00:03,  1.05it/s, loss=0.001, v_num=4]\n",
            "Epoch 7:  99% 248/251 [03:55<00:02,  1.05it/s, loss=0.001, v_num=4]\n",
            "Epoch 7:  99% 249/251 [03:56<00:01,  1.05it/s, loss=0.001, v_num=4]\n",
            "Epoch 7: 100% 250/251 [03:56<00:00,  1.06it/s, loss=0.001, v_num=4]\n",
            "Epoch 7: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.001, v_num=4]val loss = 0.481 val metric = 0.905 \n",
            "\n",
            "Epoch 00007: val_metric  was not in top 1\n",
            "Epoch 7: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.001, v_num=4]\n",
            "                                               \u001b[ATrain loss = 0.009 Train metric = 0.997\n",
            "Epoch 8:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.000, v_num=4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 8:  88% 220/251 [03:45<00:31,  1.02s/it, loss=0.000, v_num=4]\n",
            "Epoch 8:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.000, v_num=4]\n",
            "Epoch 8:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.000, v_num=4]\n",
            "Epoch 8:  89% 223/251 [03:46<00:28,  1.01s/it, loss=0.000, v_num=4]\n",
            "Epoch 8:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.000, v_num=4]\n",
            "Epoch 8:  90% 225/251 [03:47<00:26,  1.01s/it, loss=0.000, v_num=4]\n",
            "Epoch 8:  90% 226/251 [03:47<00:25,  1.01s/it, loss=0.000, v_num=4]\n",
            "Epoch 8:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.000, v_num=4]\n",
            "Epoch 8:  91% 228/251 [03:48<00:23,  1.00s/it, loss=0.000, v_num=4]\n",
            "Epoch 8:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.000, v_num=4]\n",
            "Epoch 8:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.000, v_num=4]\n",
            "Epoch 8:  92% 231/251 [03:49<00:19,  1.01it/s, loss=0.000, v_num=4]\n",
            "Epoch 8:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.000, v_num=4]\n",
            "Epoch 8:  93% 233/251 [03:49<00:17,  1.01it/s, loss=0.000, v_num=4]\n",
            "Epoch 8:  93% 234/251 [03:50<00:16,  1.02it/s, loss=0.000, v_num=4]\n",
            "Epoch 8:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.000, v_num=4]\n",
            "Epoch 8:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.000, v_num=4]\n",
            "Epoch 8:  94% 237/251 [03:51<00:13,  1.02it/s, loss=0.000, v_num=4]\n",
            "Epoch 8:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.000, v_num=4]\n",
            "Epoch 8:  95% 239/251 [03:52<00:11,  1.03it/s, loss=0.000, v_num=4]\n",
            "Epoch 8:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.000, v_num=4]\n",
            "Epoch 8:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.000, v_num=4]\n",
            "Epoch 8:  96% 242/251 [03:53<00:08,  1.04it/s, loss=0.000, v_num=4]\n",
            "Epoch 8:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.000, v_num=4]\n",
            "Epoch 8:  97% 244/251 [03:53<00:06,  1.04it/s, loss=0.000, v_num=4]\n",
            "Epoch 8:  98% 245/251 [03:54<00:05,  1.05it/s, loss=0.000, v_num=4]\n",
            "Epoch 8:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.000, v_num=4]\n",
            "Epoch 8:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.000, v_num=4]\n",
            "Epoch 8:  99% 248/251 [03:55<00:02,  1.05it/s, loss=0.000, v_num=4]\n",
            "Epoch 8:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.000, v_num=4]\n",
            "Epoch 8: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.000, v_num=4]\n",
            "Epoch 8: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.000, v_num=4]val loss = 0.545 val metric = 0.902 \n",
            "\n",
            "Epoch 00008: val_metric  was not in top 1\n",
            "Epoch 8: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.000, v_num=4]\n",
            "                                               \u001b[ATrain loss = 0.003 Train metric = 0.999\n",
            "Epoch 9:  87% 219/251 [03:45<00:32,  1.03s/it, loss=0.000, v_num=4]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 9:  88% 220/251 [03:45<00:31,  1.02s/it, loss=0.000, v_num=4]\n",
            "Epoch 9:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.000, v_num=4]\n",
            "Epoch 9:  88% 222/251 [03:46<00:29,  1.02s/it, loss=0.000, v_num=4]\n",
            "Epoch 9:  89% 223/251 [03:46<00:28,  1.02s/it, loss=0.000, v_num=4]\n",
            "Epoch 9:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.000, v_num=4]\n",
            "Epoch 9:  90% 225/251 [03:47<00:26,  1.01s/it, loss=0.000, v_num=4]\n",
            "Epoch 9:  90% 226/251 [03:47<00:25,  1.01s/it, loss=0.000, v_num=4]\n",
            "Epoch 9:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.000, v_num=4]\n",
            "Epoch 9:  91% 228/251 [03:48<00:23,  1.00s/it, loss=0.000, v_num=4]\n",
            "Epoch 9:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.000, v_num=4]\n",
            "Epoch 9:  92% 230/251 [03:49<00:20,  1.00it/s, loss=0.000, v_num=4]\n",
            "Epoch 9:  92% 231/251 [03:49<00:19,  1.01it/s, loss=0.000, v_num=4]\n",
            "Epoch 9:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.000, v_num=4]\n",
            "Epoch 9:  93% 233/251 [03:50<00:17,  1.01it/s, loss=0.000, v_num=4]\n",
            "Epoch 9:  93% 234/251 [03:50<00:16,  1.02it/s, loss=0.000, v_num=4]\n",
            "Epoch 9:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.000, v_num=4]\n",
            "Epoch 9:  94% 236/251 [03:51<00:14,  1.02it/s, loss=0.000, v_num=4]\n",
            "Epoch 9:  94% 237/251 [03:51<00:13,  1.02it/s, loss=0.000, v_num=4]\n",
            "Epoch 9:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.000, v_num=4]\n",
            "Epoch 9:  95% 239/251 [03:52<00:11,  1.03it/s, loss=0.000, v_num=4]\n",
            "Epoch 9:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.000, v_num=4]\n",
            "Epoch 9:  96% 241/251 [03:52<00:09,  1.03it/s, loss=0.000, v_num=4]\n",
            "Epoch 9:  96% 242/251 [03:53<00:08,  1.04it/s, loss=0.000, v_num=4]\n",
            "Epoch 9:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.000, v_num=4]\n",
            "Epoch 9:  97% 244/251 [03:54<00:06,  1.04it/s, loss=0.000, v_num=4]\n",
            "Epoch 9:  98% 245/251 [03:54<00:05,  1.05it/s, loss=0.000, v_num=4]\n",
            "Epoch 9:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.000, v_num=4]\n",
            "Epoch 9:  98% 247/251 [03:55<00:03,  1.05it/s, loss=0.000, v_num=4]\n",
            "Epoch 9:  99% 248/251 [03:55<00:02,  1.05it/s, loss=0.000, v_num=4]\n",
            "Epoch 9:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.000, v_num=4]\n",
            "Epoch 9: 100% 250/251 [03:56<00:00,  1.06it/s, loss=0.000, v_num=4]\n",
            "Epoch 9: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.000, v_num=4]val loss = 0.484 val metric = 0.907 \n",
            "\n",
            "Epoch 00009: val_metric  was not in top 1\n",
            "Epoch 9: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.000, v_num=4]\n",
            "                                               \u001b[ATrain loss = 0.004 Train metric = 0.999\n",
            "Saving latest checkpoint..\n",
            "Epoch 9: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.000, v_num=4]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0, mixout prob 0.6, ...\n",
            "f1                                                      0.904082\n",
            "precision                                               0.872047\n",
            "recall                                                  0.938559\n",
            "Name: 0, dtype: object\n",
            "2020-09-02 10:25:21.167322: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "100% 7000/7000 [00:04<00:00, 1668.34it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1957.01it/s]\n",
            "Train and val loader length 219 and 32\n",
            "Modelling\n",
            "Device: cuda\n",
            "TransformerWithMixout(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.7, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (drop): Dropout(p=0, inplace=False)\n",
            "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory /content/drive/My Drive/Models/WNUT-Task2/model16/3/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "[LOG] Total number of parameters to learn 124646401\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name   | Type                  | Params\n",
            "-------------------------------------------------\n",
            "0 | model  | TransformerWithMixout | 124 M \n",
            "1 | metric | PLAccuracy            | 0     \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  2.37it/s]val loss = 0.703 val metric = 0.438 \n",
            "Epoch 0:   0% 0/251 [00:00<?, ?it/s] /usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Epoch 0:  87% 219/251 [03:44<00:32,  1.02s/it, loss=0.165, v_num=5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  88% 220/251 [03:44<00:31,  1.02s/it, loss=0.165, v_num=5]\n",
            "Epoch 0:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.165, v_num=5]\n",
            "Epoch 0:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.165, v_num=5]\n",
            "Epoch 0:  89% 223/251 [03:45<00:28,  1.01s/it, loss=0.165, v_num=5]\n",
            "Epoch 0:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.165, v_num=5]\n",
            "Epoch 0:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.165, v_num=5]\n",
            "Epoch 0:  90% 226/251 [03:46<00:25,  1.00s/it, loss=0.165, v_num=5]\n",
            "Epoch 0:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.165, v_num=5]\n",
            "Epoch 0:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  92% 231/251 [03:48<00:19,  1.01it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  93% 233/251 [03:49<00:17,  1.02it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  93% 234/251 [03:49<00:16,  1.02it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  94% 237/251 [03:50<00:13,  1.03it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  97% 244/251 [03:53<00:06,  1.05it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  98% 245/251 [03:53<00:05,  1.05it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  99% 248/251 [03:54<00:02,  1.06it/s, loss=0.165, v_num=5]\n",
            "Epoch 0:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.165, v_num=5]\n",
            "Epoch 0: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.165, v_num=5]\n",
            "Epoch 0: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.165, v_num=5]val loss = 0.276 val metric = 0.884 \n",
            "\n",
            "Epoch 00000: val_metric reached 0.88379 (best 0.88379), saving model to /content/drive/My Drive/Models/WNUT-Task2/model16/3/epoch=0.ckpt as top 1\n",
            "Epoch 0: 100% 251/251 [03:58<00:00,  1.05it/s, loss=0.165, v_num=5]\n",
            "                                               \u001b[ATrain loss = 0.267 Train metric = 0.884\n",
            "Epoch 1:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.070, v_num=5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  88% 220/251 [03:45<00:31,  1.02s/it, loss=0.070, v_num=5]\n",
            "Epoch 1:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.070, v_num=5]\n",
            "Epoch 1:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.070, v_num=5]\n",
            "Epoch 1:  89% 223/251 [03:46<00:28,  1.01s/it, loss=0.070, v_num=5]\n",
            "Epoch 1:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.070, v_num=5]\n",
            "Epoch 1:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.070, v_num=5]\n",
            "Epoch 1:  90% 226/251 [03:47<00:25,  1.01s/it, loss=0.070, v_num=5]\n",
            "Epoch 1:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.070, v_num=5]\n",
            "Epoch 1:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  92% 231/251 [03:49<00:19,  1.01it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  93% 233/251 [03:49<00:17,  1.01it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  93% 234/251 [03:50<00:16,  1.02it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  94% 237/251 [03:51<00:13,  1.03it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  96% 242/251 [03:53<00:08,  1.04it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  97% 244/251 [03:53<00:06,  1.04it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  98% 245/251 [03:54<00:05,  1.05it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  99% 248/251 [03:55<00:02,  1.05it/s, loss=0.070, v_num=5]\n",
            "Epoch 1:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.070, v_num=5]\n",
            "Epoch 1: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.070, v_num=5]\n",
            "Epoch 1: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.070, v_num=5]val loss = 0.301 val metric = 0.907 \n",
            "\n",
            "Epoch 00001: val_metric reached 0.90723 (best 0.90723), saving model to /content/drive/My Drive/Models/WNUT-Task2/model16/3/epoch=1.ckpt as top 1\n",
            "Epoch 1: 100% 251/251 [03:58<00:00,  1.05it/s, loss=0.070, v_num=5]\n",
            "                                               \u001b[ATrain loss = 0.078 Train metric = 0.973\n",
            "Epoch 2:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.019, v_num=5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 2:  88% 220/251 [03:45<00:31,  1.02s/it, loss=0.019, v_num=5]\n",
            "Epoch 2:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.019, v_num=5]\n",
            "Epoch 2:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.019, v_num=5]\n",
            "Epoch 2:  89% 223/251 [03:46<00:28,  1.01s/it, loss=0.019, v_num=5]\n",
            "Epoch 2:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.019, v_num=5]\n",
            "Epoch 2:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.019, v_num=5]\n",
            "Epoch 2:  90% 226/251 [03:47<00:25,  1.00s/it, loss=0.019, v_num=5]\n",
            "Epoch 2:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.019, v_num=5]\n",
            "Epoch 2:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  92% 231/251 [03:48<00:19,  1.01it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  93% 233/251 [03:49<00:17,  1.01it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  93% 234/251 [03:49<00:16,  1.02it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  94% 237/251 [03:50<00:13,  1.03it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  97% 244/251 [03:53<00:06,  1.05it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  98% 245/251 [03:53<00:05,  1.05it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  99% 248/251 [03:54<00:02,  1.06it/s, loss=0.019, v_num=5]\n",
            "Epoch 2:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.019, v_num=5]\n",
            "Epoch 2: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.019, v_num=5]\n",
            "Epoch 2: 100% 251/251 [03:55<00:00,  1.07it/s, loss=0.019, v_num=5]val loss = 0.348 val metric = 0.899 \n",
            "\n",
            "Epoch 00002: val_metric  was not in top 1\n",
            "Epoch 2: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.019, v_num=5]\n",
            "                                               \u001b[ATrain loss = 0.032 Train metric = 0.99\n",
            "Epoch 3:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.008, v_num=5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  88% 220/251 [03:44<00:31,  1.02s/it, loss=0.008, v_num=5]\n",
            "Epoch 3:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.008, v_num=5]\n",
            "Epoch 3:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.008, v_num=5]\n",
            "Epoch 3:  89% 223/251 [03:46<00:28,  1.01s/it, loss=0.008, v_num=5]\n",
            "Epoch 3:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.008, v_num=5]\n",
            "Epoch 3:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.008, v_num=5]\n",
            "Epoch 3:  90% 226/251 [03:47<00:25,  1.00s/it, loss=0.008, v_num=5]\n",
            "Epoch 3:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.008, v_num=5]\n",
            "Epoch 3:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  92% 231/251 [03:48<00:19,  1.01it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  93% 233/251 [03:49<00:17,  1.01it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  93% 234/251 [03:50<00:16,  1.02it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  94% 237/251 [03:51<00:13,  1.03it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  97% 244/251 [03:53<00:06,  1.04it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  98% 245/251 [03:53<00:05,  1.05it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  99% 248/251 [03:55<00:02,  1.06it/s, loss=0.008, v_num=5]\n",
            "Epoch 3:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.008, v_num=5]\n",
            "Epoch 3: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.008, v_num=5]\n",
            "Epoch 3: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.008, v_num=5]val loss = 0.429 val metric = 0.896 \n",
            "\n",
            "Epoch 00003: val_metric  was not in top 1\n",
            "Epoch 3: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.008, v_num=5]\n",
            "                                               \u001b[ATrain loss = 0.014 Train metric = 0.996\n",
            "Epoch 4:  87% 219/251 [03:44<00:32,  1.02s/it, loss=0.008, v_num=5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 4:  88% 220/251 [03:44<00:31,  1.02s/it, loss=0.008, v_num=5]\n",
            "Epoch 4:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.008, v_num=5]\n",
            "Epoch 4:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.008, v_num=5]\n",
            "Epoch 4:  89% 223/251 [03:45<00:28,  1.01s/it, loss=0.008, v_num=5]\n",
            "Epoch 4:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.008, v_num=5]\n",
            "Epoch 4:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.008, v_num=5]\n",
            "Epoch 4:  90% 226/251 [03:46<00:25,  1.00s/it, loss=0.008, v_num=5]\n",
            "Epoch 4:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.008, v_num=5]\n",
            "Epoch 4:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  91% 229/251 [03:47<00:21,  1.00it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  92% 231/251 [03:48<00:19,  1.01it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  92% 232/251 [03:48<00:18,  1.01it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  93% 233/251 [03:49<00:17,  1.02it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  93% 234/251 [03:49<00:16,  1.02it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  94% 237/251 [03:50<00:13,  1.03it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  96% 240/251 [03:51<00:10,  1.04it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  97% 243/251 [03:52<00:07,  1.04it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  97% 244/251 [03:53<00:06,  1.05it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  98% 245/251 [03:53<00:05,  1.05it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  98% 246/251 [03:53<00:04,  1.05it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  99% 248/251 [03:54<00:02,  1.06it/s, loss=0.008, v_num=5]\n",
            "Epoch 4:  99% 249/251 [03:54<00:01,  1.06it/s, loss=0.008, v_num=5]\n",
            "Epoch 4: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.008, v_num=5]\n",
            "Epoch 4: 100% 251/251 [03:55<00:00,  1.07it/s, loss=0.008, v_num=5]val loss = 0.386 val metric = 0.911 \n",
            "\n",
            "Epoch 00004: val_metric reached 0.91113 (best 0.91113), saving model to /content/drive/My Drive/Models/WNUT-Task2/model16/3/epoch=4.ckpt as top 1\n",
            "Epoch 4: 100% 251/251 [03:57<00:00,  1.05it/s, loss=0.008, v_num=5]\n",
            "                                               \u001b[ATrain loss = 0.016 Train metric = 0.995\n",
            "Epoch 5:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.001, v_num=5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  88% 220/251 [03:45<00:31,  1.02s/it, loss=0.001, v_num=5]\n",
            "Epoch 5:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.001, v_num=5]\n",
            "Epoch 5:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.001, v_num=5]\n",
            "Epoch 5:  89% 223/251 [03:46<00:28,  1.02s/it, loss=0.001, v_num=5]\n",
            "Epoch 5:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.001, v_num=5]\n",
            "Epoch 5:  90% 225/251 [03:47<00:26,  1.01s/it, loss=0.001, v_num=5]\n",
            "Epoch 5:  90% 226/251 [03:47<00:25,  1.01s/it, loss=0.001, v_num=5]\n",
            "Epoch 5:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.001, v_num=5]\n",
            "Epoch 5:  91% 228/251 [03:48<00:23,  1.00s/it, loss=0.001, v_num=5]\n",
            "Epoch 5:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.001, v_num=5]\n",
            "Epoch 5:  92% 230/251 [03:48<00:20,  1.00it/s, loss=0.001, v_num=5]\n",
            "Epoch 5:  92% 231/251 [03:49<00:19,  1.01it/s, loss=0.001, v_num=5]\n",
            "Epoch 5:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.001, v_num=5]\n",
            "Epoch 5:  93% 233/251 [03:49<00:17,  1.01it/s, loss=0.001, v_num=5]\n",
            "Epoch 5:  93% 234/251 [03:50<00:16,  1.02it/s, loss=0.001, v_num=5]\n",
            "Epoch 5:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.001, v_num=5]\n",
            "Epoch 5:  94% 236/251 [03:51<00:14,  1.02it/s, loss=0.001, v_num=5]\n",
            "Epoch 5:  94% 237/251 [03:51<00:13,  1.02it/s, loss=0.001, v_num=5]\n",
            "Epoch 5:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.001, v_num=5]\n",
            "Epoch 5:  95% 239/251 [03:52<00:11,  1.03it/s, loss=0.001, v_num=5]\n",
            "Epoch 5:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.001, v_num=5]\n",
            "Epoch 5:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.001, v_num=5]\n",
            "Epoch 5:  96% 242/251 [03:53<00:08,  1.04it/s, loss=0.001, v_num=5]\n",
            "Epoch 5:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.001, v_num=5]\n",
            "Epoch 5:  97% 244/251 [03:53<00:06,  1.04it/s, loss=0.001, v_num=5]\n",
            "Epoch 5:  98% 245/251 [03:54<00:05,  1.05it/s, loss=0.001, v_num=5]\n",
            "Epoch 5:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.001, v_num=5]\n",
            "Epoch 5:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.001, v_num=5]\n",
            "Epoch 5:  99% 248/251 [03:55<00:02,  1.05it/s, loss=0.001, v_num=5]\n",
            "Epoch 5:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.001, v_num=5]\n",
            "Epoch 5: 100% 250/251 [03:56<00:00,  1.06it/s, loss=0.001, v_num=5]\n",
            "Epoch 5: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.001, v_num=5]val loss = 0.533 val metric = 0.9 \n",
            "\n",
            "Epoch 00005: val_metric  was not in top 1\n",
            "Epoch 5: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.001, v_num=5]\n",
            "                                               \u001b[ATrain loss = 0.012 Train metric = 0.996\n",
            "Epoch 6:  87% 219/251 [03:44<00:32,  1.02s/it, loss=0.010, v_num=5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  88% 220/251 [03:44<00:31,  1.02s/it, loss=0.010, v_num=5]\n",
            "Epoch 6:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.010, v_num=5]\n",
            "Epoch 6:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.010, v_num=5]\n",
            "Epoch 6:  89% 223/251 [03:45<00:28,  1.01s/it, loss=0.010, v_num=5]\n",
            "Epoch 6:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.010, v_num=5]\n",
            "Epoch 6:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.010, v_num=5]\n",
            "Epoch 6:  90% 226/251 [03:46<00:25,  1.00s/it, loss=0.010, v_num=5]\n",
            "Epoch 6:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.010, v_num=5]\n",
            "Epoch 6:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  91% 229/251 [03:47<00:21,  1.00it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  92% 231/251 [03:48<00:19,  1.01it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  93% 233/251 [03:49<00:17,  1.02it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  93% 234/251 [03:49<00:16,  1.02it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  94% 237/251 [03:50<00:13,  1.03it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  96% 240/251 [03:51<00:10,  1.04it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  97% 243/251 [03:52<00:07,  1.04it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  97% 244/251 [03:53<00:06,  1.05it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  98% 245/251 [03:53<00:05,  1.05it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  98% 246/251 [03:53<00:04,  1.05it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  99% 248/251 [03:54<00:02,  1.06it/s, loss=0.010, v_num=5]\n",
            "Epoch 6:  99% 249/251 [03:54<00:01,  1.06it/s, loss=0.010, v_num=5]\n",
            "Epoch 6: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.010, v_num=5]\n",
            "Epoch 6: 100% 251/251 [03:55<00:00,  1.07it/s, loss=0.010, v_num=5]val loss = 0.349 val metric = 0.908 \n",
            "\n",
            "Epoch 00006: val_metric  was not in top 1\n",
            "Epoch 6: 100% 251/251 [03:55<00:00,  1.07it/s, loss=0.010, v_num=5]\n",
            "                                               \u001b[ATrain loss = 0.021 Train metric = 0.993\n",
            "Epoch 7:  87% 219/251 [03:45<00:32,  1.03s/it, loss=0.001, v_num=5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  88% 220/251 [03:45<00:31,  1.02s/it, loss=0.001, v_num=5]\n",
            "Epoch 7:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.001, v_num=5]\n",
            "Epoch 7:  88% 222/251 [03:46<00:29,  1.02s/it, loss=0.001, v_num=5]\n",
            "Epoch 7:  89% 223/251 [03:46<00:28,  1.02s/it, loss=0.001, v_num=5]\n",
            "Epoch 7:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.001, v_num=5]\n",
            "Epoch 7:  90% 225/251 [03:47<00:26,  1.01s/it, loss=0.001, v_num=5]\n",
            "Epoch 7:  90% 226/251 [03:47<00:25,  1.01s/it, loss=0.001, v_num=5]\n",
            "Epoch 7:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.001, v_num=5]\n",
            "Epoch 7:  91% 228/251 [03:48<00:23,  1.00s/it, loss=0.001, v_num=5]\n",
            "Epoch 7:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.001, v_num=5]\n",
            "Epoch 7:  92% 230/251 [03:49<00:20,  1.00it/s, loss=0.001, v_num=5]\n",
            "Epoch 7:  92% 231/251 [03:49<00:19,  1.01it/s, loss=0.001, v_num=5]\n",
            "Epoch 7:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.001, v_num=5]\n",
            "Epoch 7:  93% 233/251 [03:50<00:17,  1.01it/s, loss=0.001, v_num=5]\n",
            "Epoch 7:  93% 234/251 [03:50<00:16,  1.02it/s, loss=0.001, v_num=5]\n",
            "Epoch 7:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.001, v_num=5]\n",
            "Epoch 7:  94% 236/251 [03:51<00:14,  1.02it/s, loss=0.001, v_num=5]\n",
            "Epoch 7:  94% 237/251 [03:51<00:13,  1.02it/s, loss=0.001, v_num=5]\n",
            "Epoch 7:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.001, v_num=5]\n",
            "Epoch 7:  95% 239/251 [03:52<00:11,  1.03it/s, loss=0.001, v_num=5]\n",
            "Epoch 7:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.001, v_num=5]\n",
            "Epoch 7:  96% 241/251 [03:53<00:09,  1.03it/s, loss=0.001, v_num=5]\n",
            "Epoch 7:  96% 242/251 [03:53<00:08,  1.04it/s, loss=0.001, v_num=5]\n",
            "Epoch 7:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.001, v_num=5]\n",
            "Epoch 7:  97% 244/251 [03:54<00:06,  1.04it/s, loss=0.001, v_num=5]\n",
            "Epoch 7:  98% 245/251 [03:54<00:05,  1.04it/s, loss=0.001, v_num=5]\n",
            "Epoch 7:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.001, v_num=5]\n",
            "Epoch 7:  98% 247/251 [03:55<00:03,  1.05it/s, loss=0.001, v_num=5]\n",
            "Epoch 7:  99% 248/251 [03:55<00:02,  1.05it/s, loss=0.001, v_num=5]\n",
            "Epoch 7:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.001, v_num=5]\n",
            "Epoch 7: 100% 250/251 [03:56<00:00,  1.06it/s, loss=0.001, v_num=5]\n",
            "Epoch 7: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.001, v_num=5]val loss = 0.482 val metric = 0.904 \n",
            "\n",
            "Epoch 00007: val_metric  was not in top 1\n",
            "Epoch 7: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.001, v_num=5]\n",
            "                                               \u001b[ATrain loss = 0.008 Train metric = 0.998\n",
            "Epoch 8:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.008, v_num=5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 8:  88% 220/251 [03:45<00:31,  1.02s/it, loss=0.008, v_num=5]\n",
            "Epoch 8:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.008, v_num=5]\n",
            "Epoch 8:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.008, v_num=5]\n",
            "Epoch 8:  89% 223/251 [03:46<00:28,  1.01s/it, loss=0.008, v_num=5]\n",
            "Epoch 8:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.008, v_num=5]\n",
            "Epoch 8:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.008, v_num=5]\n",
            "Epoch 8:  90% 226/251 [03:47<00:25,  1.01s/it, loss=0.008, v_num=5]\n",
            "Epoch 8:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.008, v_num=5]\n",
            "Epoch 8:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  92% 231/251 [03:48<00:19,  1.01it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  93% 233/251 [03:49<00:17,  1.01it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  93% 234/251 [03:49<00:16,  1.02it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  94% 237/251 [03:51<00:13,  1.03it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  97% 244/251 [03:53<00:06,  1.05it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  98% 245/251 [03:53<00:05,  1.05it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  99% 248/251 [03:54<00:02,  1.06it/s, loss=0.008, v_num=5]\n",
            "Epoch 8:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.008, v_num=5]\n",
            "Epoch 8: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.008, v_num=5]\n",
            "Epoch 8: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.008, v_num=5]val loss = 0.43 val metric = 0.9 \n",
            "\n",
            "Epoch 00008: val_metric  was not in top 1\n",
            "Epoch 8: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.008, v_num=5]\n",
            "                                               \u001b[ATrain loss = 0.005 Train metric = 0.998\n",
            "Epoch 9:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.015, v_num=5]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 9:  88% 220/251 [03:45<00:31,  1.02s/it, loss=0.015, v_num=5]\n",
            "Epoch 9:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.015, v_num=5]\n",
            "Epoch 9:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.015, v_num=5]\n",
            "Epoch 9:  89% 223/251 [03:46<00:28,  1.01s/it, loss=0.015, v_num=5]\n",
            "Epoch 9:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.015, v_num=5]\n",
            "Epoch 9:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.015, v_num=5]\n",
            "Epoch 9:  90% 226/251 [03:47<00:25,  1.01s/it, loss=0.015, v_num=5]\n",
            "Epoch 9:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.015, v_num=5]\n",
            "Epoch 9:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  92% 231/251 [03:48<00:19,  1.01it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  93% 233/251 [03:49<00:17,  1.01it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  93% 234/251 [03:50<00:16,  1.02it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  94% 237/251 [03:51<00:13,  1.03it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  97% 244/251 [03:53<00:06,  1.04it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  98% 245/251 [03:54<00:05,  1.05it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  99% 248/251 [03:55<00:02,  1.05it/s, loss=0.015, v_num=5]\n",
            "Epoch 9:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.015, v_num=5]\n",
            "Epoch 9: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.015, v_num=5]\n",
            "Epoch 9: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.015, v_num=5]val loss = 0.461 val metric = 0.904 \n",
            "\n",
            "Epoch 00009: val_metric  was not in top 1\n",
            "Epoch 9: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.015, v_num=5]\n",
            "                                               \u001b[ATrain loss = 0.007 Train metric = 0.997\n",
            "Saving latest checkpoint..\n",
            "Epoch 9: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.015, v_num=5]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0, mixout prob 0.7, ...\n",
            "f1                                                      0.882288\n",
            "precision                                                0.91762\n",
            "recall                                                  0.849576\n",
            "Name: 0, dtype: object\n",
            "2020-09-02 11:05:13.896715: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "100% 7000/7000 [00:04<00:00, 1620.07it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1907.43it/s]\n",
            "Train and val loader length 219 and 32\n",
            "Modelling\n",
            "Device: cuda\n",
            "TransformerWithMixout(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): MixLinear(mixout=0.8, in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (drop): Dropout(p=0, inplace=False)\n",
            "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory /content/drive/My Drive/Models/WNUT-Task2/model16/4/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "[LOG] Total number of parameters to learn 124646401\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name   | Type                  | Params\n",
            "-------------------------------------------------\n",
            "0 | model  | TransformerWithMixout | 124 M \n",
            "1 | metric | PLAccuracy            | 0     \n",
            "Validation sanity check: 100% 1/1.0 [00:00<00:00,  2.35it/s]val loss = 0.703 val metric = 0.438 \n",
            "Epoch 0:   0% 0/251 [00:00<?, ?it/s] /usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Epoch 0:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.155, v_num=6]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  88% 220/251 [03:45<00:31,  1.02s/it, loss=0.155, v_num=6]\n",
            "Epoch 0:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.155, v_num=6]\n",
            "Epoch 0:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.155, v_num=6]\n",
            "Epoch 0:  89% 223/251 [03:46<00:28,  1.01s/it, loss=0.155, v_num=6]\n",
            "Epoch 0:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.155, v_num=6]\n",
            "Epoch 0:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.155, v_num=6]\n",
            "Epoch 0:  90% 226/251 [03:47<00:25,  1.01s/it, loss=0.155, v_num=6]\n",
            "Epoch 0:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.155, v_num=6]\n",
            "Epoch 0:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  92% 231/251 [03:49<00:19,  1.01it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  93% 233/251 [03:49<00:17,  1.01it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  93% 234/251 [03:50<00:16,  1.02it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  94% 237/251 [03:51<00:13,  1.03it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  97% 244/251 [03:53<00:06,  1.04it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  98% 245/251 [03:54<00:05,  1.05it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  99% 248/251 [03:55<00:02,  1.05it/s, loss=0.155, v_num=6]\n",
            "Epoch 0:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.155, v_num=6]\n",
            "Epoch 0: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.155, v_num=6]\n",
            "Epoch 0: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.155, v_num=6]val loss = 0.263 val metric = 0.895 \n",
            "\n",
            "Epoch 00000: val_metric reached 0.89453 (best 0.89453), saving model to /content/drive/My Drive/Models/WNUT-Task2/model16/4/epoch=0.ckpt as top 1\n",
            "Epoch 0: 100% 251/251 [03:58<00:00,  1.05it/s, loss=0.155, v_num=6]\n",
            "                                               \u001b[ATrain loss = 0.273 Train metric = 0.88\n",
            "Epoch 1:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.063, v_num=6]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  88% 220/251 [03:45<00:31,  1.02s/it, loss=0.063, v_num=6]\n",
            "Epoch 1:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.063, v_num=6]\n",
            "Epoch 1:  88% 222/251 [03:46<00:29,  1.02s/it, loss=0.063, v_num=6]\n",
            "Epoch 1:  89% 223/251 [03:46<00:28,  1.02s/it, loss=0.063, v_num=6]\n",
            "Epoch 1:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.063, v_num=6]\n",
            "Epoch 1:  90% 225/251 [03:47<00:26,  1.01s/it, loss=0.063, v_num=6]\n",
            "Epoch 1:  90% 226/251 [03:47<00:25,  1.01s/it, loss=0.063, v_num=6]\n",
            "Epoch 1:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.063, v_num=6]\n",
            "Epoch 1:  91% 228/251 [03:48<00:23,  1.00s/it, loss=0.063, v_num=6]\n",
            "Epoch 1:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.063, v_num=6]\n",
            "Epoch 1:  92% 230/251 [03:48<00:20,  1.00it/s, loss=0.063, v_num=6]\n",
            "Epoch 1:  92% 231/251 [03:49<00:19,  1.01it/s, loss=0.063, v_num=6]\n",
            "Epoch 1:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.063, v_num=6]\n",
            "Epoch 1:  93% 233/251 [03:50<00:17,  1.01it/s, loss=0.063, v_num=6]\n",
            "Epoch 1:  93% 234/251 [03:50<00:16,  1.02it/s, loss=0.063, v_num=6]\n",
            "Epoch 1:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.063, v_num=6]\n",
            "Epoch 1:  94% 236/251 [03:51<00:14,  1.02it/s, loss=0.063, v_num=6]\n",
            "Epoch 1:  94% 237/251 [03:51<00:13,  1.02it/s, loss=0.063, v_num=6]\n",
            "Epoch 1:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.063, v_num=6]\n",
            "Epoch 1:  95% 239/251 [03:52<00:11,  1.03it/s, loss=0.063, v_num=6]\n",
            "Epoch 1:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.063, v_num=6]\n",
            "Epoch 1:  96% 241/251 [03:52<00:09,  1.03it/s, loss=0.063, v_num=6]\n",
            "Epoch 1:  96% 242/251 [03:53<00:08,  1.04it/s, loss=0.063, v_num=6]\n",
            "Epoch 1:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.063, v_num=6]\n",
            "Epoch 1:  97% 244/251 [03:53<00:06,  1.04it/s, loss=0.063, v_num=6]\n",
            "Epoch 1:  98% 245/251 [03:54<00:05,  1.05it/s, loss=0.063, v_num=6]\n",
            "Epoch 1:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.063, v_num=6]\n",
            "Epoch 1:  98% 247/251 [03:55<00:03,  1.05it/s, loss=0.063, v_num=6]\n",
            "Epoch 1:  99% 248/251 [03:55<00:02,  1.05it/s, loss=0.063, v_num=6]\n",
            "Epoch 1:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.063, v_num=6]\n",
            "Epoch 1: 100% 250/251 [03:56<00:00,  1.06it/s, loss=0.063, v_num=6]\n",
            "Epoch 1: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.063, v_num=6]val loss = 0.312 val metric = 0.896 \n",
            "\n",
            "Epoch 00001: val_metric reached 0.89648 (best 0.89648), saving model to /content/drive/My Drive/Models/WNUT-Task2/model16/4/epoch=1.ckpt as top 1\n",
            "Epoch 1: 100% 251/251 [03:58<00:00,  1.05it/s, loss=0.063, v_num=6]\n",
            "                                               \u001b[ATrain loss = 0.08 Train metric = 0.972\n",
            "Epoch 2:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.017, v_num=6]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 2:  88% 220/251 [03:44<00:31,  1.02s/it, loss=0.017, v_num=6]\n",
            "Epoch 2:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.017, v_num=6]\n",
            "Epoch 2:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.017, v_num=6]\n",
            "Epoch 2:  89% 223/251 [03:45<00:28,  1.01s/it, loss=0.017, v_num=6]\n",
            "Epoch 2:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.017, v_num=6]\n",
            "Epoch 2:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.017, v_num=6]\n",
            "Epoch 2:  90% 226/251 [03:46<00:25,  1.00s/it, loss=0.017, v_num=6]\n",
            "Epoch 2:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.017, v_num=6]\n",
            "Epoch 2:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  92% 231/251 [03:48<00:19,  1.01it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  93% 233/251 [03:49<00:17,  1.02it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  93% 234/251 [03:49<00:16,  1.02it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  94% 237/251 [03:50<00:13,  1.03it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  96% 240/251 [03:51<00:10,  1.03it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  97% 243/251 [03:52<00:07,  1.04it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  97% 244/251 [03:53<00:06,  1.05it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  98% 245/251 [03:53<00:05,  1.05it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  99% 248/251 [03:54<00:02,  1.06it/s, loss=0.017, v_num=6]\n",
            "Epoch 2:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.017, v_num=6]\n",
            "Epoch 2: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.017, v_num=6]\n",
            "Epoch 2: 100% 251/251 [03:55<00:00,  1.07it/s, loss=0.017, v_num=6]val loss = 0.367 val metric = 0.906 \n",
            "\n",
            "Epoch 00002: val_metric reached 0.90625 (best 0.90625), saving model to /content/drive/My Drive/Models/WNUT-Task2/model16/4/epoch=2.ckpt as top 1\n",
            "Epoch 2: 100% 251/251 [03:57<00:00,  1.06it/s, loss=0.017, v_num=6]\n",
            "                                               \u001b[ATrain loss = 0.034 Train metric = 0.989\n",
            "Epoch 3:  87% 219/251 [03:45<00:32,  1.03s/it, loss=0.004, v_num=6]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  88% 220/251 [03:45<00:31,  1.03s/it, loss=0.004, v_num=6]\n",
            "Epoch 3:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.004, v_num=6]\n",
            "Epoch 3:  88% 222/251 [03:46<00:29,  1.02s/it, loss=0.004, v_num=6]\n",
            "Epoch 3:  89% 223/251 [03:46<00:28,  1.02s/it, loss=0.004, v_num=6]\n",
            "Epoch 3:  89% 224/251 [03:47<00:27,  1.01s/it, loss=0.004, v_num=6]\n",
            "Epoch 3:  90% 225/251 [03:47<00:26,  1.01s/it, loss=0.004, v_num=6]\n",
            "Epoch 3:  90% 226/251 [03:47<00:25,  1.01s/it, loss=0.004, v_num=6]\n",
            "Epoch 3:  90% 227/251 [03:48<00:24,  1.00s/it, loss=0.004, v_num=6]\n",
            "Epoch 3:  91% 228/251 [03:48<00:23,  1.00s/it, loss=0.004, v_num=6]\n",
            "Epoch 3:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.004, v_num=6]\n",
            "Epoch 3:  92% 230/251 [03:49<00:20,  1.00it/s, loss=0.004, v_num=6]\n",
            "Epoch 3:  92% 231/251 [03:49<00:19,  1.01it/s, loss=0.004, v_num=6]\n",
            "Epoch 3:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.004, v_num=6]\n",
            "Epoch 3:  93% 233/251 [03:50<00:17,  1.01it/s, loss=0.004, v_num=6]\n",
            "Epoch 3:  93% 234/251 [03:50<00:16,  1.01it/s, loss=0.004, v_num=6]\n",
            "Epoch 3:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.004, v_num=6]\n",
            "Epoch 3:  94% 236/251 [03:51<00:14,  1.02it/s, loss=0.004, v_num=6]\n",
            "Epoch 3:  94% 237/251 [03:51<00:13,  1.02it/s, loss=0.004, v_num=6]\n",
            "Epoch 3:  95% 238/251 [03:52<00:12,  1.03it/s, loss=0.004, v_num=6]\n",
            "Epoch 3:  95% 239/251 [03:52<00:11,  1.03it/s, loss=0.004, v_num=6]\n",
            "Epoch 3:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.004, v_num=6]\n",
            "Epoch 3:  96% 241/251 [03:53<00:09,  1.03it/s, loss=0.004, v_num=6]\n",
            "Epoch 3:  96% 242/251 [03:53<00:08,  1.04it/s, loss=0.004, v_num=6]\n",
            "Epoch 3:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.004, v_num=6]\n",
            "Epoch 3:  97% 244/251 [03:54<00:06,  1.04it/s, loss=0.004, v_num=6]\n",
            "Epoch 3:  98% 245/251 [03:54<00:05,  1.04it/s, loss=0.004, v_num=6]\n",
            "Epoch 3:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.004, v_num=6]\n",
            "Epoch 3:  98% 247/251 [03:55<00:03,  1.05it/s, loss=0.004, v_num=6]\n",
            "Epoch 3:  99% 248/251 [03:55<00:02,  1.05it/s, loss=0.004, v_num=6]\n",
            "Epoch 3:  99% 249/251 [03:56<00:01,  1.05it/s, loss=0.004, v_num=6]\n",
            "Epoch 3: 100% 250/251 [03:56<00:00,  1.06it/s, loss=0.004, v_num=6]\n",
            "Epoch 3: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.004, v_num=6]val loss = 0.41 val metric = 0.912 \n",
            "\n",
            "Epoch 00003: val_metric reached 0.91211 (best 0.91211), saving model to /content/drive/My Drive/Models/WNUT-Task2/model16/4/epoch=3.ckpt as top 1\n",
            "Epoch 3: 100% 251/251 [03:58<00:00,  1.05it/s, loss=0.004, v_num=6]\n",
            "                                               \u001b[ATrain loss = 0.018 Train metric = 0.995\n",
            "Epoch 4:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.019, v_num=6]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 4:  88% 220/251 [03:45<00:31,  1.02s/it, loss=0.019, v_num=6]\n",
            "Epoch 4:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.019, v_num=6]\n",
            "Epoch 4:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.019, v_num=6]\n",
            "Epoch 4:  89% 223/251 [03:46<00:28,  1.01s/it, loss=0.019, v_num=6]\n",
            "Epoch 4:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.019, v_num=6]\n",
            "Epoch 4:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.019, v_num=6]\n",
            "Epoch 4:  90% 226/251 [03:47<00:25,  1.01s/it, loss=0.019, v_num=6]\n",
            "Epoch 4:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.019, v_num=6]\n",
            "Epoch 4:  91% 228/251 [03:48<00:23,  1.00s/it, loss=0.019, v_num=6]\n",
            "Epoch 4:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.019, v_num=6]\n",
            "Epoch 4:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.019, v_num=6]\n",
            "Epoch 4:  92% 231/251 [03:49<00:19,  1.01it/s, loss=0.019, v_num=6]\n",
            "Epoch 4:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.019, v_num=6]\n",
            "Epoch 4:  93% 233/251 [03:49<00:17,  1.01it/s, loss=0.019, v_num=6]\n",
            "Epoch 4:  93% 234/251 [03:50<00:16,  1.02it/s, loss=0.019, v_num=6]\n",
            "Epoch 4:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.019, v_num=6]\n",
            "Epoch 4:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.019, v_num=6]\n",
            "Epoch 4:  94% 237/251 [03:51<00:13,  1.03it/s, loss=0.019, v_num=6]\n",
            "Epoch 4:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.019, v_num=6]\n",
            "Epoch 4:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.019, v_num=6]\n",
            "Epoch 4:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.019, v_num=6]\n",
            "Epoch 4:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.019, v_num=6]\n",
            "Epoch 4:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.019, v_num=6]\n",
            "Epoch 4:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.019, v_num=6]\n",
            "Epoch 4:  97% 244/251 [03:53<00:06,  1.04it/s, loss=0.019, v_num=6]\n",
            "Epoch 4:  98% 245/251 [03:53<00:05,  1.05it/s, loss=0.019, v_num=6]\n",
            "Epoch 4:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.019, v_num=6]\n",
            "Epoch 4:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.019, v_num=6]\n",
            "Epoch 4:  99% 248/251 [03:55<00:02,  1.06it/s, loss=0.019, v_num=6]\n",
            "Epoch 4:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.019, v_num=6]\n",
            "Epoch 4: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.019, v_num=6]\n",
            "Epoch 4: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.019, v_num=6]val loss = 0.393 val metric = 0.909 \n",
            "\n",
            "Epoch 00004: val_metric  was not in top 1\n",
            "Epoch 4: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.019, v_num=6]\n",
            "                                               \u001b[ATrain loss = 0.016 Train metric = 0.995\n",
            "Epoch 5:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.043, v_num=6]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  88% 220/251 [03:45<00:31,  1.02s/it, loss=0.043, v_num=6]\n",
            "Epoch 5:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.043, v_num=6]\n",
            "Epoch 5:  88% 222/251 [03:46<00:29,  1.02s/it, loss=0.043, v_num=6]\n",
            "Epoch 5:  89% 223/251 [03:46<00:28,  1.02s/it, loss=0.043, v_num=6]\n",
            "Epoch 5:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.043, v_num=6]\n",
            "Epoch 5:  90% 225/251 [03:47<00:26,  1.01s/it, loss=0.043, v_num=6]\n",
            "Epoch 5:  90% 226/251 [03:47<00:25,  1.01s/it, loss=0.043, v_num=6]\n",
            "Epoch 5:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.043, v_num=6]\n",
            "Epoch 5:  91% 228/251 [03:48<00:23,  1.00s/it, loss=0.043, v_num=6]\n",
            "Epoch 5:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.043, v_num=6]\n",
            "Epoch 5:  92% 230/251 [03:48<00:20,  1.00it/s, loss=0.043, v_num=6]\n",
            "Epoch 5:  92% 231/251 [03:49<00:19,  1.01it/s, loss=0.043, v_num=6]\n",
            "Epoch 5:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.043, v_num=6]\n",
            "Epoch 5:  93% 233/251 [03:50<00:17,  1.01it/s, loss=0.043, v_num=6]\n",
            "Epoch 5:  93% 234/251 [03:50<00:16,  1.02it/s, loss=0.043, v_num=6]\n",
            "Epoch 5:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.043, v_num=6]\n",
            "Epoch 5:  94% 236/251 [03:51<00:14,  1.02it/s, loss=0.043, v_num=6]\n",
            "Epoch 5:  94% 237/251 [03:51<00:13,  1.02it/s, loss=0.043, v_num=6]\n",
            "Epoch 5:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.043, v_num=6]\n",
            "Epoch 5:  95% 239/251 [03:52<00:11,  1.03it/s, loss=0.043, v_num=6]\n",
            "Epoch 5:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.043, v_num=6]\n",
            "Epoch 5:  96% 241/251 [03:52<00:09,  1.03it/s, loss=0.043, v_num=6]\n",
            "Epoch 5:  96% 242/251 [03:53<00:08,  1.04it/s, loss=0.043, v_num=6]\n",
            "Epoch 5:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.043, v_num=6]\n",
            "Epoch 5:  97% 244/251 [03:53<00:06,  1.04it/s, loss=0.043, v_num=6]\n",
            "Epoch 5:  98% 245/251 [03:54<00:05,  1.05it/s, loss=0.043, v_num=6]\n",
            "Epoch 5:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.043, v_num=6]\n",
            "Epoch 5:  98% 247/251 [03:55<00:03,  1.05it/s, loss=0.043, v_num=6]\n",
            "Epoch 5:  99% 248/251 [03:55<00:02,  1.05it/s, loss=0.043, v_num=6]\n",
            "Epoch 5:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.043, v_num=6]\n",
            "Epoch 5: 100% 250/251 [03:56<00:00,  1.06it/s, loss=0.043, v_num=6]\n",
            "Epoch 5: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.043, v_num=6]val loss = 0.409 val metric = 0.888 \n",
            "\n",
            "Epoch 00005: val_metric  was not in top 1\n",
            "Epoch 5: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.043, v_num=6]\n",
            "                                               \u001b[ATrain loss = 0.015 Train metric = 0.995\n",
            "Epoch 6:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.002, v_num=6]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  88% 220/251 [03:45<00:31,  1.02s/it, loss=0.002, v_num=6]\n",
            "Epoch 6:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.002, v_num=6]\n",
            "Epoch 6:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.002, v_num=6]\n",
            "Epoch 6:  89% 223/251 [03:46<00:28,  1.01s/it, loss=0.002, v_num=6]\n",
            "Epoch 6:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.002, v_num=6]\n",
            "Epoch 6:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.002, v_num=6]\n",
            "Epoch 6:  90% 226/251 [03:47<00:25,  1.01s/it, loss=0.002, v_num=6]\n",
            "Epoch 6:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.002, v_num=6]\n",
            "Epoch 6:  91% 228/251 [03:48<00:23,  1.00s/it, loss=0.002, v_num=6]\n",
            "Epoch 6:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.002, v_num=6]\n",
            "Epoch 6:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.002, v_num=6]\n",
            "Epoch 6:  92% 231/251 [03:49<00:19,  1.01it/s, loss=0.002, v_num=6]\n",
            "Epoch 6:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.002, v_num=6]\n",
            "Epoch 6:  93% 233/251 [03:49<00:17,  1.01it/s, loss=0.002, v_num=6]\n",
            "Epoch 6:  93% 234/251 [03:50<00:16,  1.02it/s, loss=0.002, v_num=6]\n",
            "Epoch 6:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.002, v_num=6]\n",
            "Epoch 6:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.002, v_num=6]\n",
            "Epoch 6:  94% 237/251 [03:51<00:13,  1.03it/s, loss=0.002, v_num=6]\n",
            "Epoch 6:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.002, v_num=6]\n",
            "Epoch 6:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.002, v_num=6]\n",
            "Epoch 6:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.002, v_num=6]\n",
            "Epoch 6:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.002, v_num=6]\n",
            "Epoch 6:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.002, v_num=6]\n",
            "Epoch 6:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.002, v_num=6]\n",
            "Epoch 6:  97% 244/251 [03:53<00:06,  1.04it/s, loss=0.002, v_num=6]\n",
            "Epoch 6:  98% 245/251 [03:54<00:05,  1.05it/s, loss=0.002, v_num=6]\n",
            "Epoch 6:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.002, v_num=6]\n",
            "Epoch 6:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.002, v_num=6]\n",
            "Epoch 6:  99% 248/251 [03:55<00:02,  1.05it/s, loss=0.002, v_num=6]\n",
            "Epoch 6:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.002, v_num=6]\n",
            "Epoch 6: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.002, v_num=6]\n",
            "Epoch 6: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.002, v_num=6]val loss = 0.442 val metric = 0.903 \n",
            "\n",
            "Epoch 00006: val_metric  was not in top 1\n",
            "Epoch 6: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.002, v_num=6]\n",
            "                                               \u001b[ATrain loss = 0.017 Train metric = 0.995\n",
            "Epoch 7:  87% 219/251 [03:45<00:32,  1.03s/it, loss=0.002, v_num=6]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  88% 220/251 [03:45<00:31,  1.03s/it, loss=0.002, v_num=6]\n",
            "Epoch 7:  88% 221/251 [03:46<00:30,  1.02s/it, loss=0.002, v_num=6]\n",
            "Epoch 7:  88% 222/251 [03:46<00:29,  1.02s/it, loss=0.002, v_num=6]\n",
            "Epoch 7:  89% 223/251 [03:46<00:28,  1.02s/it, loss=0.002, v_num=6]\n",
            "Epoch 7:  89% 224/251 [03:47<00:27,  1.01s/it, loss=0.002, v_num=6]\n",
            "Epoch 7:  90% 225/251 [03:47<00:26,  1.01s/it, loss=0.002, v_num=6]\n",
            "Epoch 7:  90% 226/251 [03:47<00:25,  1.01s/it, loss=0.002, v_num=6]\n",
            "Epoch 7:  90% 227/251 [03:48<00:24,  1.01s/it, loss=0.002, v_num=6]\n",
            "Epoch 7:  91% 228/251 [03:48<00:23,  1.00s/it, loss=0.002, v_num=6]\n",
            "Epoch 7:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.002, v_num=6]\n",
            "Epoch 7:  92% 230/251 [03:49<00:20,  1.00it/s, loss=0.002, v_num=6]\n",
            "Epoch 7:  92% 231/251 [03:49<00:19,  1.01it/s, loss=0.002, v_num=6]\n",
            "Epoch 7:  92% 232/251 [03:50<00:18,  1.01it/s, loss=0.002, v_num=6]\n",
            "Epoch 7:  93% 233/251 [03:50<00:17,  1.01it/s, loss=0.002, v_num=6]\n",
            "Epoch 7:  93% 234/251 [03:50<00:16,  1.01it/s, loss=0.002, v_num=6]\n",
            "Epoch 7:  94% 235/251 [03:51<00:15,  1.02it/s, loss=0.002, v_num=6]\n",
            "Epoch 7:  94% 236/251 [03:51<00:14,  1.02it/s, loss=0.002, v_num=6]\n",
            "Epoch 7:  94% 237/251 [03:51<00:13,  1.02it/s, loss=0.002, v_num=6]\n",
            "Epoch 7:  95% 238/251 [03:52<00:12,  1.02it/s, loss=0.002, v_num=6]\n",
            "Epoch 7:  95% 239/251 [03:52<00:11,  1.03it/s, loss=0.002, v_num=6]\n",
            "Epoch 7:  96% 240/251 [03:52<00:10,  1.03it/s, loss=0.002, v_num=6]\n",
            "Epoch 7:  96% 241/251 [03:53<00:09,  1.03it/s, loss=0.002, v_num=6]\n",
            "Epoch 7:  96% 242/251 [03:53<00:08,  1.04it/s, loss=0.002, v_num=6]\n",
            "Epoch 7:  97% 243/251 [03:54<00:07,  1.04it/s, loss=0.002, v_num=6]\n",
            "Epoch 7:  97% 244/251 [03:54<00:06,  1.04it/s, loss=0.002, v_num=6]\n",
            "Epoch 7:  98% 245/251 [03:54<00:05,  1.04it/s, loss=0.002, v_num=6]\n",
            "Epoch 7:  98% 246/251 [03:55<00:04,  1.05it/s, loss=0.002, v_num=6]\n",
            "Epoch 7:  98% 247/251 [03:55<00:03,  1.05it/s, loss=0.002, v_num=6]\n",
            "Epoch 7:  99% 248/251 [03:55<00:02,  1.05it/s, loss=0.002, v_num=6]\n",
            "Epoch 7:  99% 249/251 [03:56<00:01,  1.05it/s, loss=0.002, v_num=6]\n",
            "Epoch 7: 100% 250/251 [03:56<00:00,  1.06it/s, loss=0.002, v_num=6]\n",
            "Epoch 7: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.002, v_num=6]val loss = 0.4 val metric = 0.908 \n",
            "\n",
            "Epoch 00007: val_metric  was not in top 1\n",
            "Epoch 7: 100% 251/251 [03:56<00:00,  1.06it/s, loss=0.002, v_num=6]\n",
            "                                               \u001b[ATrain loss = 0.014 Train metric = 0.997\n",
            "Epoch 8:  87% 219/251 [03:44<00:32,  1.03s/it, loss=0.010, v_num=6]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 8:  88% 220/251 [03:44<00:31,  1.02s/it, loss=0.010, v_num=6]\n",
            "Epoch 8:  88% 221/251 [03:45<00:30,  1.02s/it, loss=0.010, v_num=6]\n",
            "Epoch 8:  88% 222/251 [03:45<00:29,  1.02s/it, loss=0.010, v_num=6]\n",
            "Epoch 8:  89% 223/251 [03:46<00:28,  1.01s/it, loss=0.010, v_num=6]\n",
            "Epoch 8:  89% 224/251 [03:46<00:27,  1.01s/it, loss=0.010, v_num=6]\n",
            "Epoch 8:  90% 225/251 [03:46<00:26,  1.01s/it, loss=0.010, v_num=6]\n",
            "Epoch 8:  90% 226/251 [03:47<00:25,  1.00s/it, loss=0.010, v_num=6]\n",
            "Epoch 8:  90% 227/251 [03:47<00:24,  1.00s/it, loss=0.010, v_num=6]\n",
            "Epoch 8:  91% 228/251 [03:47<00:22,  1.00it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  91% 229/251 [03:48<00:21,  1.00it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  92% 230/251 [03:48<00:20,  1.01it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  92% 231/251 [03:48<00:19,  1.01it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  92% 232/251 [03:49<00:18,  1.01it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  93% 233/251 [03:49<00:17,  1.02it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  93% 234/251 [03:49<00:16,  1.02it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  94% 235/251 [03:50<00:15,  1.02it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  94% 236/251 [03:50<00:14,  1.02it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  94% 237/251 [03:50<00:13,  1.03it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  95% 238/251 [03:51<00:12,  1.03it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  95% 239/251 [03:51<00:11,  1.03it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  96% 240/251 [03:51<00:10,  1.03it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  96% 241/251 [03:52<00:09,  1.04it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  96% 242/251 [03:52<00:08,  1.04it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  97% 243/251 [03:53<00:07,  1.04it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  97% 244/251 [03:53<00:06,  1.05it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  98% 245/251 [03:53<00:05,  1.05it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  98% 246/251 [03:54<00:04,  1.05it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  98% 247/251 [03:54<00:03,  1.05it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  99% 248/251 [03:54<00:02,  1.06it/s, loss=0.010, v_num=6]\n",
            "Epoch 8:  99% 249/251 [03:55<00:01,  1.06it/s, loss=0.010, v_num=6]\n",
            "Epoch 8: 100% 250/251 [03:55<00:00,  1.06it/s, loss=0.010, v_num=6]\n",
            "Epoch 8: 100% 251/251 [03:55<00:00,  1.07it/s, loss=0.010, v_num=6]val loss = 0.394 val metric = 0.907 \n",
            "\n",
            "Epoch 00008: val_metric  was not in top 1\n",
            "Epoch 8: 100% 251/251 [03:55<00:00,  1.07it/s, loss=0.010, v_num=6]\n",
            "                                               \u001b[ATrain loss = 0.006 Train metric = 0.998\n",
            "Saving latest checkpoint..\n",
            "Epoch 8: 100% 251/251 [03:55<00:00,  1.06it/s, loss=0.010, v_num=6]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0, mixout prob 0.8, ...\n",
            "f1                                                       0.89916\n",
            "precision                                               0.891667\n",
            "recall                                                   0.90678\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDrZ-SMmfPah",
        "colab_type": "text"
      },
      "source": [
        "#### L2 regularization - .001, .005, .02, .1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNyJkZwhqWAF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fa0da989-53b2-4839-850a-132f921cac04"
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model17/1/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --model_save_path {model_dir} --l2 .001\n",
        "\n",
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model17/2/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --model_save_path {model_dir} --l2 .005\n",
        "\n",
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model17/3/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --model_save_path {model_dir} --l2 .02"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-08-31 15:18:08.524848: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "Tokenization\n",
            "100% 7000/7000 [00:03<00:00, 2144.83it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2435.86it/s]\n",
            "Train and val loader length 219 and 32\n",
            "Modelling\n",
            "Transformer(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (drop): Dropout(p=0, inplace=False)\n",
            "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory /content/drive/My Drive/Models/WNUT-Task2/model17/1/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "Device: cuda\n",
            "[LOG] Total number of parameters to learn 124646401\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200831_151825-28pvlrew\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstilted-galaxy-13\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization/runs/28pvlrew\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "  0%|                                                   | 0/219 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Current training Loss 0.081: 100%|████████████| 219/219 [01:08<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.027: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.161: 100%|██████████████████| 32/32 [00:03<00:00, 10.02it/s]\n",
            "Train loss = 0.065 Train metric = 0.984 Val loss = 0.376 Val metric = 0.904\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model17/1/\n",
            "Current training Loss 0.019: 100%|████████████| 219/219 [01:08<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.017: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.125: 100%|██████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.04 Train metric = 0.991 Val loss = 0.417 Val metric = 0.887\n",
            "Current training Loss 0.02: 100%|█████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.014: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.094: 100%|██████████████████| 32/32 [00:03<00:00, 10.03it/s]\n",
            "Train loss = 0.026 Train metric = 0.997 Val loss = 0.565 Val metric = 0.884\n",
            "Current training Loss 0.014: 100%|████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.014: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.093: 100%|██████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.017 Train metric = 0.999 Val loss = 0.547 Val metric = 0.891\n",
            "Current training Loss 0.014: 100%|████████████| 219/219 [01:08<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.014: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.093: 100%|██████████████████| 32/32 [00:03<00:00, 10.06it/s]\n",
            "Train loss = 0.021 Train metric = 0.998 Val loss = 0.566 Val metric = 0.88\n",
            "Current training Loss 0.014: 100%|████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.014: 100%|████████████████| 219/219 [00:22<00:00,  9.74it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.093: 100%|██████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.015 Train metric = 1.0 Val loss = 0.536 Val metric = 0.903\n",
            "Early stopping\n",
            "100%|███████████████████████████████████████████| 32/32 [00:03<00:00, 10.24it/s]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0, mixout prob 0, mu...\n",
            "f1                                                      0.903766\n",
            "precision                                               0.892562\n",
            "recall                                                  0.915254\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 5015\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_metric 0.9029535864978903\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                     _step 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  _runtime 586.9962215423584\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                train_loss 0.014567841775715351\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _timestamp 1598887674.862861\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  val_loss 0.5361558198928833\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_metric 0.9996972449288526\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced stilted-galaxy-13: https://app.wandb.ai/victor7246/wnut-task2-regularization/runs/28pvlrew\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-08-31 15:28:03.617594: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "Tokenization\n",
            "100% 7000/7000 [00:03<00:00, 2166.86it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2572.61it/s]\n",
            "Train and val loader length 219 and 32\n",
            "Modelling\n",
            "Transformer(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (drop): Dropout(p=0, inplace=False)\n",
            "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory /content/drive/My Drive/Models/WNUT-Task2/model17/2/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "Device: cuda\n",
            "[LOG] Total number of parameters to learn 124646401\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200831_152820-18hqk41t\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwoven-frog-14\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization/runs/18hqk41t\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "  0%|                                                   | 0/219 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Current training Loss 0.165: 100%|████████████| 219/219 [01:08<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.084: 100%|████████████████| 219/219 [00:22<00:00,  9.81it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.506: 100%|██████████████████| 32/32 [00:03<00:00, 10.00it/s]\n",
            "Train loss = 0.118 Train metric = 0.984 Val loss = 0.732 Val metric = 0.889\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model17/2/\n",
            "Current training Loss 0.069: 100%|████████████| 219/219 [01:08<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.069: 100%|████████████████| 219/219 [00:22<00:00,  9.81it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.535: 100%|██████████████████| 32/32 [00:03<00:00, 10.02it/s]\n",
            "Train loss = 0.083 Train metric = 0.995 Val loss = 0.8 Val metric = 0.891\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model17/2/\n",
            "Current training Loss 0.068: 100%|████████████| 219/219 [01:08<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.068: 100%|████████████████| 219/219 [00:22<00:00,  9.81it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.461: 100%|██████████████████| 32/32 [00:03<00:00, 10.02it/s]\n",
            "Train loss = 0.075 Train metric = 0.997 Val loss = 0.852 Val metric = 0.888\n",
            "Current training Loss 0.068: 100%|████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.068: 100%|████████████████| 219/219 [00:22<00:00,  9.82it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.547: 100%|██████████████████| 32/32 [00:03<00:00, 10.03it/s]\n",
            "Train loss = 0.073 Train metric = 0.998 Val loss = 0.888 Val metric = 0.903\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model17/2/\n",
            "Current training Loss 0.068: 100%|████████████| 219/219 [01:09<00:00,  3.16it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.067: 100%|████████████████| 219/219 [00:22<00:00,  9.81it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.478: 100%|██████████████████| 32/32 [00:03<00:00, 10.03it/s]\n",
            "Train loss = 0.07 Train metric = 0.999 Val loss = 0.871 Val metric = 0.893\n",
            "Current training Loss 0.069: 100%|████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.068: 100%|████████████████| 219/219 [00:22<00:00,  9.73it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.481: 100%|██████████████████| 32/32 [00:03<00:00, 10.03it/s]\n",
            "Train loss = 0.07 Train metric = 0.999 Val loss = 0.863 Val metric = 0.889\n",
            "Current training Loss 0.068: 100%|████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.068: 100%|████████████████| 219/219 [00:22<00:00,  9.82it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.465: 100%|██████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.068 Train metric = 1.0 Val loss = 0.937 Val metric = 0.886\n",
            "Current training Loss 0.067: 100%|████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.067: 100%|████████████████| 219/219 [00:22<00:00,  9.82it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.465: 100%|██████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.068 Train metric = 1.0 Val loss = 0.997 Val metric = 0.893\n",
            "Current training Loss 0.067: 100%|████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.067: 100%|████████████████| 219/219 [00:22<00:00,  9.82it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.467: 100%|██████████████████| 32/32 [00:03<00:00, 10.02it/s]\n",
            "Train loss = 0.068 Train metric = 1.0 Val loss = 1.021 Val metric = 0.896\n",
            "Early stopping\n",
            "100%|███████████████████████████████████████████| 32/32 [00:03<00:00, 10.22it/s]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0, mixout prob 0, mu...\n",
            "f1                                                      0.902821\n",
            "precision                                               0.890722\n",
            "recall                                                  0.915254\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 5280\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                     _step 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_metric 0.9996971532404603\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_metric 0.8958990536277602\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                train_loss 0.06772541254758835\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  val_loss 1.021141529083252\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  _runtime 875.2545685768127\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _timestamp 1598888558.2627578\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced woven-frog-14: https://app.wandb.ai/victor7246/wnut-task2-regularization/runs/18hqk41t\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-08-31 15:42:47.471009: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "Tokenization\n",
            "100% 7000/7000 [00:03<00:00, 2109.73it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2388.76it/s]\n",
            "Train and val loader length 219 and 32\n",
            "Modelling\n",
            "Transformer(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (drop): Dropout(p=0, inplace=False)\n",
            "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory /content/drive/My Drive/Models/WNUT-Task2/model17/3/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "Device: cuda\n",
            "[LOG] Total number of parameters to learn 124646401\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200831_154304-1bph42zn\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlemon-galaxy-15\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization/runs/1bph42zn\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "  0%|                                                   | 0/219 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Current training Loss 0.399: 100%|████████████| 219/219 [01:08<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.289: 100%|████████████████| 219/219 [00:22<00:00,  9.82it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.852: 100%|██████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.324 Train metric = 0.982 Val loss = 2.107 Val metric = 0.884\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model17/3/\n",
            "Current training Loss 0.271: 100%|████████████| 219/219 [01:08<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.271: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.879: 100%|██████████████████| 32/32 [00:03<00:00, 10.05it/s]\n",
            "Train loss = 0.283 Train metric = 0.997 Val loss = 2.175 Val metric = 0.889\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model17/3/\n",
            "Current training Loss 0.27: 100%|█████████████| 219/219 [01:08<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.27: 100%|█████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.842: 100%|██████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.279 Train metric = 0.997 Val loss = 2.285 Val metric = 0.9\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model17/3/\n",
            "Current training Loss 0.269: 100%|████████████| 219/219 [01:08<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.269: 100%|████████████████| 219/219 [00:22<00:00,  9.84it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.841: 100%|██████████████████| 32/32 [00:03<00:00, 10.08it/s]\n",
            "Train loss = 0.274 Train metric = 0.999 Val loss = 2.347 Val metric = 0.891\n",
            "Current training Loss 0.284: 100%|████████████| 219/219 [01:09<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.27: 100%|█████████████████| 219/219 [00:22<00:00,  9.84it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.84: 100%|███████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.309 Train metric = 0.986 Val loss = 2.356 Val metric = 0.868\n",
            "Current training Loss 0.269: 100%|████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.269: 100%|████████████████| 219/219 [00:22<00:00,  9.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.868: 100%|██████████████████| 32/32 [00:03<00:00, 10.03it/s]\n",
            "Train loss = 0.273 Train metric = 0.999 Val loss = 2.196 Val metric = 0.905\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model17/3/\n",
            "Current training Loss 0.269: 100%|████████████| 219/219 [01:08<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.269: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.849: 100%|██████████████████| 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.27 Train metric = 1.0 Val loss = 2.321 Val metric = 0.905\n",
            "Current training Loss 0.269: 100%|████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.269: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.839: 100%|██████████████████| 32/32 [00:03<00:00, 10.03it/s]\n",
            "Train loss = 0.273 Train metric = 0.998 Val loss = 2.334 Val metric = 0.911\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model17/3/\n",
            "Current training Loss 0.269: 100%|████████████| 219/219 [01:08<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.269: 100%|████████████████| 219/219 [00:22<00:00,  9.89it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.839: 100%|██████████████████| 32/32 [00:03<00:00, 10.10it/s]\n",
            "Train loss = 0.27 Train metric = 1.0 Val loss = 2.32 Val metric = 0.895\n",
            "Current training Loss 0.269: 100%|████████████| 219/219 [01:08<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.269: 100%|████████████████| 219/219 [00:22<00:00,  9.89it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.838: 100%|██████████████████| 32/32 [00:03<00:00, 10.07it/s]\n",
            "Train loss = 0.269 Train metric = 1.0 Val loss = 2.334 Val metric = 0.903\n",
            "Current training Loss 0.269: 100%|████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.269: 100%|████████████████| 219/219 [00:22<00:00,  9.90it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.838: 100%|██████████████████| 32/32 [00:03<00:00, 10.11it/s]\n",
            "Train loss = 0.269 Train metric = 1.0 Val loss = 2.384 Val metric = 0.906\n",
            "Current training Loss 0.269: 100%|████████████| 219/219 [01:08<00:00,  3.21it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.269: 100%|████████████████| 219/219 [00:22<00:00,  9.90it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.838: 100%|██████████████████| 32/32 [00:03<00:00, 10.11it/s]\n",
            "Train loss = 0.269 Train metric = 1.0 Val loss = 2.397 Val metric = 0.906\n",
            "Current training Loss 0.269: 100%|████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.269: 100%|████████████████| 219/219 [00:22<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.838: 100%|██████████████████| 32/32 [00:03<00:00, 10.12it/s]\n",
            "Train loss = 0.269 Train metric = 1.0 Val loss = 2.416 Val metric = 0.907\n",
            "Early stopping\n",
            "100%|███████████████████████████████████████████| 32/32 [00:03<00:00, 10.32it/s]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0, mixout prob 0, mu...\n",
            "f1                                                      0.910849\n",
            "precision                                               0.923747\n",
            "recall                                                  0.898305\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 5550\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _timestamp 1598889823.7532501\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                     _step 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  _runtime 1256.9087958335876\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  val_loss 2.4161174297332764\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_metric 0.9070247933884298\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                train_loss 0.2687602639198303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_metric 0.9998486453761163\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced lemon-galaxy-15: https://app.wandb.ai/victor7246/wnut-task2-regularization/runs/1bph42zn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZDaohmuPr6H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "07ebd77e-74eb-42dc-e1e1-fade3bf2eb61"
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model17/4/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --model_save_path {model_dir} --l2 .1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-08-31 16:05:12.247221: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "Tokenization\n",
            "100% 7000/7000 [00:03<00:00, 2244.95it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2824.74it/s]\n",
            "Train and val loader length 219 and 32\n",
            "Modelling\n",
            "Transformer(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (drop): Dropout(p=0, inplace=False)\n",
            "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory /content/drive/My Drive/Models/WNUT-Task2/model17/4/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "Device: cuda\n",
            "[LOG] Total number of parameters to learn 124646401\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200831_160528-19v3rnqc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlilac-wildflower-16\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization/runs/19v3rnqc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "  0%|                                                   | 0/219 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Current training Loss 1.431: 100%|████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 1.36: 100%|█████████████████| 219/219 [00:22<00:00,  9.88it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 9.25: 100%|███████████████████| 32/32 [00:03<00:00, 10.07it/s]\n",
            "Train loss = 1.396 Train metric = 0.984 Val loss = 9.487 Val metric = 0.893\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model17/4/\n",
            "Current training Loss 1.346: 100%|████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 1.346: 100%|████████████████| 219/219 [00:22<00:00,  9.88it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 9.268: 100%|██████████████████| 32/32 [00:03<00:00, 10.09it/s]\n",
            "Train loss = 1.359 Train metric = 0.996 Val loss = 9.519 Val metric = 0.896\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model17/4/\n",
            "Current training Loss 1.35: 100%|█████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 1.343: 100%|████████████████| 219/219 [00:22<00:00,  9.87it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 9.189: 100%|██████████████████| 32/32 [00:03<00:00, 10.09it/s]\n",
            "Train loss = 1.353 Train metric = 0.997 Val loss = 9.614 Val metric = 0.892\n",
            "Current training Loss 1.342: 100%|████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 1.342: 100%|████████████████| 219/219 [00:22<00:00,  9.88it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 9.247: 100%|██████████████████| 32/32 [00:03<00:00, 10.10it/s]\n",
            "Train loss = 1.349 Train metric = 0.997 Val loss = 9.681 Val metric = 0.901\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model17/4/\n",
            "Current training Loss 1.341: 100%|████████████| 219/219 [01:08<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 1.341: 100%|████████████████| 219/219 [00:22<00:00,  9.87it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 9.203: 100%|██████████████████| 32/32 [00:03<00:00, 10.08it/s]\n",
            "Train loss = 1.347 Train metric = 0.997 Val loss = 9.648 Val metric = 0.893\n",
            "Current training Loss 1.34: 100%|█████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 1.339: 100%|████████████████| 219/219 [00:22<00:00,  9.80it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 9.166: 100%|██████████████████| 32/32 [00:03<00:00, 10.07it/s]\n",
            "Train loss = 1.345 Train metric = 0.998 Val loss = 9.669 Val metric = 0.894\n",
            "Current training Loss 1.339: 100%|████████████| 219/219 [01:08<00:00,  3.21it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 1.338: 100%|████████████████| 219/219 [00:22<00:00,  9.88it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 9.159: 100%|██████████████████| 32/32 [00:03<00:00, 10.09it/s]\n",
            "Train loss = 1.342 Train metric = 0.998 Val loss = 9.686 Val metric = 0.886\n",
            "Current training Loss 1.338: 100%|████████████| 219/219 [01:08<00:00,  3.21it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 1.337: 100%|████████████████| 219/219 [00:22<00:00,  9.88it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 9.154: 100%|██████████████████| 32/32 [00:03<00:00, 10.08it/s]\n",
            "Train loss = 1.338 Train metric = 1.0 Val loss = 9.723 Val metric = 0.903\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model17/4/\n",
            "Current training Loss 1.337: 100%|████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 1.337: 100%|████████████████| 219/219 [00:22<00:00,  9.87it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 9.15: 100%|███████████████████| 32/32 [00:03<00:00, 10.08it/s]\n",
            "Train loss = 1.338 Train metric = 0.999 Val loss = 9.645 Val metric = 0.901\n",
            "Current training Loss 1.336: 100%|████████████| 219/219 [01:08<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 1.336: 100%|████████████████| 219/219 [00:22<00:00,  9.88it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 9.153: 100%|██████████████████| 32/32 [00:03<00:00, 10.09it/s]\n",
            "Train loss = 1.336 Train metric = 1.0 Val loss = 9.69 Val metric = 0.907\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model17/4/\n",
            "Current training Loss 1.335: 100%|████████████| 219/219 [01:08<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 1.335: 100%|████████████████| 219/219 [00:22<00:00,  9.88it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 9.14: 100%|███████████████████| 32/32 [00:03<00:00, 10.08it/s]\n",
            "Train loss = 1.336 Train metric = 1.0 Val loss = 9.694 Val metric = 0.907\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model17/4/\n",
            "Current training Loss 1.335: 100%|████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 1.335: 100%|████████████████| 219/219 [00:22<00:00,  9.87it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 9.136: 100%|██████████████████| 32/32 [00:03<00:00, 10.10it/s]\n",
            "Train loss = 1.335 Train metric = 1.0 Val loss = 9.7 Val metric = 0.906\n",
            "Current training Loss 1.334: 100%|████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 1.334: 100%|████████████████| 219/219 [00:22<00:00,  9.81it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 9.133: 100%|██████████████████| 32/32 [00:03<00:00, 10.07it/s]\n",
            "Train loss = 1.335 Train metric = 1.0 Val loss = 9.707 Val metric = 0.906\n",
            "Current training Loss 1.334: 100%|████████████| 219/219 [01:08<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 1.334: 100%|████████████████| 219/219 [00:22<00:00,  9.88it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 9.132: 100%|██████████████████| 32/32 [00:03<00:00, 10.08it/s]\n",
            "Train loss = 1.335 Train metric = 1.0 Val loss = 9.707 Val metric = 0.907\n",
            "Current training Loss 1.334: 100%|████████████| 219/219 [01:08<00:00,  3.20it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 1.334: 100%|████████████████| 219/219 [00:22<00:00,  9.88it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 9.131: 100%|██████████████████| 32/32 [00:03<00:00, 10.08it/s]\n",
            "Train loss = 1.334 Train metric = 1.0 Val loss = 9.708 Val metric = 0.906\n",
            "100%|███████████████████████████████████████████| 32/32 [00:03<00:00, 10.30it/s]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0, mixout prob 0, mu...\n",
            "f1                                                      0.907001\n",
            "precision                                               0.894845\n",
            "recall                                                  0.919492\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 5995\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_metric 0.9058577405857741\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  _runtime 1444.0288863182068\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                     _step 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                train_loss 1.3344969749450684\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _timestamp 1598891355.6879766\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_metric 0.9996971532404603\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  val_loss 9.708072662353516\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced lilac-wildflower-16: https://app.wandb.ai/victor7246/wnut-task2-regularization/runs/19v3rnqc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRHHRbFAfe5-",
        "colab_type": "text"
      },
      "source": [
        "#### Multi-Sample dropout - 3, 5, 7 with dropout .1, .2 and .3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blEHb1QWqzG2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b0fb0a4-dc81-4885-ee6e-4e17bc1c24c8"
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model18/1/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --model_save_path {model_dir} --dropout .1 --multi_sample_dropout_count 3\n",
        "\n",
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model18/2/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --model_save_path {model_dir} --dropout .1 --multi_sample_dropout_count 5\n",
        "\n",
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model18/3/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --model_save_path {model_dir} --dropout .1 --multi_sample_dropout_count 7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-09-01 05:53:00.606932: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "Tokenization\n",
            "Downloading: 100% 481/481 [00:00<00:00, 346kB/s]\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 978kB/s]\n",
            "Downloading: 100% 456k/456k [00:01<00:00, 410kB/s]\n",
            "100% 7000/7000 [00:04<00:00, 1648.88it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1940.21it/s]\n",
            "Train and val loader length 219 and 32\n",
            "Modelling\n",
            "Device: cuda\n",
            "Downloading: 100% 501M/501M [00:41<00:00, 12.0MB/s]\n",
            "TransformerMultiSample(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (final_out): Linear(in_features=3, out_features=1, bias=False)\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory /content/drive/My Drive/Models/WNUT-Task2/model18/1/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "[LOG] Total number of parameters to learn 124645635\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200901_055421-1ozk38ow\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33methereal-planet-20\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization/runs/1ozk38ow\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "  0%|                                                   | 0/219 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Current training Loss 0.203: 100%|████████████| 219/219 [03:37<00:00,  1.01it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.075: 100%|████████████████| 219/219 [01:18<00:00,  2.78it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.092: 100%|██████████████████| 32/32 [00:11<00:00,  2.84it/s]\n",
            "Train loss = 0.078 Train metric = 0.974 Val loss = 0.236 Val metric = 0.891\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/1/\n",
            "Current training Loss 0.019: 100%|████████████| 219/219 [03:40<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.006: 100%|████████████████| 219/219 [01:16<00:00,  2.85it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.174: 100%|██████████████████| 32/32 [00:10<00:00,  2.92it/s]\n",
            "Train loss = 0.064 Train metric = 0.975 Val loss = 0.373 Val metric = 0.851\n",
            "Current training Loss 0.02: 100%|█████████████| 219/219 [03:39<00:00,  1.06it/s]Running evaluation on whole training data\n",
            "Current training Loss 0.02: 100%|█████████████| 219/219 [03:39<00:00,  1.00s/it]\n",
            "Current eval Loss 0.083: 100%|████████████████| 219/219 [01:18<00:00,  2.81it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.182: 100%|██████████████████| 32/32 [00:11<00:00,  2.86it/s]\n",
            "Train loss = 0.025 Train metric = 0.991 Val loss = 0.387 Val metric = 0.89\n",
            "Current training Loss 0.005: 100%|████████████| 219/219 [03:39<00:00,  1.00s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.002: 100%|████████████████| 219/219 [01:17<00:00,  2.81it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.005: 100%|██████████████████| 32/32 [00:11<00:00,  2.87it/s]\n",
            "Train loss = 0.016 Train metric = 0.995 Val loss = 0.428 Val metric = 0.9\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/1/\n",
            "Current training Loss 0.002: 100%|████████████| 219/219 [03:39<00:00,  1.06it/s]Running evaluation on whole training data\n",
            "Current training Loss 0.002: 100%|████████████| 219/219 [03:39<00:00,  1.00s/it]\n",
            "Current eval Loss 0.001: 100%|████████████████| 219/219 [01:17<00:00,  2.82it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.48: 100%|███████████████████| 32/32 [00:11<00:00,  2.88it/s]\n",
            "Train loss = 0.025 Train metric = 0.991 Val loss = 0.554 Val metric = 0.888\n",
            "Current training Loss 0.002: 100%|████████████| 219/219 [03:38<00:00,  1.00it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100%|████████████████| 219/219 [01:17<00:00,  2.81it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.035: 100%|██████████████████| 32/32 [00:11<00:00,  2.86it/s]\n",
            "Train loss = 0.018 Train metric = 0.993 Val loss = 0.52 Val metric = 0.89\n",
            "Current training Loss 0.002: 100%|████████████| 219/219 [03:39<00:00,  1.06it/s]Running evaluation on whole training data\n",
            "Current training Loss 0.002: 100%|████████████| 219/219 [03:39<00:00,  1.00s/it]\n",
            "Current eval Loss 0.001: 100%|████████████████| 219/219 [01:17<00:00,  2.81it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.002: 100%|██████████████████| 32/32 [00:11<00:00,  2.88it/s]\n",
            "Train loss = 0.007 Train metric = 0.998 Val loss = 0.405 Val metric = 0.905\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/1/\n",
            "Current training Loss 0.002: 100%|████████████| 219/219 [03:39<00:00,  1.00s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100%|████████████████| 219/219 [01:15<00:00,  2.88it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.001: 100%|██████████████████| 32/32 [00:10<00:00,  2.94it/s]\n",
            "Train loss = 0.002 Train metric = 1.0 Val loss = 0.404 Val metric = 0.902\n",
            "Current training Loss 0.002: 100%|████████████| 219/219 [03:39<00:00,  1.00s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100%|████████████████| 219/219 [01:17<00:00,  2.81it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.001: 100%|██████████████████| 32/32 [00:11<00:00,  2.87it/s]\n",
            "Train loss = 0.002 Train metric = 1.0 Val loss = 0.409 Val metric = 0.897\n",
            "Current training Loss 0.003: 100%|████████████| 219/219 [03:39<00:00,  1.07it/s]Running evaluation on whole training data\n",
            "Current training Loss 0.003: 100%|████████████| 219/219 [03:39<00:00,  1.00s/it]\n",
            "Current eval Loss 0.001: 100%|████████████████| 219/219 [01:17<00:00,  2.82it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.001: 100%|██████████████████| 32/32 [00:11<00:00,  2.87it/s]\n",
            "Train loss = 0.001 Train metric = 1.0 Val loss = 0.441 Val metric = 0.898\n",
            "Current training Loss 0.002: 100%|████████████| 219/219 [03:39<00:00,  1.00s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100%|████████████████| 219/219 [01:17<00:00,  2.81it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.001: 100%|██████████████████| 32/32 [00:11<00:00,  2.86it/s]\n",
            "Train loss = 0.001 Train metric = 1.0 Val loss = 0.405 Val metric = 0.9\n",
            "Current training Loss 0.001: 100%|███████████▉| 218/219 [03:37<00:01,  1.00s/it]Running evaluation on whole training data\n",
            "Current training Loss 0.001: 100%|████████████| 219/219 [03:38<00:00,  1.00it/s]\n",
            "Current eval Loss 0.001: 100%|████████████████| 219/219 [01:17<00:00,  2.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.001: 100%|██████████████████| 32/32 [00:11<00:00,  2.87it/s]\n",
            "Train loss = 0.001 Train metric = 1.0 Val loss = 0.455 Val metric = 0.903\n",
            "Early stopping\n",
            "100%|███████████████████████████████████████████| 32/32 [00:10<00:00,  2.96it/s]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0.1, mixout prob 0, ...\n",
            "f1                                                      0.904905\n",
            "precision                                               0.857685\n",
            "recall                                                  0.957627\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 726\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_metric 0.9031600407747197\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  _runtime 3788.757520675659\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                     _step 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  val_loss 0.4547903537750244\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_metric 0.9998486453761163\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                train_loss 0.0010636288207024336\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _timestamp 1598943368.572247\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced ethereal-planet-20: https://app.wandb.ai/victor7246/wnut-task2-regularization/runs/1ozk38ow\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-09-01 06:56:25.623569: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "Tokenization\n",
            "100% 7000/7000 [00:04<00:00, 1658.17it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1940.60it/s]\n",
            "Train and val loader length 219 and 32\n",
            "Modelling\n",
            "Device: cuda\n",
            "TransformerMultiSample(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (final_out): Linear(in_features=5, out_features=1, bias=False)\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory /content/drive/My Drive/Models/WNUT-Task2/model18/2/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "[LOG] Total number of parameters to learn 124645637\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200901_065647-2u7ydvwq\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfiery-music-21\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization/runs/2u7ydvwq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "  0%|                                                   | 0/219 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Current training Loss 0.069: 100%|████████████| 219/219 [03:40<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.049: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.078: 100%|██████████████████| 32/32 [00:11<00:00,  2.82it/s]\n",
            "Train loss = 0.075 Train metric = 0.976 Val loss = 0.265 Val metric = 0.894\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/2/\n",
            "Current training Loss 0.128: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]Running evaluation on whole training data\n",
            "\n",
            "Current eval Loss 0.013: 100%|████████████████| 219/219 [01:18<00:00,  2.80it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.017: 100%|██████████████████| 32/32 [00:11<00:00,  2.86it/s]\n",
            "Train loss = 0.04 Train metric = 0.987 Val loss = 0.301 Val metric = 0.897\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/2/\n",
            "Current training Loss 0.005: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "  0%|                                                   | 0/219 [00:00<?, ?it/s]Running evaluation on whole training data\n",
            "Current eval Loss 0.003: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.062: 100%|██████████████████| 32/32 [00:11<00:00,  2.81it/s]\n",
            "Train loss = 0.017 Train metric = 0.995 Val loss = 0.39 Val metric = 0.895\n",
            "Current training Loss 0.003: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.045: 100%|████████████████| 219/219 [01:17<00:00,  2.81it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.231: 100%|██████████████████| 32/32 [00:11<00:00,  2.87it/s]\n",
            "Train loss = 0.015 Train metric = 0.995 Val loss = 0.4 Val metric = 0.906\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/2/\n",
            "Current training Loss 0.097: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.003: 100%|██████████████████| 32/32 [00:11<00:00,  2.82it/s]\n",
            "Train loss = 0.008 Train metric = 0.998 Val loss = 0.43 Val metric = 0.904\n",
            "Current training Loss 0.002: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100%|████████████████| 219/219 [01:18<00:00,  2.80it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.025: 100%|██████████████████| 32/32 [00:11<00:00,  2.82it/s]\n",
            "Train loss = 0.015 Train metric = 0.995 Val loss = 0.516 Val metric = 0.898\n",
            "Current training Loss 0.001: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.0: 100%|████████████████████| 32/32 [00:11<00:00,  2.83it/s]\n",
            "Train loss = 0.007 Train metric = 0.998 Val loss = 0.438 Val metric = 0.899\n",
            "Current training Loss 0.001: 100%|████████████| 219/219 [03:40<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [01:18<00:00,  2.80it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.0: 100%|████████████████████| 32/32 [00:11<00:00,  2.85it/s]\n",
            "Train loss = 0.003 Train metric = 0.999 Val loss = 0.47 Val metric = 0.904\n",
            "Current training Loss 0.001: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100%|██████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.0: 100%|████████████████████| 32/32 [00:11<00:00,  2.81it/s]\n",
            "Train loss = 0.004 Train metric = 0.999 Val loss = 0.474 Val metric = 0.898\n",
            "Early stopping\n",
            "100%|███████████████████████████████████████████| 32/32 [00:10<00:00,  2.93it/s]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0.1, mixout prob 0, ...\n",
            "f1                                                      0.906155\n",
            "precision                                               0.865125\n",
            "recall                                                  0.951271\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1261\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                     _step 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_metric 0.999242539009241\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  _runtime 2838.632424354553\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_metric 0.8976545842217484\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  val_loss 0.47358372807502747\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                train_loss 0.0038646110333502293\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _timestamp 1598946223.4740875\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced fiery-music-21: https://app.wandb.ai/victor7246/wnut-task2-regularization/runs/2u7ydvwq\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-09-01 07:44:02.220566: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "Tokenization\n",
            "100% 7000/7000 [00:04<00:00, 1651.31it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1955.26it/s]\n",
            "Train and val loader length 219 and 32\n",
            "Modelling\n",
            "Device: cuda\n",
            "TransformerMultiSample(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (final_out): Linear(in_features=7, out_features=1, bias=False)\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory /content/drive/My Drive/Models/WNUT-Task2/model18/3/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "[LOG] Total number of parameters to learn 124645639\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200901_074424-19ycvm5v\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mclassic-resonance-22\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization/runs/19ycvm5v\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "  0%|                                                   | 0/219 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Current training Loss 0.059: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.055: 100%|████████████████| 219/219 [01:19<00:00,  2.74it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.116: 100%|██████████████████| 32/32 [00:11<00:00,  2.80it/s]\n",
            "Train loss = 0.082 Train metric = 0.972 Val loss = 0.265 Val metric = 0.888\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/3/\n",
            "Current training Loss 0.014: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.008: 100%|████████████████| 219/219 [01:16<00:00,  2.85it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.13: 100%|███████████████████| 32/32 [00:10<00:00,  2.92it/s]\n",
            "Train loss = 0.038 Train metric = 0.987 Val loss = 0.298 Val metric = 0.898\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/3/\n",
            "Current training Loss 0.01: 100%|█████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.004: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.043: 100%|██████████████████| 32/32 [00:11<00:00,  2.81it/s]\n",
            "Train loss = 0.022 Train metric = 0.994 Val loss = 0.306 Val metric = 0.909\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/3/\n",
            "Current training Loss 0.008: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.003: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.014: 100%|██████████████████| 32/32 [00:11<00:00,  2.81it/s]\n",
            "Train loss = 0.023 Train metric = 0.994 Val loss = 0.377 Val metric = 0.912\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/3/\n",
            "Current training Loss 0.012: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.003: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.013: 100%|██████████████████| 32/32 [00:11<00:00,  2.80it/s]\n",
            "Train loss = 0.008 Train metric = 0.998 Val loss = 0.418 Val metric = 0.892\n",
            "Current training Loss 0.003: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.002: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.018: 100%|██████████████████| 32/32 [00:11<00:00,  2.77it/s]\n",
            "Train loss = 0.011 Train metric = 0.997 Val loss = 0.446 Val metric = 0.9\n",
            "Current training Loss 0.019: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.002: 100%|████████████████| 219/219 [01:19<00:00,  2.74it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.003: 100%|██████████████████| 32/32 [00:11<00:00,  2.80it/s]\n",
            "Train loss = 0.011 Train metric = 0.997 Val loss = 0.428 Val metric = 0.906\n",
            "Current training Loss 0.003: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.002: 100%|████████████████| 219/219 [01:16<00:00,  2.86it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.042: 100%|██████████████████| 32/32 [00:10<00:00,  2.92it/s]\n",
            "Train loss = 0.003 Train metric = 1.0 Val loss = 0.412 Val metric = 0.903\n",
            "Current training Loss 0.003: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.002: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.002: 100%|██████████████████| 32/32 [00:11<00:00,  2.81it/s]\n",
            "Train loss = 0.003 Train metric = 1.0 Val loss = 0.455 Val metric = 0.893\n",
            "Early stopping\n",
            "100%|███████████████████████████████████████████| 32/32 [00:10<00:00,  2.92it/s]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0.1, mixout prob 0, ...\n",
            "f1                                                       0.91221\n",
            "precision                                               0.870906\n",
            "recall                                                  0.957627\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1508\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                     _step 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                train_loss 0.003007085993885994\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_metric 0.9996972449288526\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _timestamp 1598949086.2798688\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_metric 0.8933474128827879\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  val_loss 0.4550885856151581\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  _runtime 2844.8589808940887\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced classic-resonance-22: https://app.wandb.ai/victor7246/wnut-task2-regularization/runs/19ycvm5v\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ITUTW3QLcCr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "270d1637-6f05-440f-d676-44ac55f102df"
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model18/4/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --model_save_path {model_dir} --dropout .2 --multi_sample_dropout_count 7\n",
        "\n",
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model18/5/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --model_save_path {model_dir} --dropout .3 --multi_sample_dropout_count 7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-09-01 08:37:07.080656: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "Tokenization\n",
            "100% 7000/7000 [00:04<00:00, 1623.51it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1829.78it/s]\n",
            "Train and val loader length 219 and 32\n",
            "Modelling\n",
            "Device: cuda\n",
            "TransformerMultiSample(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (final_out): Linear(in_features=7, out_features=1, bias=False)\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory /content/drive/My Drive/Models/WNUT-Task2/model18/4/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "[LOG] Total number of parameters to learn 124645639\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200901_083728-3gl20lx2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mruby-pond-23\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization/runs/3gl20lx2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "  0%|                                                   | 0/219 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Current training Loss 0.045: 100%|████████████| 219/219 [03:39<00:00,  1.05it/s]Running evaluation on whole training data\n",
            "Current training Loss 0.045: 100%|████████████| 219/219 [03:39<00:00,  1.00s/it]\n",
            "Current eval Loss 0.035: 100%|████████████████| 219/219 [01:19<00:00,  2.74it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.182: 100%|██████████████████| 32/32 [00:11<00:00,  2.80it/s]\n",
            "Train loss = 0.105 Train metric = 0.963 Val loss = 0.283 Val metric = 0.89\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/4/\n",
            "Current training Loss 0.072: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.028: 100%|████████████████| 219/219 [01:16<00:00,  2.86it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.248: 100%|██████████████████| 32/32 [00:10<00:00,  2.92it/s]\n",
            "Train loss = 0.054 Train metric = 0.984 Val loss = 0.274 Val metric = 0.897\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/4/\n",
            "Current training Loss 0.014: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.008: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.388: 100%|██████████████████| 32/32 [00:11<00:00,  2.81it/s]\n",
            "Train loss = 0.034 Train metric = 0.99 Val loss = 0.343 Val metric = 0.895\n",
            "Current training Loss 0.022: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.016: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.344: 100%|██████████████████| 32/32 [00:11<00:00,  2.81it/s]\n",
            "Train loss = 0.061 Train metric = 0.978 Val loss = 0.433 Val metric = 0.904\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/4/\n",
            "Current training Loss 0.007: 100%|███████████▉| 218/219 [03:41<00:01,  1.01s/it]Running evaluation on whole training data\n",
            "Current training Loss 0.007: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Current eval Loss 0.003: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.304: 100%|██████████████████| 32/32 [00:11<00:00,  2.81it/s]\n",
            "Train loss = 0.034 Train metric = 0.99 Val loss = 0.461 Val metric = 0.891\n",
            "Current training Loss 0.007: 100%|████████████| 219/219 [03:40<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.003: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.011: 100%|██████████████████| 32/32 [00:11<00:00,  2.78it/s]\n",
            "Train loss = 0.019 Train metric = 0.996 Val loss = 0.405 Val metric = 0.904\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/4/\n",
            "Current training Loss 0.007: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.002: 100%|████████████████| 219/219 [01:19<00:00,  2.74it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.029: 100%|██████████████████| 32/32 [00:11<00:00,  2.80it/s]\n",
            "Train loss = 0.01 Train metric = 0.998 Val loss = 0.384 Val metric = 0.912\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/4/\n",
            "Current training Loss 0.063: 100%|████████████| 219/219 [03:41<00:00,  1.04it/s]Running evaluation on whole training data\n",
            "Current training Loss 0.063: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Current eval Loss 0.002: 100%|████████████████| 219/219 [01:16<00:00,  2.87it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.011: 100%|██████████████████| 32/32 [00:10<00:00,  2.93it/s]\n",
            "Train loss = 0.01 Train metric = 0.997 Val loss = 0.411 Val metric = 0.903\n",
            "Current training Loss 0.005: 100%|████████████| 219/219 [03:40<00:00,  1.05it/s]Running evaluation on whole training data\n",
            "Current training Loss 0.005: 100%|████████████| 219/219 [03:40<00:00,  1.01s/it]\n",
            "Current eval Loss 0.002: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.03: 100%|███████████████████| 32/32 [00:11<00:00,  2.81it/s]\n",
            "Train loss = 0.01 Train metric = 0.997 Val loss = 0.418 Val metric = 0.913\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/4/\n",
            "Current training Loss 0.004: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.002: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.005: 100%|██████████████████| 32/32 [00:11<00:00,  2.81it/s]\n",
            "Train loss = 0.007 Train metric = 0.998 Val loss = 0.394 Val metric = 0.909\n",
            "Current training Loss 0.004: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.002: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.025: 100%|██████████████████| 32/32 [00:11<00:00,  2.82it/s]\n",
            "Train loss = 0.004 Train metric = 0.999 Val loss = 0.417 Val metric = 0.911\n",
            "Current training Loss 0.003: 100%|████████████| 219/219 [03:40<00:00,  1.05it/s]Running evaluation on whole training data\n",
            "Current training Loss 0.003: 100%|████████████| 219/219 [03:40<00:00,  1.01s/it]\n",
            "Current eval Loss 0.002: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.004: 100%|██████████████████| 32/32 [00:11<00:00,  2.80it/s]\n",
            "Train loss = 0.007 Train metric = 0.998 Val loss = 0.451 Val metric = 0.909\n",
            "Current training Loss 0.003: 100%|███████████▉| 218/219 [03:40<00:01,  1.01s/it]Running evaluation on whole training data\n",
            "Current training Loss 0.003: 100%|████████████| 219/219 [03:40<00:00,  1.01s/it]\n",
            "Current eval Loss 0.001: 100%|████████████████| 219/219 [01:16<00:00,  2.85it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.005: 100%|██████████████████| 32/32 [00:10<00:00,  2.92it/s]\n",
            "Train loss = 0.005 Train metric = 0.999 Val loss = 0.489 Val metric = 0.899\n",
            "Current training Loss 0.003: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.002: 100%|████████████████| 219/219 [01:16<00:00,  2.86it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.002: 100%|██████████████████| 32/32 [00:10<00:00,  2.93it/s]\n",
            "Train loss = 0.003 Train metric = 0.999 Val loss = 0.447 Val metric = 0.905\n",
            "Early stopping\n",
            "100%|███████████████████████████████████████████| 32/32 [00:10<00:00,  2.93it/s]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0.2, mixout prob 0, ...\n",
            "f1                                                      0.912779\n",
            "precision                                               0.875486\n",
            "recall                                                   0.95339\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1777\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  val_loss 0.44746774435043335\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                train_loss 0.00255225389264524\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  _runtime 4399.161231994629\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_metric 0.9990920096852299\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _timestamp 1598953825.4628665\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_metric 0.9051546391752577\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                     _step 13\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced ruby-pond-23: https://app.wandb.ai/victor7246/wnut-task2-regularization/runs/3gl20lx2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-09-01 09:50:44.106276: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "Tokenization\n",
            "100% 7000/7000 [00:04<00:00, 1606.37it/s]\n",
            "100% 1000/1000 [00:00<00:00, 1969.34it/s]\n",
            "Train and val loader length 219 and 32\n",
            "Modelling\n",
            "Device: cuda\n",
            "TransformerMultiSample(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.3, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.3, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (final_out): Linear(in_features=7, out_features=1, bias=False)\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory /content/drive/My Drive/Models/WNUT-Task2/model18/5/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "[LOG] Total number of parameters to learn 124645639\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200901_095105-1r33mnyl\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mroyal-wildflower-24\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization/runs/1r33mnyl\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "  0%|                                                   | 0/219 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Current training Loss 0.067: 100%|████████████| 219/219 [03:40<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.098: 100%|████████████████| 219/219 [01:19<00:00,  2.74it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.561: 100%|██████████████████| 32/32 [00:11<00:00,  2.80it/s]\n",
            "Train loss = 0.167 Train metric = 0.943 Val loss = 0.359 Val metric = 0.886\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/5/\n",
            "Current training Loss 0.136: 100%|███████████▉| 218/219 [03:41<00:01,  1.01s/it]Running evaluation on whole training data\n",
            "Current training Loss 0.136: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Current eval Loss 0.059: 100%|████████████████| 219/219 [01:16<00:00,  2.86it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.377: 100%|██████████████████| 32/32 [00:10<00:00,  2.92it/s]\n",
            "Train loss = 0.103 Train metric = 0.965 Val loss = 0.333 Val metric = 0.893\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/5/\n",
            "Current training Loss 0.195: 100%|████████████| 219/219 [03:40<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.014: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.035: 100%|██████████████████| 32/32 [00:11<00:00,  2.81it/s]\n",
            "Train loss = 0.06 Train metric = 0.98 Val loss = 0.275 Val metric = 0.9\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/5/\n",
            "Current training Loss 0.115: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.025: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.011: 100%|██████████████████| 32/32 [00:11<00:00,  2.80it/s]\n",
            "Train loss = 0.062 Train metric = 0.979 Val loss = 0.353 Val metric = 0.903\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/5/\n",
            "Current training Loss 0.028: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.012: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.029: 100%|██████████████████| 32/32 [00:11<00:00,  2.81it/s]\n",
            "Train loss = 0.063 Train metric = 0.979 Val loss = 0.387 Val metric = 0.904\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/5/\n",
            "Current training Loss 0.029: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.014: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.031: 100%|██████████████████| 32/32 [00:11<00:00,  2.77it/s]\n",
            "Train loss = 0.069 Train metric = 0.977 Val loss = 0.423 Val metric = 0.902\n",
            "Current training Loss 0.019: 100%|████████████| 219/219 [03:40<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.007: 100%|████████████████| 219/219 [01:19<00:00,  2.74it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.011: 100%|██████████████████| 32/32 [00:11<00:00,  2.79it/s]\n",
            "Train loss = 0.063 Train metric = 0.979 Val loss = 0.43 Val metric = 0.899\n",
            "Current training Loss 0.01: 100%|█████████████| 219/219 [03:40<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.003: 100%|████████████████| 219/219 [01:16<00:00,  2.86it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.026: 100%|██████████████████| 32/32 [00:10<00:00,  2.92it/s]\n",
            "Train loss = 0.056 Train metric = 0.98 Val loss = 0.437 Val metric = 0.897\n",
            "Current training Loss 0.009: 100%|████████████| 219/219 [03:41<00:00,  1.05it/s]Running evaluation on whole training data\n",
            "Current training Loss 0.009: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Current eval Loss 0.003: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.037: 100%|██████████████████| 32/32 [00:11<00:00,  2.80it/s]\n",
            "Train loss = 0.049 Train metric = 0.982 Val loss = 0.429 Val metric = 0.898\n",
            "Current training Loss 0.015: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.002: 100%|████████████████| 219/219 [01:19<00:00,  2.76it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.008: 100%|██████████████████| 32/32 [00:11<00:00,  2.81it/s]\n",
            "Train loss = 0.025 Train metric = 0.991 Val loss = 0.391 Val metric = 0.9\n",
            "Early stopping\n",
            "100%|███████████████████████████████████████████| 32/32 [00:10<00:00,  2.93it/s]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0.3, mixout prob 0, ...\n",
            "f1                                                      0.901296\n",
            "precision                                               0.851224\n",
            "recall                                                  0.957627\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2456\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_metric 0.9914247028734768\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_metric 0.9004065040650406\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                     _step 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                train_loss 0.02530435100197792\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  val_loss 0.39096787571907043\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _timestamp 1598957000.1601985\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  _runtime 3156.839745759964\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced royal-wildflower-24: https://app.wandb.ai/victor7246/wnut-task2-regularization/runs/1r33mnyl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd_pNJWuHk4F",
        "colab_type": "text"
      },
      "source": [
        "#### Multi-Sample - 7 with dropout .2 and regularization .02"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLO1ZVehFWQ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fc410a22-4607-4302-da52-e37f6823a20e"
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model18/6/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --model_save_path {model_dir} --dropout .2 --multi_sample_dropout_count 7 --l2 .02"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
            "2020-09-01 10:43:54.863451: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "Tokenization\n",
            "100% 7000/7000 [00:04<00:00, 1660.40it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2011.28it/s]\n",
            "Train and val loader length 219 and 32\n",
            "Modelling\n",
            "Device: cuda\n",
            "TransformerMultiSample(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (final_out): Linear(in_features=7, out_features=1, bias=False)\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory /content/drive/My Drive/Models/WNUT-Task2/model18/6/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "[LOG] Total number of parameters to learn 124645639\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.9.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200901_104415-3u7327ie\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgallant-hill-25\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://app.wandb.ai/victor7246/wnut-task2-regularization/runs/3u7327ie\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
            "\n",
            "  0%|                                                   | 0/219 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Current training Loss 0.332: 100%|████████████| 219/219 [03:40<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.314: 100%|████████████████| 219/219 [01:19<00:00,  2.76it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 2.179: 100%|██████████████████| 32/32 [00:11<00:00,  2.83it/s]\n",
            "Train loss = 0.373 Train metric = 0.964 Val loss = 2.126 Val metric = 0.9\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/6/\n",
            "Current training Loss 0.37: 100%|█████████████| 219/219 [03:40<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.284: 100%|████████████████| 219/219 [01:16<00:00,  2.87it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.914: 100%|██████████████████| 32/32 [00:10<00:00,  2.93it/s]\n",
            "Train loss = 0.324 Train metric = 0.981 Val loss = 2.126 Val metric = 0.876\n",
            "Current training Loss 0.303: 100%|████████████| 219/219 [03:40<00:00,  1.05it/s]Running evaluation on whole training data\n",
            "Current training Loss 0.303: 100%|████████████| 219/219 [03:40<00:00,  1.01s/it]\n",
            "Current eval Loss 0.273: 100%|████████████████| 219/219 [01:19<00:00,  2.76it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.87: 100%|███████████████████| 32/32 [00:11<00:00,  2.82it/s]\n",
            "Train loss = 0.297 Train metric = 0.992 Val loss = 2.131 Val metric = 0.886\n",
            "Current training Loss 0.276: 100%|████████████| 219/219 [03:40<00:00,  1.01s/it]Running evaluation on whole training data\n",
            "\n",
            "Current eval Loss 0.272: 100%|████████████████| 219/219 [01:19<00:00,  2.76it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.912: 100%|██████████████████| 32/32 [00:11<00:00,  2.82it/s]\n",
            "Train loss = 0.297 Train metric = 0.991 Val loss = 2.229 Val metric = 0.9\n",
            "Current training Loss 0.476: 100%|████████████| 219/219 [03:41<00:00,  1.05it/s]Running evaluation on whole training data\n",
            "Current training Loss 0.476: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Current eval Loss 0.311: 100%|████████████████| 219/219 [01:19<00:00,  2.76it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.854: 100%|██████████████████| 32/32 [00:11<00:00,  2.82it/s]\n",
            "Train loss = 0.296 Train metric = 0.991 Val loss = 2.228 Val metric = 0.906\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/6/\n",
            "Current training Loss 0.274: 100%|████████████| 219/219 [03:40<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.272: 100%|████████████████| 219/219 [01:16<00:00,  2.87it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.949: 100%|██████████████████| 32/32 [00:10<00:00,  2.94it/s]\n",
            "Train loss = 0.293 Train metric = 0.993 Val loss = 2.254 Val metric = 0.898\n",
            "Current training Loss 0.277: 100%|████████████| 219/219 [03:40<00:00,  1.05it/s]Running evaluation on whole training data\n",
            "Current training Loss 0.277: 100%|████████████| 219/219 [03:40<00:00,  1.01s/it]\n",
            "Current eval Loss 0.271: 100%|████████████████| 219/219 [01:19<00:00,  2.76it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 2.089: 100%|██████████████████| 32/32 [00:11<00:00,  2.82it/s]\n",
            "Train loss = 0.295 Train metric = 0.992 Val loss = 2.316 Val metric = 0.895\n",
            "Current training Loss 0.273: 100%|████████████| 219/219 [03:40<00:00,  1.05it/s]Running evaluation on whole training data\n",
            "Current training Loss 0.273: 100%|████████████| 219/219 [03:40<00:00,  1.01s/it]\n",
            "Current eval Loss 0.27: 100%|█████████████████| 219/219 [01:19<00:00,  2.76it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.841: 100%|██████████████████| 32/32 [00:11<00:00,  2.82it/s]\n",
            "Train loss = 0.282 Train metric = 0.996 Val loss = 2.233 Val metric = 0.911\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/6/\n",
            "Current training Loss 0.275: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.271: 100%|████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.842: 100%|██████████████████| 32/32 [00:11<00:00,  2.81it/s]\n",
            "Train loss = 0.283 Train metric = 0.997 Val loss = 2.231 Val metric = 0.914\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model18/6/\n",
            "Current training Loss 0.272: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.27: 100%|█████████████████| 219/219 [01:19<00:00,  2.76it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.841: 100%|██████████████████| 32/32 [00:11<00:00,  2.82it/s]\n",
            "Train loss = 0.274 Train metric = 0.999 Val loss = 2.218 Val metric = 0.907\n",
            "Current training Loss 0.272: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.27: 100%|█████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.875: 100%|██████████████████| 32/32 [00:11<00:00,  2.81it/s]\n",
            "Train loss = 0.274 Train metric = 0.999 Val loss = 2.271 Val metric = 0.907\n",
            "Current training Loss 0.271: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.27: 100%|█████████████████| 219/219 [01:19<00:00,  2.75it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.845: 100%|██████████████████| 32/32 [00:11<00:00,  2.81it/s]\n",
            "Train loss = 0.273 Train metric = 0.999 Val loss = 2.278 Val metric = 0.911\n",
            "Current training Loss 0.271: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.27: 100%|█████████████████| 219/219 [01:16<00:00,  2.86it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.843: 100%|██████████████████| 32/32 [00:10<00:00,  2.94it/s]\n",
            "Train loss = 0.273 Train metric = 0.999 Val loss = 2.313 Val metric = 0.907\n",
            "Current training Loss 0.271: 100%|████████████| 219/219 [03:41<00:00,  1.01s/it]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.27: 100%|█████████████████| 219/219 [01:19<00:00,  2.76it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.849: 100%|██████████████████| 32/32 [00:11<00:00,  2.81it/s]\n",
            "Train loss = 0.273 Train metric = 0.999 Val loss = 2.316 Val metric = 0.905\n",
            "Early stopping\n",
            "100%|███████████████████████████████████████████| 32/32 [00:10<00:00,  2.94it/s]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0.2, mixout prob 0, ...\n",
            "f1                                                      0.913706\n",
            "precision                                               0.877193\n",
            "recall                                                   0.95339\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2558\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                     _step 13\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _timestamp 1598961427.766099\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_metric 0.9048582995951417\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_metric 0.9993946731234866\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  val_loss 2.315802812576294\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                train_loss 0.2727433443069458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                  _runtime 4393.647965669632\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced gallant-hill-25: https://app.wandb.ai/victor7246/wnut-task2-regularization/runs/3u7327ie\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzw7AtyDBxNI",
        "colab_type": "text"
      },
      "source": [
        "#### Augmentation\n",
        "\n",
        "Random word replacement and translation augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5VsWPaySZQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7859200e-90cd-41b1-8528-28ddec46081b"
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model19/1/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --train_data ../data/processed/augmented/train.tsv --model_save_path {model_dir} --aug_p_rate .1 --dropout .1\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-02 05:28:26.575849: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "100% 14000/14000 [00:05<00:00, 2548.16it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2655.05it/s]\n",
            "Train and val loader length 438 and 32\n",
            "Modelling\n",
            "Device: cuda\n",
            "Transformer(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (drop): Dropout(p=0.1, inplace=False)\n",
            "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory /content/drive/My Drive/Models/WNUT-Task2/model19/1/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "[LOG] Total number of parameters to learn 124646401\n",
            "  0% 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Current training Loss 0.161: 100% 438/438 [02:17<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.081: 100% 438/438 [00:44<00:00,  9.91it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.108: 100% 32/32 [00:03<00:00, 10.10it/s]\n",
            "Train loss = 0.062 Train metric = 0.978 Val loss = 0.269 Val metric = 0.9\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model19/1/\n",
            "Current training Loss 0.005: 100% 438/438 [02:17<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.006: 100% 438/438 [00:44<00:00,  9.90it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.058: 100% 32/32 [00:03<00:00, 10.11it/s]\n",
            "Train loss = 0.029 Train metric = 0.989 Val loss = 0.354 Val metric = 0.9\n",
            "Current training Loss 0.004: 100% 438/438 [02:17<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100% 438/438 [00:44<00:00,  9.87it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.011: 100% 32/32 [00:03<00:00, 10.11it/s]\n",
            "Train loss = 0.012 Train metric = 0.996 Val loss = 0.335 Val metric = 0.912\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model19/1/\n",
            "Current training Loss 0.033: 100% 438/438 [02:17<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.002: 100% 438/438 [00:44<00:00,  9.91it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.022: 100% 32/32 [00:03<00:00, 10.11it/s]\n",
            "Train loss = 0.031 Train metric = 0.99 Val loss = 0.467 Val metric = 0.899\n",
            "Current training Loss 0.004: 100% 438/438 [02:17<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100% 438/438 [00:44<00:00,  9.91it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.012: 100% 32/32 [00:03<00:00, 10.11it/s]\n",
            "Train loss = 0.022 Train metric = 0.992 Val loss = 0.589 Val metric = 0.898\n",
            "Current training Loss 0.001: 100% 438/438 [02:17<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100% 438/438 [00:44<00:00,  9.94it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.001: 100% 32/32 [00:03<00:00, 10.13it/s]\n",
            "Train loss = 0.007 Train metric = 0.997 Val loss = 0.474 Val metric = 0.913\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model19/1/\n",
            "Current training Loss 0.0: 100% 438/438 [02:17<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100% 438/438 [00:44<00:00,  9.91it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.0: 100% 32/32 [00:03<00:00, 10.14it/s]\n",
            "Train loss = 0.002 Train metric = 0.999 Val loss = 0.41 Val metric = 0.92\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model19/1/\n",
            "Current training Loss 0.001: 100% 438/438 [02:17<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100% 438/438 [00:44<00:00,  9.94it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.008: 100% 32/32 [00:03<00:00, 10.16it/s]\n",
            "Train loss = 0.006 Train metric = 0.999 Val loss = 0.461 Val metric = 0.904\n",
            "Current training Loss 0.002: 100% 438/438 [02:17<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100% 438/438 [00:44<00:00,  9.94it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.077: 100% 32/32 [00:03<00:00, 10.15it/s]\n",
            "Train loss = 0.003 Train metric = 0.999 Val loss = 0.515 Val metric = 0.909\n",
            "Current training Loss 0.0: 100% 438/438 [02:17<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100% 438/438 [00:44<00:00,  9.94it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.043: 100% 32/32 [00:03<00:00, 10.15it/s]\n",
            "Train loss = 0.001 Train metric = 1.0 Val loss = 0.624 Val metric = 0.903\n",
            "Current training Loss 0.0: 100% 438/438 [02:17<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100% 438/438 [00:44<00:00,  9.91it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.125: 100% 32/32 [00:03<00:00, 10.15it/s]\n",
            "Train loss = 0.001 Train metric = 1.0 Val loss = 0.609 Val metric = 0.909\n",
            "Current training Loss 0.0: 100% 438/438 [02:17<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100% 438/438 [00:44<00:00,  9.95it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.077: 100% 32/32 [00:03<00:00, 10.15it/s]\n",
            "Train loss = 0.0 Train metric = 1.0 Val loss = 0.611 Val metric = 0.907\n",
            "Early stopping\n",
            "100% 32/32 [00:03<00:00, 10.34it/s]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0.1, mixout prob 0, ...\n",
            "f1                                                      0.920408\n",
            "precision                                               0.887795\n",
            "recall                                                  0.955508\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKMOvQiFmT4u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7c96d0ab-1e28-4e32-ea86-bf0bfa216533"
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model19/1/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --train_data ../data/processed/augmented/train.tsv --model_save_path {model_dir} --aug_p_rate .1 --dropout .1\n",
        "\n",
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model19/2/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --train_data ../data/processed/augmented/train.tsv --model_save_path {model_dir} --aug_p_rate .2 --dropout .1\n",
        "\n",
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model19/3/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --train_data ../data/processed/augmented/train.tsv --model_save_path {model_dir} --aug_p_rate .3 --dropout .1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-02 03:59:53.570047: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "Downloading: 100% 481/481 [00:00<00:00, 425kB/s]\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 2.09MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.06MB/s]\n",
            "100% 14000/14000 [00:05<00:00, 2593.01it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2741.77it/s]\n",
            "Train and val loader length 438 and 32\n",
            "Modelling\n",
            "Device: cuda\n",
            "Downloading: 100% 501M/501M [00:15<00:00, 33.0MB/s]\n",
            "Transformer(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (drop): Dropout(p=0.1, inplace=False)\n",
            "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
            ")\n",
            "Traceback (most recent call last):\n",
            "  File \"exp8_torch.py\", line 316, in <module>\n",
            "    main(args)\n",
            "  File \"exp8_torch.py\", line 135, in main\n",
            "    with open(os.path.join(model_save_dir, 'config.pkl'), 'wb') as handle:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/My Drive/Models/WNUT-Task2/model19/1/config.pkl'\n",
            "2020-09-02 04:00:35.931133: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "100% 21000/21000 [00:07<00:00, 2789.40it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2713.42it/s]\n",
            "Train and val loader length 657 and 32\n",
            "Modelling\n",
            "Device: cuda\n",
            "Transformer(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (drop): Dropout(p=0.1, inplace=False)\n",
            "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory /content/drive/My Drive/Models/WNUT-Task2/model19/2/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "[LOG] Total number of parameters to learn 124646401\n",
            "  0% 0/657 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Current training Loss 0.063: 100% 657/657 [03:26<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.036: 100% 657/657 [01:06<00:00,  9.92it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.02: 100% 32/32 [00:03<00:00, 10.11it/s]\n",
            "Train loss = 0.061 Train metric = 0.978 Val loss = 0.253 Val metric = 0.899\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model19/2/\n",
            "Current training Loss 0.002: 100% 657/657 [03:26<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100% 657/657 [01:06<00:00,  9.87it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.06: 100% 32/32 [00:03<00:00, 10.12it/s]\n",
            "Train loss = 0.02 Train metric = 0.993 Val loss = 0.348 Val metric = 0.91\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model19/2/\n",
            "Current training Loss 0.001: 100% 657/657 [03:26<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100% 657/657 [01:06<00:00,  9.92it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.145: 100% 32/32 [00:03<00:00, 10.14it/s]\n",
            "Train loss = 0.012 Train metric = 0.995 Val loss = 0.33 Val metric = 0.908\n",
            "Current training Loss 0.023: 100% 657/657 [03:26<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100% 657/657 [01:06<00:00,  9.88it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.008: 100% 32/32 [00:03<00:00, 10.08it/s]\n",
            "Train loss = 0.006 Train metric = 0.998 Val loss = 0.423 Val metric = 0.914\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model19/2/\n",
            "Current training Loss 0.002: 100% 657/657 [03:26<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100% 657/657 [01:06<00:00,  9.86it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.684: 100% 32/32 [00:03<00:00, 10.07it/s]\n",
            "Train loss = 0.01 Train metric = 0.996 Val loss = 0.435 Val metric = 0.911\n",
            "Current training Loss 0.005: 100% 657/657 [03:26<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100% 657/657 [01:06<00:00,  9.89it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.562: 100% 32/32 [00:03<00:00, 10.07it/s]\n",
            "Train loss = 0.003 Train metric = 0.999 Val loss = 0.532 Val metric = 0.908\n",
            "Current training Loss 0.0: 100% 657/657 [03:26<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100% 657/657 [01:06<00:00,  9.86it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.934: 100% 32/32 [00:03<00:00, 10.10it/s]\n",
            "Train loss = 0.001 Train metric = 1.0 Val loss = 0.596 Val metric = 0.909\n",
            "Current training Loss 0.001: 100% 657/657 [03:26<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100% 657/657 [01:06<00:00,  9.88it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.903: 100% 32/32 [00:03<00:00, 10.08it/s]\n",
            "Train loss = 0.001 Train metric = 1.0 Val loss = 0.543 Val metric = 0.899\n",
            "Current training Loss 0.0: 100% 657/657 [03:26<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100% 657/657 [01:06<00:00,  9.88it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.861: 100% 32/32 [00:03<00:00, 10.08it/s]\n",
            "Train loss = 0.0 Train metric = 1.0 Val loss = 0.62 Val metric = 0.906\n",
            "Early stopping\n",
            "100% 32/32 [00:03<00:00, 10.27it/s]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0.1, mixout prob 0, ...\n",
            "f1                                                      0.914228\n",
            "precision                                               0.872832\n",
            "recall                                                  0.959746\n",
            "Name: 0, dtype: object\n",
            "2020-09-02 04:42:47.482916: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "100% 21000/21000 [00:08<00:00, 2614.10it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2520.30it/s]\n",
            "Train and val loader length 657 and 32\n",
            "Modelling\n",
            "Device: cuda\n",
            "Transformer(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (drop): Dropout(p=0.1, inplace=False)\n",
            "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory /content/drive/My Drive/Models/WNUT-Task2/model19/3/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "[LOG] Total number of parameters to learn 124646401\n",
            "  0% 0/657 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Current training Loss 0.063: 100% 657/657 [03:26<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.036: 100% 657/657 [01:06<00:00,  9.84it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.02: 100% 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.061 Train metric = 0.978 Val loss = 0.253 Val metric = 0.899\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model19/3/\n",
            "Current training Loss 0.002: 100% 657/657 [03:27<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100% 657/657 [01:07<00:00,  9.80it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.06: 100% 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.02 Train metric = 0.993 Val loss = 0.348 Val metric = 0.91\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model19/3/\n",
            "Current training Loss 0.001: 100% 657/657 [03:27<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100% 657/657 [01:06<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.145: 100% 32/32 [00:03<00:00, 10.03it/s]\n",
            "Train loss = 0.012 Train metric = 0.995 Val loss = 0.33 Val metric = 0.908\n",
            "Current training Loss 0.023: 100% 657/657 [03:26<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100% 657/657 [01:06<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.008: 100% 32/32 [00:03<00:00, 10.03it/s]\n",
            "Train loss = 0.006 Train metric = 0.998 Val loss = 0.423 Val metric = 0.914\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model19/3/\n",
            "Current training Loss 0.002: 100% 657/657 [03:27<00:00,  3.17it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100% 657/657 [01:06<00:00,  9.81it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.684: 100% 32/32 [00:03<00:00, 10.04it/s]\n",
            "Train loss = 0.01 Train metric = 0.996 Val loss = 0.435 Val metric = 0.911\n",
            "Current training Loss 0.005: 100% 657/657 [03:26<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100% 657/657 [01:06<00:00,  9.88it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.562: 100% 32/32 [00:03<00:00, 10.09it/s]\n",
            "Train loss = 0.003 Train metric = 0.999 Val loss = 0.532 Val metric = 0.908\n",
            "Current training Loss 0.0: 100% 657/657 [03:26<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100% 657/657 [01:06<00:00,  9.86it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.934: 100% 32/32 [00:03<00:00, 10.09it/s]\n",
            "Train loss = 0.001 Train metric = 1.0 Val loss = 0.596 Val metric = 0.909\n",
            "Current training Loss 0.001: 100% 657/657 [03:26<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100% 657/657 [01:06<00:00,  9.88it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.903: 100% 32/32 [00:03<00:00, 10.10it/s]\n",
            "Train loss = 0.001 Train metric = 1.0 Val loss = 0.543 Val metric = 0.899\n",
            "Current training Loss 0.0: 100% 657/657 [03:26<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100% 657/657 [01:06<00:00,  9.89it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.861: 100% 32/32 [00:03<00:00, 10.07it/s]\n",
            "Train loss = 0.0 Train metric = 1.0 Val loss = 0.62 Val metric = 0.906\n",
            "Early stopping\n",
            "100% 32/32 [00:03<00:00, 10.27it/s]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0.1, mixout prob 0, ...\n",
            "f1                                                      0.914228\n",
            "precision                                               0.872832\n",
            "recall                                                  0.959746\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jB7ik48SczT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b2908d47-c1bc-41c6-8fda-f9be89183954"
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model19/4/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --train_data ../data/processed/augmented/train.tsv --model_save_path {model_dir} --aug_p_rate .1  --dropout .2 --multi_sample_dropout_count 7 --l2 .02"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-02 06:13:21.358249: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "100% 14000/14000 [00:05<00:00, 2508.50it/s]\n",
            "100% 1000/1000 [00:00<00:00, 2734.34it/s]\n",
            "Train and val loader length 438 and 32\n",
            "Modelling\n",
            "Device: cuda\n",
            "TransformerMultiSample(\n",
            "  (base_model): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.2, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (final_out): Linear(in_features=7, out_features=1, bias=False)\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory /content/drive/My Drive/Models/WNUT-Task2/model19/4/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "[LOG] Total number of parameters to learn 124645639\n",
            "  0% 0/438 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Current training Loss 0.285: 100% 438/438 [02:17<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.267: 100% 438/438 [00:44<00:00,  9.89it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 2.076: 100% 32/32 [00:03<00:00, 10.08it/s]\n",
            "Train loss = 0.221 Train metric = 0.97 Val loss = 2.14 Val metric = 0.896\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model19/4/\n",
            "Current training Loss 0.166: 100% 438/438 [02:17<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.156: 100% 438/438 [00:44<00:00,  9.90it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 2.204: 100% 32/32 [00:03<00:00, 10.10it/s]\n",
            "Train loss = 0.18 Train metric = 0.985 Val loss = 2.186 Val metric = 0.893\n",
            "Current training Loss 0.141: 100% 438/438 [02:17<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.141: 100% 438/438 [00:44<00:00,  9.87it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.937: 100% 32/32 [00:03<00:00, 10.11it/s]\n",
            "Train loss = 0.174 Train metric = 0.987 Val loss = 2.23 Val metric = 0.903\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model19/4/\n",
            "Current training Loss 0.148: 100% 438/438 [02:17<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.148: 100% 438/438 [00:44<00:00,  9.90it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 2.134: 100% 32/32 [00:03<00:00, 10.10it/s]\n",
            "Train loss = 0.161 Train metric = 0.991 Val loss = 2.231 Val metric = 0.902\n",
            "Current training Loss 0.145: 100% 438/438 [02:17<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.137: 100% 438/438 [00:44<00:00,  9.89it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.874: 100% 32/32 [00:03<00:00, 10.09it/s]\n",
            "Train loss = 0.158 Train metric = 0.992 Val loss = 2.283 Val metric = 0.898\n",
            "Current training Loss 0.149: 100% 438/438 [02:17<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.139: 100% 438/438 [00:44<00:00,  9.90it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 2.594: 100% 32/32 [00:03<00:00, 10.06it/s]\n",
            "Train loss = 0.194 Train metric = 0.979 Val loss = 2.363 Val metric = 0.892\n",
            "Current training Loss 0.159: 100% 438/438 [02:17<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.136: 100% 438/438 [00:44<00:00,  9.86it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 2.647: 100% 32/32 [00:03<00:00, 10.10it/s]\n",
            "Train loss = 0.186 Train metric = 0.983 Val loss = 2.426 Val metric = 0.891\n",
            "Current training Loss 0.142: 100% 438/438 [02:17<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.136: 100% 438/438 [00:44<00:00,  9.90it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 2.295: 100% 32/32 [00:03<00:00, 10.10it/s]\n",
            "Train loss = 0.147 Train metric = 0.996 Val loss = 2.303 Val metric = 0.904\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model19/4/\n",
            "Current training Loss 0.168: 100% 438/438 [02:17<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.136: 100% 438/438 [00:44<00:00,  9.91it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.951: 100% 32/32 [00:03<00:00, 10.10it/s]\n",
            "Train loss = 0.139 Train metric = 0.999 Val loss = 2.317 Val metric = 0.903\n",
            "Current training Loss 0.137: 100% 438/438 [02:17<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.136: 100% 438/438 [00:44<00:00,  9.91it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 2.551: 100% 32/32 [00:03<00:00, 10.12it/s]\n",
            "Train loss = 0.142 Train metric = 0.998 Val loss = 2.373 Val metric = 0.9\n",
            "Current training Loss 0.137: 100% 438/438 [02:17<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.135: 100% 438/438 [00:44<00:00,  9.87it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 2.548: 100% 32/32 [00:03<00:00, 10.10it/s]\n",
            "Train loss = 0.14 Train metric = 0.998 Val loss = 2.38 Val metric = 0.9\n",
            "Current training Loss 0.137: 100% 438/438 [02:17<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.135: 100% 438/438 [00:44<00:00,  9.92it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 2.244: 100% 32/32 [00:03<00:00, 10.12it/s]\n",
            "Train loss = 0.137 Train metric = 0.999 Val loss = 2.367 Val metric = 0.901\n",
            "Current training Loss 0.136: 100% 438/438 [02:17<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.135: 100% 438/438 [00:44<00:00,  9.91it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 2.153: 100% 32/32 [00:03<00:00, 10.12it/s]\n",
            "Train loss = 0.138 Train metric = 0.999 Val loss = 2.387 Val metric = 0.904\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model19/4/\n",
            "Current training Loss 0.136: 100% 438/438 [02:17<00:00,  3.19it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.135: 100% 438/438 [00:44<00:00,  9.89it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.94: 100% 32/32 [00:03<00:00, 10.12it/s]\n",
            "Train loss = 0.137 Train metric = 1.0 Val loss = 2.383 Val metric = 0.906\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model19/4/\n",
            "Current training Loss 0.136: 100% 438/438 [02:17<00:00,  3.18it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.135: 100% 438/438 [00:44<00:00,  9.91it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 1.876: 100% 32/32 [00:03<00:00, 10.11it/s]\n",
            "Train loss = 0.136 Train metric = 1.0 Val loss = 2.375 Val metric = 0.905\n",
            "100% 32/32 [00:03<00:00, 10.31it/s]\n",
            "Evaluation\n",
            "description    roberta-base with dropout 0.2, mixout prob 0, ...\n",
            "f1                                                      0.905433\n",
            "precision                                               0.862069\n",
            "recall                                                   0.95339\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZGj3T1Y51y-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9f327c19-8cf0-4837-9138-c54ed1d2dc23"
      },
      "source": [
        "model_dir = '/content/drive/My\\ Drive/Models/WNUT-Task2/model19/5/'\n",
        "!cd WNUT-2020-Task-2/experiments/ && python exp8_torch.py --train_data ../data/processed/translated/train.tsv --val_data ../data/processed/translated/valid.tsv --model_save_path {model_dir} --transformer_model_name xlm-roberta-base"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-02 07:04:44.542669: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Tokenization\n",
            "Downloading: 100% 512/512 [00:00<00:00, 521kB/s]\n",
            "Downloading: 100% 5.07M/5.07M [00:00<00:00, 7.31MB/s]\n",
            "100% 21000/21000 [00:04<00:00, 4845.70it/s]\n",
            "100% 3000/3000 [00:00<00:00, 5099.16it/s]\n",
            "Train and val loader length 657 and 94\n",
            "Modelling\n",
            "Device: cuda\n",
            "Downloading: 100% 1.12G/1.12G [00:35<00:00, 31.5MB/s]\n",
            "Transformer(\n",
            "  (base_model): XLMRobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (drop): Dropout(p=0, inplace=False)\n",
            "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory /content/drive/My Drive/Models/WNUT-Task2/model19/5/ exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
            "  warnings.warn(*args, **kwargs)\n",
            "[LOG] Total number of parameters to learn 278044417\n",
            "  0% 0/657 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "Current training Loss 0.015: 100% 657/657 [03:46<00:00,  2.90it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.012: 100% 657/657 [01:06<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.343: 100% 94/94 [00:09<00:00,  9.84it/s]\n",
            "Train loss = 0.07 Train metric = 0.974 Val loss = 0.346 Val metric = 0.87\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model19/5/\n",
            "Current training Loss 0.004: 100% 657/657 [03:46<00:00,  2.90it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.004: 100% 657/657 [01:06<00:00,  9.82it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.517: 100% 94/94 [00:09<00:00,  9.84it/s]\n",
            "Train loss = 0.023 Train metric = 0.992 Val loss = 0.368 Val metric = 0.883\n",
            "Saving best model in /content/drive/My Drive/Models/WNUT-Task2/model19/5/\n",
            "Current training Loss 0.008: 100% 657/657 [03:46<00:00,  2.91it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.008: 100% 657/657 [01:06<00:00,  9.84it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.588: 100% 94/94 [00:09<00:00,  9.84it/s]\n",
            "Train loss = 0.017 Train metric = 0.994 Val loss = 0.481 Val metric = 0.883\n",
            "Current training Loss 0.003: 100% 657/657 [03:46<00:00,  2.91it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.003: 100% 657/657 [01:06<00:00,  9.81it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.472: 100% 94/94 [00:09<00:00,  9.82it/s]\n",
            "Train loss = 0.011 Train metric = 0.997 Val loss = 0.464 Val metric = 0.879\n",
            "Current training Loss 0.001: 100% 657/657 [03:46<00:00,  2.90it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.001: 100% 657/657 [01:06<00:00,  9.83it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.55: 100% 94/94 [00:09<00:00,  9.83it/s]\n",
            "Train loss = 0.009 Train metric = 0.997 Val loss = 0.499 Val metric = 0.876\n",
            "Current training Loss 0.0: 100% 657/657 [03:46<00:00,  2.90it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100% 657/657 [01:06<00:00,  9.82it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.538: 100% 94/94 [00:09<00:00,  9.82it/s]\n",
            "Train loss = 0.003 Train metric = 0.999 Val loss = 0.574 Val metric = 0.879\n",
            "Current training Loss 0.0: 100% 657/657 [03:46<00:00,  2.90it/s]\n",
            "Running evaluation on whole training data\n",
            "Current eval Loss 0.0: 100% 657/657 [01:06<00:00,  9.81it/s]\n",
            "Running evaluation on validation data\n",
            "Current eval Loss 0.424: 100% 94/94 [00:09<00:00,  9.83it/s]\n",
            "Train loss = 0.006 Train metric = 0.998 Val loss = 0.556 Val metric = 0.879\n",
            "Early stopping\n",
            "100% 94/94 [00:09<00:00, 10.10it/s]\n",
            "Evaluation\n",
            "description    xlm-roberta-base with dropout 0, mixout prob 0...\n",
            "f1                                                       0.88338\n",
            "precision                                               0.873619\n",
            "recall                                                  0.893362\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}